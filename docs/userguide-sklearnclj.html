<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link href="https://stackpath.bootstrapcdn.com/bootswatch/4.5.0/sandstone/bootstrap.min.css" rel="stylesheet" type="text/css">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/solarized-light.min.css" rel="stylesheet" type="text/css">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/ag-grid/24.0.0/styles/ag-grid.min.css" rel="stylesheet" type="text/css">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/ag-grid/24.0.0/styles/ag-theme-balham.min.css" rel="stylesheet" type="text/css">
        <link href="https://unpkg.com/leaflet@1.6.0/dist/leaflet.css" rel="stylesheet" type="text/css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
        <!-- The loading of KaTeX is deferred to speed up page rendering -->
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    </head>
    <body>
        <p id="loading">Loading ...</p>
        <div id="app"></div>
    </body>
    <script src="https://cdn.statically.io/gh/scicloj/gorilla-notes@master/dist/0.5.10/main.js"></script>
    <script>
     shadow.loader.load("main");
     gorilla_notes.main.main_BANG_(false, "{:options {:reverse-notes? false, :header? false, :notes-in-cards? false, :initially-collapse? false, :auto-scroll? false, :port 1903, :custom-header [:div {:style {:font-style \"italic\", :font-family \"\\\"Lucida Console\\\", Courier, monospace\"}} \"(notespace)\" [:p \"Tue Mar 23 08:22:54 CET 2021\"] nil [:hr]], :custom-footer [:div [:hr] [:hr]]}, :ids [\"94\" \"91\" \"95\" \"96\" \"87\" \"97\"], :id->content {\"87\" [:div [:p] nil nil [:p/markdown \"## Sklearn regression\"]], \"91\" [:div [:p] nil nil [:p/markdown \"# Models\"]], \"94\" [:div [:p] [:div [:p/code {:code \"(comment\\n  (note/init-with-browser)\\n  (notespace.api/update-config\\n   #(assoc % :source-base-path \\\"userguide\\\"))\\n\\n  (note/eval-this-notespace)\\n  (note/reread-this-notespace)\\n  (note/render-static-html \\\"docs/userguide-sklearnclj.html\\\")\\n  (note/init)\\n\\n  )\", :bg-class \"bg-light\"}]] nil [:p/code {:code \"nil\\n\"}]], \"95\" [:div [:p] nil nil [:p/markdown \"## Sklearn classification\"]], \"96\" [:div [:p] nil nil ([:div [:h3 \":sklearn.classification/ada-boost-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"322px\"}} [:p/markdown \"_unnamed [5 2]:\\n\\n|           :name | :default |\\n|-----------------|----------|\\n|      :algorithm |  SAMME.R |\\n| :base-estimator |          |\\n|  :learning-rate |    1.000 |\\n|   :n-estimators |       50 |\\n|   :random-state |          |\\n\"]]] [:span (\"An AdaBoost classifier.\" [:br] \"\" [:br] \"    An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\" [:br] \"    classifier on the original dataset and then fits additional copies of the\" [:br] \"    classifier on the same dataset but where the weights of incorrectly\" [:br] \"    classified instances are adjusted such that subsequent classifiers focus\" [:br] \"    more on difficult cases.\" [:br] \"\" [:br] \"    This class implements the algorithm known as AdaBoost-SAMME [2].\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <adaboost>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.14\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    base_estimator : object, default=None\" [:br] \"        The base estimator from which the boosted ensemble is built.\" [:br] \"        Support for sample weighting is required, as well as proper\" [:br] \"        ``classes_`` and ``n_classes_`` attributes. If ``None``, then\" [:br] \"        the base estimator is ``DecisionTreeClassifier(max_depth=1)``.\" [:br] \"\" [:br] \"    n_estimators : int, default=50\" [:br] \"        The maximum number of estimators at which boosting is terminated.\" [:br] \"        In case of perfect fit, the learning procedure is stopped early.\" [:br] \"\" [:br] \"    learning_rate : float, default=1.\" [:br] \"        Learning rate shrinks the contribution of each classifier by\" [:br] \"        ``learning_rate``. There is a trade-off between ``learning_rate`` and\" [:br] \"        ``n_estimators``.\" [:br] \"\" [:br] \"    algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\" [:br] \"        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\" [:br] \"        ``base_estimator`` must support calculation of class probabilities.\" [:br] \"        If 'SAMME' then use the SAMME discrete boosting algorithm.\" [:br] \"        The SAMME.R algorithm typically converges faster than SAMME,\" [:br] \"        achieving a lower test error with fewer boosting iterations.\" [:br] \"\" [:br] \"    random_state : int or RandomState, default=None\" [:br] \"        Controls the random seed given at each `base_estimator` at each\" [:br] \"        boosting iteration.\" [:br] \"        Thus, it is only used when `base_estimator` exposes a `random_state`.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    base_estimator_ : estimator\" [:br] \"        The base estimator from which the ensemble is grown.\" [:br] \"\" [:br] \"    estimators_ : list of classifiers\" [:br] \"        The collection of fitted sub-estimators.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        The classes labels.\" [:br] \"\" [:br] \"    n_classes_ : int\" [:br] \"        The number of classes.\" [:br] \"\" [:br] \"    estimator_weights_ : ndarray of floats\" [:br] \"        Weights for each estimator in the boosted ensemble.\" [:br] \"\" [:br] \"    estimator_errors_ : ndarray of floats\" [:br] \"        Classification error for each estimator in the boosted\" [:br] \"        ensemble.\" [:br] \"\" [:br] \"    feature_importances_ : ndarray of shape (n_features,)\" [:br] \"        The impurity-based feature importances if supported by the\" [:br] \"        ``base_estimator`` (when based on decision trees).\" [:br] \"\" [:br] \"        Warning: impurity-based feature importances can be misleading for\" [:br] \"        high cardinality features (many unique values). See\" [:br] \"        :func:`sklearn.inspection.permutation_importance` as an alternative.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    AdaBoostRegressor\" [:br] \"        An AdaBoost regressor that begins by fitting a regressor on the\" [:br] \"        original dataset and then fits additional copies of the regressor\" [:br] \"        on the same dataset but where the weights of instances are\" [:br] \"        adjusted according to the error of the current prediction.\" [:br] \"\" [:br] \"    GradientBoostingClassifier\" [:br] \"        GB builds an additive model in a forward stage-wise fashion. Regression\" [:br] \"        trees are fit on the negative gradient of the binomial or multinomial\" [:br] \"        deviance loss function. Binary classification is a special case where\" [:br] \"        only a single regression tree is induced.\" [:br] \"\" [:br] \"    sklearn.tree.DecisionTreeClassifier\" [:br] \"        A non-parametric supervised learning method used for classification.\" [:br] \"        Creates a model that predicts the value of a target variable by\" [:br] \"        learning simple decision rules inferred from the data features.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    .. [1] Y. Freund, R. Schapire, \\\"A Decision-Theoretic Generalization of\" [:br] \"           on-Line Learning and an Application to Boosting\\\", 1995.\" [:br] \"\" [:br] \"    .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \\\"Multi-class AdaBoost\\\", 2009.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.ensemble import AdaBoostClassifier\" [:br] \"    >>> from sklearn.datasets import make_classification\" [:br] \"    >>> X, y = make_classification(n_samples=1000, n_features=4,\" [:br] \"    ...                            n_informative=2, n_redundant=0,\" [:br] \"    ...                            random_state=0, shuffle=False)\" [:br] \"    >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    AdaBoostClassifier(n_estimators=100, random_state=0)\" [:br] \"    >>> clf.predict([[0, 0, 0, 0]])\" [:br] \"    array([1])\" [:br] \"    >>> clf.score(X, y)\" [:br] \"    0.983...\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/bagging-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [11 2]:\\n\\n|               :name | :default |\\n|---------------------|----------|\\n|          :bootstrap |     true |\\n| :bootstrap-features |    false |\\n|             :n-jobs |          |\\n|       :random-state |          |\\n|          :oob-score |    false |\\n|     :base-estimator |          |\\n|       :max-features |    1.000 |\\n|         :warm-start |    false |\\n|       :n-estimators |       10 |\\n|        :max-samples |    1.000 |\\n|            :verbose |        0 |\\n\"]]] [:span (\"A Bagging classifier.\" [:br] \"\" [:br] \"    A Bagging classifier is an ensemble meta-estimator that fits base\" [:br] \"    classifiers each on random subsets of the original dataset and then\" [:br] \"    aggregate their individual predictions (either by voting or by averaging)\" [:br] \"    to form a final prediction. Such a meta-estimator can typically be used as\" [:br] \"    a way to reduce the variance of a black-box estimator (e.g., a decision\" [:br] \"    tree), by introducing randomization into its construction procedure and\" [:br] \"    then making an ensemble out of it.\" [:br] \"\" [:br] \"    This algorithm encompasses several works from the literature. When random\" [:br] \"    subsets of the dataset are drawn as random subsets of the samples, then\" [:br] \"    this algorithm is known as Pasting [1]_. If samples are drawn with\" [:br] \"    replacement, then the method is known as Bagging [2]_. When random subsets\" [:br] \"    of the dataset are drawn as random subsets of the features, then the method\" [:br] \"    is known as Random Subspaces [3]_. Finally, when base estimators are built\" [:br] \"    on subsets of both samples and features, then the method is known as\" [:br] \"    Random Patches [4]_.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <bagging>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.15\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    base_estimator : object, default=None\" [:br] \"        The base estimator to fit on random subsets of the dataset.\" [:br] \"        If None, then the base estimator is a decision tree.\" [:br] \"\" [:br] \"    n_estimators : int, default=10\" [:br] \"        The number of base estimators in the ensemble.\" [:br] \"\" [:br] \"    max_samples : int or float, default=1.0\" [:br] \"        The number of samples to draw from X to train each base estimator (with\" [:br] \"        replacement by default, see `bootstrap` for more details).\" [:br] \"\" [:br] \"        - If int, then draw `max_samples` samples.\" [:br] \"        - If float, then draw `max_samples * X.shape[0]` samples.\" [:br] \"\" [:br] \"    max_features : int or float, default=1.0\" [:br] \"        The number of features to draw from X to train each base estimator (\" [:br] \"        without replacement by default, see `bootstrap_features` for more\" [:br] \"        details).\" [:br] \"\" [:br] \"        - If int, then draw `max_features` features.\" [:br] \"        - If float, then draw `max_features * X.shape[1]` features.\" [:br] \"\" [:br] \"    bootstrap : bool, default=True\" [:br] \"        Whether samples are drawn with replacement. If False, sampling\" [:br] \"        without replacement is performed.\" [:br] \"\" [:br] \"    bootstrap_features : bool, default=False\" [:br] \"        Whether features are drawn with replacement.\" [:br] \"\" [:br] \"    oob_score : bool, default=False\" [:br] \"        Whether to use out-of-bag samples to estimate\" [:br] \"        the generalization error.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to True, reuse the solution of the previous call to fit\" [:br] \"        and add more estimators to the ensemble, otherwise, just fit\" [:br] \"        a whole new ensemble. See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"           *warm_start* constructor parameter.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of jobs to run in parallel for both :meth:`fit` and\" [:br] \"        :meth:`predict`. ``None`` means 1 unless in a\" [:br] \"        :obj:`joblib.parallel_backend` context. ``-1`` means using all\" [:br] \"        processors. See :term:`Glossary <n_jobs>` for more details.\" [:br] \"\" [:br] \"    random_state : int or RandomState, default=None\" [:br] \"        Controls the random resampling of the original dataset\" [:br] \"        (sample wise and feature wise).\" [:br] \"        If the base estimator accepts a `random_state` attribute, a different\" [:br] \"        seed is generated for each instance in the ensemble.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        Controls the verbosity when fitting and predicting.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    base_estimator_ : estimator\" [:br] \"        The base estimator from which the ensemble is grown.\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        The number of features when :meth:`fit` is performed.\" [:br] \"\" [:br] \"    estimators_ : list of estimators\" [:br] \"        The collection of fitted base estimators.\" [:br] \"\" [:br] \"    estimators_samples_ : list of arrays\" [:br] \"        The subset of drawn samples (i.e., the in-bag samples) for each base\" [:br] \"        estimator. Each subset is defined by an array of the indices selected.\" [:br] \"\" [:br] \"    estimators_features_ : list of arrays\" [:br] \"        The subset of drawn features for each base estimator.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        The classes labels.\" [:br] \"\" [:br] \"    n_classes_ : int or list\" [:br] \"        The number of classes.\" [:br] \"\" [:br] \"    oob_score_ : float\" [:br] \"        Score of the training dataset obtained using an out-of-bag estimate.\" [:br] \"        This attribute exists only when ``oob_score`` is True.\" [:br] \"\" [:br] \"    oob_decision_function_ : ndarray of shape (n_samples, n_classes)\" [:br] \"        Decision function computed with out-of-bag estimate on the training\" [:br] \"        set. If n_estimators is small it might be possible that a data point\" [:br] \"        was never left out during the bootstrap. In this case,\" [:br] \"        `oob_decision_function_` might contain NaN. This attribute exists\" [:br] \"        only when ``oob_score`` is True.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.svm import SVC\" [:br] \"    >>> from sklearn.ensemble import BaggingClassifier\" [:br] \"    >>> from sklearn.datasets import make_classification\" [:br] \"    >>> X, y = make_classification(n_samples=100, n_features=4,\" [:br] \"    ...                            n_informative=2, n_redundant=0,\" [:br] \"    ...                            random_state=0, shuffle=False)\" [:br] \"    >>> clf = BaggingClassifier(base_estimator=SVC(),\" [:br] \"    ...                         n_estimators=10, random_state=0).fit(X, y)\" [:br] \"    >>> clf.predict([[0, 0, 0, 0]])\" [:br] \"    array([1])\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"\" [:br] \"    .. [1] L. Breiman, \\\"Pasting small votes for classification in large\" [:br] \"           databases and on-line\\\", Machine Learning, 36(1), 85-103, 1999.\" [:br] \"\" [:br] \"    .. [2] L. Breiman, \\\"Bagging predictors\\\", Machine Learning, 24(2), 123-140,\" [:br] \"           1996.\" [:br] \"\" [:br] \"    .. [3] T. Ho, \\\"The random subspace method for constructing decision\" [:br] \"           forests\\\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\" [:br] \"           1998.\" [:br] \"\" [:br] \"    .. [4] G. Louppe and P. Geurts, \\\"Ensembles on Random Patches\\\", Machine\" [:br] \"           Learning and Knowledge Discovery in Databases, 346-361, 2012.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/bernoulli-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"276px\"}} [:p/markdown \"_unnamed [4 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |    1.000 |\\n|    :binarize |    0.000 |\\n| :class-prior |          |\\n|   :fit-prior |     true |\\n\"]]] [:span (\"Naive Bayes classifier for multivariate Bernoulli models.\" [:br] \"\" [:br] \"    Like MultinomialNB, this classifier is suitable for discrete data. The\" [:br] \"    difference is that while MultinomialNB works with occurrence counts,\" [:br] \"    BernoulliNB is designed for binary/boolean features.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <bernoulli_naive_bayes>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : float, default=1.0\" [:br] \"        Additive (Laplace/Lidstone) smoothing parameter\" [:br] \"        (0 for no smoothing).\" [:br] \"\" [:br] \"    binarize : float or None, default=0.0\" [:br] \"        Threshold for binarizing (mapping to booleans) of sample features.\" [:br] \"        If None, input is presumed to already consist of binary vectors.\" [:br] \"\" [:br] \"    fit_prior : bool, default=True\" [:br] \"        Whether to learn class prior probabilities or not.\" [:br] \"        If false, a uniform prior will be used.\" [:br] \"\" [:br] \"    class_prior : array-like of shape (n_classes,), default=None\" [:br] \"        Prior probabilities of the classes. If specified the priors are not\" [:br] \"        adjusted according to the data.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    class_count_ : ndarray of shape (n_classes)\" [:br] \"        Number of samples encountered for each class during fitting. This\" [:br] \"        value is weighted by the sample weight when provided.\" [:br] \"\" [:br] \"    class_log_prior_ : ndarray of shape (n_classes)\" [:br] \"        Log probability of each class (smoothed).\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        Class labels known to the classifier\" [:br] \"\" [:br] \"    feature_count_ : ndarray of shape (n_classes, n_features)\" [:br] \"        Number of samples encountered for each (class, feature)\" [:br] \"        during fitting. This value is weighted by the sample weight when\" [:br] \"        provided.\" [:br] \"\" [:br] \"    feature_log_prob_ : ndarray of shape (n_classes, n_features)\" [:br] \"        Empirical log probability of features given a class, P(x_i|y).\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        Number of features of each sample.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> rng = np.random.RandomState(1)\" [:br] \"    >>> X = rng.randint(5, size=(6, 100))\" [:br] \"    >>> Y = np.array([1, 2, 3, 4, 4, 5])\" [:br] \"    >>> from sklearn.naive_bayes import BernoulliNB\" [:br] \"    >>> clf = BernoulliNB()\" [:br] \"    >>> clf.fit(X, Y)\" [:br] \"    BernoulliNB()\" [:br] \"    >>> print(clf.predict(X[2:3]))\" [:br] \"    [3]\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\" [:br] \"    Information Retrieval. Cambridge University Press, pp. 234-265.\" [:br] \"    https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\" [:br] \"\" [:br] \"    A. McCallum and K. Nigam (1998). A comparison of event models for naive\" [:br] \"    Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\" [:br] \"    Text Categorization, pp. 41-48.\" [:br] \"\" [:br] \"    V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\" [:br] \"    naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/calibrated-classifier-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"230px\"}} [:p/markdown \"_unnamed [3 2]:\\n\\n|           :name | :default |\\n|-----------------|----------|\\n| :base-estimator |          |\\n|             :cv |          |\\n|         :method |  sigmoid |\\n\"]]] [:span (\"Probability calibration with isotonic regression or logistic regression.\" [:br] \"\" [:br] \"    The calibration is based on the :term:`decision_function` method of the\" [:br] \"    `base_estimator` if it exists, else on :term:`predict_proba`.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <calibration>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    base_estimator : instance BaseEstimator\" [:br] \"        The classifier whose output need to be calibrated to provide more\" [:br] \"        accurate `predict_proba` outputs.\" [:br] \"\" [:br] \"    method : 'sigmoid' or 'isotonic'\" [:br] \"        The method to use for calibration. Can be 'sigmoid' which\" [:br] \"        corresponds to Platt's method (i.e. a logistic regression model) or\" [:br] \"        'isotonic' which is a non-parametric approach. It is not advised to\" [:br] \"        use isotonic calibration with too few calibration samples\" [:br] \"        ``(<<1000)`` since it tends to overfit.\" [:br] \"\" [:br] \"    cv : integer, cross-validation generator, iterable or \\\"prefit\\\", optional\" [:br] \"        Determines the cross-validation splitting strategy.\" [:br] \"        Possible inputs for cv are:\" [:br] \"\" [:br] \"        - None, to use the default 5-fold cross-validation,\" [:br] \"        - integer, to specify the number of folds.\" [:br] \"        - :term:`CV splitter`,\" [:br] \"        - An iterable yielding (train, test) splits as arrays of indices.\" [:br] \"\" [:br] \"        For integer/None inputs, if ``y`` is binary or multiclass,\" [:br] \"        :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\" [:br] \"        neither binary nor multiclass, :class:`sklearn.model_selection.KFold`\" [:br] \"        is used.\" [:br] \"\" [:br] \"        Refer :ref:`User Guide <cross_validation>` for the various\" [:br] \"        cross-validation strategies that can be used here.\" [:br] \"\" [:br] \"        If \\\"prefit\\\" is passed, it is assumed that `base_estimator` has been\" [:br] \"        fitted already and all data is used for calibration.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"            ``cv`` default value if None changed from 3-fold to 5-fold.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    classes_ : array, shape (n_classes)\" [:br] \"        The class labels.\" [:br] \"\" [:br] \"    calibrated_classifiers_ : list (len() equal to cv or 1 if cv == \\\"prefit\\\")\" [:br] \"        The list of calibrated classifiers, one for each cross-validation fold,\" [:br] \"        which has been fitted on all but the validation fold and calibrated\" [:br] \"        on the validation fold.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    .. [1] Obtaining calibrated probability estimates from decision trees\" [:br] \"           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\" [:br] \"\" [:br] \"    .. [2] Transforming Classifier Scores into Accurate Multiclass\" [:br] \"           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\" [:br] \"\" [:br] \"    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\" [:br] \"           Regularized Likelihood Methods, J. Platt, (1999)\" [:br] \"\" [:br] \"    .. [4] Predicting Good Probabilities with Supervised Learning,\" [:br] \"           A. Niculescu-Mizil & R. Caruana, ICML 2005\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/categorical-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"230px\"}} [:p/markdown \"_unnamed [3 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |    1.000 |\\n| :class-prior |          |\\n|   :fit-prior |     true |\\n\"]]] [:span (\"Naive Bayes classifier for categorical features\" [:br] \"\" [:br] \"    The categorical Naive Bayes classifier is suitable for classification with\" [:br] \"    discrete features that are categorically distributed. The categories of\" [:br] \"    each feature are drawn from a categorical distribution.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <categorical_naive_bayes>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : float, default=1.0\" [:br] \"        Additive (Laplace/Lidstone) smoothing parameter\" [:br] \"        (0 for no smoothing).\" [:br] \"\" [:br] \"    fit_prior : bool, default=True\" [:br] \"        Whether to learn class prior probabilities or not.\" [:br] \"        If false, a uniform prior will be used.\" [:br] \"\" [:br] \"    class_prior : array-like of shape (n_classes,), default=None\" [:br] \"        Prior probabilities of the classes. If specified the priors are not\" [:br] \"        adjusted according to the data.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    category_count_ : list of arrays of shape (n_features,)\" [:br] \"        Holds arrays of shape (n_classes, n_categories of respective feature)\" [:br] \"        for each feature. Each array provides the number of samples\" [:br] \"        encountered for each class and category of the specific feature.\" [:br] \"\" [:br] \"    class_count_ : ndarray of shape (n_classes,)\" [:br] \"        Number of samples encountered for each class during fitting. This\" [:br] \"        value is weighted by the sample weight when provided.\" [:br] \"\" [:br] \"    class_log_prior_ : ndarray of shape (n_classes,)\" [:br] \"        Smoothed empirical log probability for each class.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        Class labels known to the classifier\" [:br] \"\" [:br] \"    feature_log_prob_ : list of arrays of shape (n_features,)\" [:br] \"        Holds arrays of shape (n_classes, n_categories of respective feature)\" [:br] \"        for each feature. Each array provides the empirical log probability\" [:br] \"        of categories given the respective feature and class, ``P(x_i|y)``.\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        Number of features of each sample.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> rng = np.random.RandomState(1)\" [:br] \"    >>> X = rng.randint(5, size=(6, 100))\" [:br] \"    >>> y = np.array([1, 2, 3, 4, 5, 6])\" [:br] \"    >>> from sklearn.naive_bayes import CategoricalNB\" [:br] \"    >>> clf = CategoricalNB()\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    CategoricalNB()\" [:br] \"    >>> print(clf.predict(X[2:3]))\" [:br] \"    [3]\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/complement-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"276px\"}} [:p/markdown \"_unnamed [4 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |    1.000 |\\n| :class-prior |          |\\n|   :fit-prior |     true |\\n|        :norm |    false |\\n\"]]] [:span (\"The Complement Naive Bayes classifier described in Rennie et al. (2003).\" [:br] \"\" [:br] \"    The Complement Naive Bayes classifier was designed to correct the \\\"severe\" [:br] \"    assumptions\\\" made by the standard Multinomial Naive Bayes classifier. It is\" [:br] \"    particularly suited for imbalanced data sets.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <complement_naive_bayes>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.20\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : float, default=1.0\" [:br] \"        Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\" [:br] \"\" [:br] \"    fit_prior : bool, default=True\" [:br] \"        Only used in edge case with a single class in the training set.\" [:br] \"\" [:br] \"    class_prior : array-like of shape (n_classes,), default=None\" [:br] \"        Prior probabilities of the classes. Not used.\" [:br] \"\" [:br] \"    norm : bool, default=False\" [:br] \"        Whether or not a second normalization of the weights is performed. The\" [:br] \"        default behavior mirrors the implementations found in Mahout and Weka,\" [:br] \"        which do not follow the full algorithm described in Table 9 of the\" [:br] \"        paper.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    class_count_ : ndarray of shape (n_classes,)\" [:br] \"        Number of samples encountered for each class during fitting. This\" [:br] \"        value is weighted by the sample weight when provided.\" [:br] \"\" [:br] \"    class_log_prior_ : ndarray of shape (n_classes,)\" [:br] \"        Smoothed empirical log probability for each class. Only used in edge\" [:br] \"        case with a single class in the training set.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        Class labels known to the classifier\" [:br] \"\" [:br] \"    feature_all_ : ndarray of shape (n_features,)\" [:br] \"        Number of samples encountered for each feature during fitting. This\" [:br] \"        value is weighted by the sample weight when provided.\" [:br] \"\" [:br] \"    feature_count_ : ndarray of shape (n_classes, n_features)\" [:br] \"        Number of samples encountered for each (class, feature) during fitting.\" [:br] \"        This value is weighted by the sample weight when provided.\" [:br] \"\" [:br] \"    feature_log_prob_ : ndarray of shape (n_classes, n_features)\" [:br] \"        Empirical weights for class complements.\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        Number of features of each sample.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> rng = np.random.RandomState(1)\" [:br] \"    >>> X = rng.randint(5, size=(6, 100))\" [:br] \"    >>> y = np.array([1, 2, 3, 4, 5, 6])\" [:br] \"    >>> from sklearn.naive_bayes import ComplementNB\" [:br] \"    >>> clf = ComplementNB()\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    ComplementNB()\" [:br] \"    >>> print(clf.predict(X[2:3]))\" [:br] \"    [3]\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\" [:br] \"    Tackling the poor assumptions of naive bayes text classifiers. In ICML\" [:br] \"    (Vol. 3, pp. 616-623).\" [:br] \"    https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/decision-tree-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [14 2]:\\n\\n|                     :name |   :default |\\n|---------------------------|------------|\\n| :min-weight-fraction-leaf |      0.000 |\\n|           :max-leaf-nodes |            |\\n|    :min-impurity-decrease |      0.000 |\\n|        :min-samples-split |      2.000 |\\n|                  :presort | deprecated |\\n|                :ccp-alpha |      0.000 |\\n|                 :splitter |       best |\\n|             :random-state |            |\\n|         :min-samples-leaf |          1 |\\n|             :max-features |            |\\n|       :min-impurity-split |            |\\n|                :max-depth |            |\\n|             :class-weight |            |\\n|                :criterion |       gini |\\n\"]]] [:span (\"A decision tree classifier.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <tree>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    criterion : {\\\"gini\\\", \\\"entropy\\\"}, default=\\\"gini\\\"\" [:br] \"        The function to measure the quality of a split. Supported criteria are\" [:br] \"        \\\"gini\\\" for the Gini impurity and \\\"entropy\\\" for the information gain.\" [:br] \"\" [:br] \"    splitter : {\\\"best\\\", \\\"random\\\"}, default=\\\"best\\\"\" [:br] \"        The strategy used to choose the split at each node. Supported\" [:br] \"        strategies are \\\"best\\\" to choose the best split and \\\"random\\\" to choose\" [:br] \"        the best random split.\" [:br] \"\" [:br] \"    max_depth : int, default=None\" [:br] \"        The maximum depth of the tree. If None, then nodes are expanded until\" [:br] \"        all leaves are pure or until all leaves contain less than\" [:br] \"        min_samples_split samples.\" [:br] \"\" [:br] \"    min_samples_split : int or float, default=2\" [:br] \"        The minimum number of samples required to split an internal node:\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_split` as the minimum number.\" [:br] \"        - If float, then `min_samples_split` is a fraction and\" [:br] \"          `ceil(min_samples_split * n_samples)` are the minimum\" [:br] \"          number of samples for each split.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_samples_leaf : int or float, default=1\" [:br] \"        The minimum number of samples required to be at a leaf node.\" [:br] \"        A split point at any depth will only be considered if it leaves at\" [:br] \"        least ``min_samples_leaf`` training samples in each of the left and\" [:br] \"        right branches.  This may have the effect of smoothing the model,\" [:br] \"        especially in regression.\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_leaf` as the minimum number.\" [:br] \"        - If float, then `min_samples_leaf` is a fraction and\" [:br] \"          `ceil(min_samples_leaf * n_samples)` are the minimum\" [:br] \"          number of samples for each node.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_weight_fraction_leaf : float, default=0.0\" [:br] \"        The minimum weighted fraction of the sum total of weights (of all\" [:br] \"        the input samples) required to be at a leaf node. Samples have\" [:br] \"        equal weight when sample_weight is not provided.\" [:br] \"\" [:br] \"    max_features : int, float or {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"}, default=None\" [:br] \"        The number of features to consider when looking for the best split:\" [:br] \"\" [:br] \"            - If int, then consider `max_features` features at each split.\" [:br] \"            - If float, then `max_features` is a fraction and\" [:br] \"              `int(max_features * n_features)` features are considered at each\" [:br] \"              split.\" [:br] \"            - If \\\"auto\\\", then `max_features=sqrt(n_features)`.\" [:br] \"            - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\" [:br] \"            - If \\\"log2\\\", then `max_features=log2(n_features)`.\" [:br] \"            - If None, then `max_features=n_features`.\" [:br] \"\" [:br] \"        Note: the search for a split does not stop until at least one\" [:br] \"        valid partition of the node samples is found, even if it requires to\" [:br] \"        effectively inspect more than ``max_features`` features.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Controls the randomness of the estimator. The features are always\" [:br] \"        randomly permuted at each split, even if ``splitter`` is set to\" [:br] \"        ``\\\"best\\\"``. When ``max_features < n_features``, the algorithm will\" [:br] \"        select ``max_features`` at random at each split before finding the best\" [:br] \"        split among them. But the best found split may vary across different\" [:br] \"        runs, even if ``max_features=n_features``. That is the case, if the\" [:br] \"        improvement of the criterion is identical for several splits and one\" [:br] \"        split has to be selected at random. To obtain a deterministic behaviour\" [:br] \"        during fitting, ``random_state`` has to be fixed to an integer.\" [:br] \"        See :term:`Glossary <random_state>` for details.\" [:br] \"\" [:br] \"    max_leaf_nodes : int, default=None\" [:br] \"        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\" [:br] \"        Best nodes are defined as relative reduction in impurity.\" [:br] \"        If None then unlimited number of leaf nodes.\" [:br] \"\" [:br] \"    min_impurity_decrease : float, default=0.0\" [:br] \"        A node will be split if this split induces a decrease of the impurity\" [:br] \"        greater than or equal to this value.\" [:br] \"\" [:br] \"        The weighted impurity decrease equation is the following::\" [:br] \"\" [:br] \"            N_t / N * (impurity - N_t_R / N_t * right_impurity\" [:br] \"                                - N_t_L / N_t * left_impurity)\" [:br] \"\" [:br] \"        where ``N`` is the total number of samples, ``N_t`` is the number of\" [:br] \"        samples at the current node, ``N_t_L`` is the number of samples in the\" [:br] \"        left child, and ``N_t_R`` is the number of samples in the right child.\" [:br] \"\" [:br] \"        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\" [:br] \"        if ``sample_weight`` is passed.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    min_impurity_split : float, default=0\" [:br] \"        Threshold for early stopping in tree growth. A node will split\" [:br] \"        if its impurity is above the threshold, otherwise it is a leaf.\" [:br] \"\" [:br] \"        .. deprecated:: 0.19\" [:br] \"           ``min_impurity_split`` has been deprecated in favor of\" [:br] \"           ``min_impurity_decrease`` in 0.19. The default value of\" [:br] \"           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\" [:br] \"           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\" [:br] \"\" [:br] \"    class_weight : dict, list of dict or \\\"balanced\\\", default=None\" [:br] \"        Weights associated with classes in the form ``{class_label: weight}``.\" [:br] \"        If None, all classes are supposed to have weight one. For\" [:br] \"        multi-output problems, a list of dicts can be provided in the same\" [:br] \"        order as the columns of y.\" [:br] \"\" [:br] \"        Note that for multioutput (including multilabel) weights should be\" [:br] \"        defined for each class of every column in its own dict. For example,\" [:br] \"        for four-class multilabel classification weights should be\" [:br] \"        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\" [:br] \"        [{1:1}, {2:5}, {3:1}, {4:1}].\" [:br] \"\" [:br] \"        The \\\"balanced\\\" mode uses the values of y to automatically adjust\" [:br] \"        weights inversely proportional to class frequencies in the input data\" [:br] \"        as ``n_samples / (n_classes * np.bincount(y))``\" [:br] \"\" [:br] \"        For multi-output, the weights of each column of y will be multiplied.\" [:br] \"\" [:br] \"        Note that these weights will be multiplied with sample_weight (passed\" [:br] \"        through the fit method) if sample_weight is specified.\" [:br] \"\" [:br] \"    presort : deprecated, default='deprecated'\" [:br] \"        This parameter is deprecated and will be removed in v0.24.\" [:br] \"\" [:br] \"        .. deprecated:: 0.22\" [:br] \"\" [:br] \"    ccp_alpha : non-negative float, default=0.0\" [:br] \"        Complexity parameter used for Minimal Cost-Complexity Pruning. The\" [:br] \"        subtree with the largest cost complexity that is smaller than\" [:br] \"        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\" [:br] \"        :ref:`minimal_cost_complexity_pruning` for details.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    classes_ : ndarray of shape (n_classes,) or list of ndarray\" [:br] \"        The classes labels (single output problem),\" [:br] \"        or a list of arrays of class labels (multi-output problem).\" [:br] \"\" [:br] \"    feature_importances_ : ndarray of shape (n_features,)\" [:br] \"        The impurity-based feature importances.\" [:br] \"        The higher, the more important the feature.\" [:br] \"        The importance of a feature is computed as the (normalized)\" [:br] \"        total reduction of the criterion brought by that feature.  It is also\" [:br] \"        known as the Gini importance [4]_.\" [:br] \"\" [:br] \"        Warning: impurity-based feature importances can be misleading for\" [:br] \"        high cardinality features (many unique values). See\" [:br] \"        :func:`sklearn.inspection.permutation_importance` as an alternative.\" [:br] \"\" [:br] \"    max_features_ : int\" [:br] \"        The inferred value of max_features.\" [:br] \"\" [:br] \"    n_classes_ : int or list of int\" [:br] \"        The number of classes (for single output problems),\" [:br] \"        or a list containing the number of classes for each\" [:br] \"        output (for multi-output problems).\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        The number of features when ``fit`` is performed.\" [:br] \"\" [:br] \"    n_outputs_ : int\" [:br] \"        The number of outputs when ``fit`` is performed.\" [:br] \"\" [:br] \"    tree_ : Tree\" [:br] \"        The underlying Tree object. Please refer to\" [:br] \"        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\" [:br] \"        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\" [:br] \"        for basic usage of these attributes.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    DecisionTreeRegressor : A decision tree regressor.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The default values for the parameters controlling the size of the trees\" [:br] \"    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\" [:br] \"    unpruned trees which can potentially be very large on some data sets. To\" [:br] \"    reduce memory consumption, the complexity and size of the trees should be\" [:br] \"    controlled by setting those parameter values.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"\" [:br] \"    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\" [:br] \"\" [:br] \"    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \\\"Classification\" [:br] \"           and Regression Trees\\\", Wadsworth, Belmont, CA, 1984.\" [:br] \"\" [:br] \"    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \\\"Elements of Statistical\" [:br] \"           Learning\\\", Springer, 2009.\" [:br] \"\" [:br] \"    .. [4] L. Breiman, and A. Cutler, \\\"Random Forests\\\",\" [:br] \"           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import load_iris\" [:br] \"    >>> from sklearn.model_selection import cross_val_score\" [:br] \"    >>> from sklearn.tree import DecisionTreeClassifier\" [:br] \"    >>> clf = DecisionTreeClassifier(random_state=0)\" [:br] \"    >>> iris = load_iris()\" [:br] \"    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\" [:br] \"    ...                             # doctest: +SKIP\" [:br] \"    ...\" [:br] \"    array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\" [:br] \"            0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/dummy-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"230px\"}} [:p/markdown \"_unnamed [3 2]:\\n\\n|         :name | :default |\\n|---------------|----------|\\n|     :constant |          |\\n| :random-state |          |\\n|     :strategy |     warn |\\n\"]]] [:span (\"\" [:br] \"    DummyClassifier is a classifier that makes predictions using simple rules.\" [:br] \"\" [:br] \"    This classifier is useful as a simple baseline to compare with other\" [:br] \"    (real) classifiers. Do not use it for real problems.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <dummy_estimators>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.13\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    strategy : str, default=\\\"stratified\\\"\" [:br] \"        Strategy to use to generate predictions.\" [:br] \"\" [:br] \"        * \\\"stratified\\\": generates predictions by respecting the training\" [:br] \"          set's class distribution.\" [:br] \"        * \\\"most_frequent\\\": always predicts the most frequent label in the\" [:br] \"          training set.\" [:br] \"        * \\\"prior\\\": always predicts the class that maximizes the class prior\" [:br] \"          (like \\\"most_frequent\\\") and ``predict_proba`` returns the class prior.\" [:br] \"        * \\\"uniform\\\": generates predictions uniformly at random.\" [:br] \"        * \\\"constant\\\": always predicts a constant label that is provided by\" [:br] \"          the user. This is useful for metrics that evaluate a non-majority\" [:br] \"          class\" [:br] \"\" [:br] \"          .. versionchanged:: 0.22\" [:br] \"             The default value of `strategy` will change to \\\"prior\\\" in version\" [:br] \"             0.24. Starting from version 0.22, a warning will be raised if\" [:br] \"             `strategy` is not explicitly set.\" [:br] \"\" [:br] \"          .. versionadded:: 0.17\" [:br] \"             Dummy Classifier now supports prior fitting strategy using\" [:br] \"             parameter *prior*.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance or None, optional, default=None\" [:br] \"        Controls the randomness to generate the predictions when\" [:br] \"        ``strategy='stratified'`` or ``strategy='uniform'``.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    constant : int or str or array-like of shape (n_outputs,)\" [:br] \"        The explicit constant as predicted by the \\\"constant\\\" strategy. This\" [:br] \"        parameter is useful only for the \\\"constant\\\" strategy.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    classes_ : array or list of array of shape (n_classes,)\" [:br] \"        Class labels for each output.\" [:br] \"\" [:br] \"    n_classes_ : array or list of array of shape (n_classes,)\" [:br] \"        Number of label for each output.\" [:br] \"\" [:br] \"    class_prior_ : array or list of array of shape (n_classes,)\" [:br] \"        Probability of each class for each output.\" [:br] \"\" [:br] \"    n_outputs_ : int,\" [:br] \"        Number of outputs.\" [:br] \"\" [:br] \"    sparse_output_ : bool,\" [:br] \"        True if the array returned from predict is to be in sparse CSC format.\" [:br] \"        Is automatically set to True if the input y is passed in sparse format.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> from sklearn.dummy import DummyClassifier\" [:br] \"    >>> X = np.array([-1, 1, 1, 1])\" [:br] \"    >>> y = np.array([0, 1, 1, 1])\" [:br] \"    >>> dummy_clf = DummyClassifier(strategy=\\\"most_frequent\\\")\" [:br] \"    >>> dummy_clf.fit(X, y)\" [:br] \"    DummyClassifier(strategy='most_frequent')\" [:br] \"    >>> dummy_clf.predict(X)\" [:br] \"    array([1, 1, 1, 1])\" [:br] \"    >>> dummy_clf.score(X, y)\" [:br] \"    0.75\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/extra-tree-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [13 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :ccp-alpha |    0.000 |\\n|                 :splitter |   random |\\n|             :random-state |          |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     auto |\\n|       :min-impurity-split |          |\\n|                :max-depth |          |\\n|             :class-weight |          |\\n|                :criterion |     gini |\\n\"]]] [:span (\"An extremely randomized tree classifier.\" [:br] \"\" [:br] \"    Extra-trees differ from classic decision trees in the way they are built.\" [:br] \"    When looking for the best split to separate the samples of a node into two\" [:br] \"    groups, random splits are drawn for each of the `max_features` randomly\" [:br] \"    selected features and the best split among those is chosen. When\" [:br] \"    `max_features` is set 1, this amounts to building a totally random\" [:br] \"    decision tree.\" [:br] \"\" [:br] \"    Warning: Extra-trees should only be used within ensemble methods.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <tree>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    criterion : {\\\"gini\\\", \\\"entropy\\\"}, default=\\\"gini\\\"\" [:br] \"        The function to measure the quality of a split. Supported criteria are\" [:br] \"        \\\"gini\\\" for the Gini impurity and \\\"entropy\\\" for the information gain.\" [:br] \"\" [:br] \"    splitter : {\\\"random\\\", \\\"best\\\"}, default=\\\"random\\\"\" [:br] \"        The strategy used to choose the split at each node. Supported\" [:br] \"        strategies are \\\"best\\\" to choose the best split and \\\"random\\\" to choose\" [:br] \"        the best random split.\" [:br] \"\" [:br] \"    max_depth : int, default=None\" [:br] \"        The maximum depth of the tree. If None, then nodes are expanded until\" [:br] \"        all leaves are pure or until all leaves contain less than\" [:br] \"        min_samples_split samples.\" [:br] \"\" [:br] \"    min_samples_split : int or float, default=2\" [:br] \"        The minimum number of samples required to split an internal node:\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_split` as the minimum number.\" [:br] \"        - If float, then `min_samples_split` is a fraction and\" [:br] \"          `ceil(min_samples_split * n_samples)` are the minimum\" [:br] \"          number of samples for each split.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_samples_leaf : int or float, default=1\" [:br] \"        The minimum number of samples required to be at a leaf node.\" [:br] \"        A split point at any depth will only be considered if it leaves at\" [:br] \"        least ``min_samples_leaf`` training samples in each of the left and\" [:br] \"        right branches.  This may have the effect of smoothing the model,\" [:br] \"        especially in regression.\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_leaf` as the minimum number.\" [:br] \"        - If float, then `min_samples_leaf` is a fraction and\" [:br] \"          `ceil(min_samples_leaf * n_samples)` are the minimum\" [:br] \"          number of samples for each node.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_weight_fraction_leaf : float, default=0.0\" [:br] \"        The minimum weighted fraction of the sum total of weights (of all\" [:br] \"        the input samples) required to be at a leaf node. Samples have\" [:br] \"        equal weight when sample_weight is not provided.\" [:br] \"\" [:br] \"    max_features : int, float, {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"} or None, default=\\\"auto\\\"\" [:br] \"        The number of features to consider when looking for the best split:\" [:br] \"\" [:br] \"            - If int, then consider `max_features` features at each split.\" [:br] \"            - If float, then `max_features` is a fraction and\" [:br] \"              `int(max_features * n_features)` features are considered at each\" [:br] \"              split.\" [:br] \"            - If \\\"auto\\\", then `max_features=sqrt(n_features)`.\" [:br] \"            - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\" [:br] \"            - If \\\"log2\\\", then `max_features=log2(n_features)`.\" [:br] \"            - If None, then `max_features=n_features`.\" [:br] \"\" [:br] \"        Note: the search for a split does not stop until at least one\" [:br] \"        valid partition of the node samples is found, even if it requires to\" [:br] \"        effectively inspect more than ``max_features`` features.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Used to pick randomly the `max_features` used at each split.\" [:br] \"        See :term:`Glossary <random_state>` for details.\" [:br] \"\" [:br] \"    max_leaf_nodes : int, default=None\" [:br] \"        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\" [:br] \"        Best nodes are defined as relative reduction in impurity.\" [:br] \"        If None then unlimited number of leaf nodes.\" [:br] \"\" [:br] \"    min_impurity_decrease : float, default=0.0\" [:br] \"        A node will be split if this split induces a decrease of the impurity\" [:br] \"        greater than or equal to this value.\" [:br] \"\" [:br] \"        The weighted impurity decrease equation is the following::\" [:br] \"\" [:br] \"            N_t / N * (impurity - N_t_R / N_t * right_impurity\" [:br] \"                                - N_t_L / N_t * left_impurity)\" [:br] \"\" [:br] \"        where ``N`` is the total number of samples, ``N_t`` is the number of\" [:br] \"        samples at the current node, ``N_t_L`` is the number of samples in the\" [:br] \"        left child, and ``N_t_R`` is the number of samples in the right child.\" [:br] \"\" [:br] \"        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\" [:br] \"        if ``sample_weight`` is passed.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    min_impurity_split : float, (default=0)\" [:br] \"        Threshold for early stopping in tree growth. A node will split\" [:br] \"        if its impurity is above the threshold, otherwise it is a leaf.\" [:br] \"\" [:br] \"        .. deprecated:: 0.19\" [:br] \"           ``min_impurity_split`` has been deprecated in favor of\" [:br] \"           ``min_impurity_decrease`` in 0.19. The default value of\" [:br] \"           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\" [:br] \"           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\" [:br] \"\" [:br] \"    class_weight : dict, list of dict or \\\"balanced\\\", default=None\" [:br] \"        Weights associated with classes in the form ``{class_label: weight}``.\" [:br] \"        If None, all classes are supposed to have weight one. For\" [:br] \"        multi-output problems, a list of dicts can be provided in the same\" [:br] \"        order as the columns of y.\" [:br] \"\" [:br] \"        Note that for multioutput (including multilabel) weights should be\" [:br] \"        defined for each class of every column in its own dict. For example,\" [:br] \"        for four-class multilabel classification weights should be\" [:br] \"        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\" [:br] \"        [{1:1}, {2:5}, {3:1}, {4:1}].\" [:br] \"\" [:br] \"        The \\\"balanced\\\" mode uses the values of y to automatically adjust\" [:br] \"        weights inversely proportional to class frequencies in the input data\" [:br] \"        as ``n_samples / (n_classes * np.bincount(y))``\" [:br] \"\" [:br] \"        For multi-output, the weights of each column of y will be multiplied.\" [:br] \"\" [:br] \"        Note that these weights will be multiplied with sample_weight (passed\" [:br] \"        through the fit method) if sample_weight is specified.\" [:br] \"\" [:br] \"    ccp_alpha : non-negative float, default=0.0\" [:br] \"        Complexity parameter used for Minimal Cost-Complexity Pruning. The\" [:br] \"        subtree with the largest cost complexity that is smaller than\" [:br] \"        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\" [:br] \"        :ref:`minimal_cost_complexity_pruning` for details.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    classes_ : ndarray of shape (n_classes,) or list of ndarray\" [:br] \"        The classes labels (single output problem),\" [:br] \"        or a list of arrays of class labels (multi-output problem).\" [:br] \"\" [:br] \"    max_features_ : int\" [:br] \"        The inferred value of max_features.\" [:br] \"\" [:br] \"    n_classes_ : int or list of int\" [:br] \"        The number of classes (for single output problems),\" [:br] \"        or a list containing the number of classes for each\" [:br] \"        output (for multi-output problems).\" [:br] \"\" [:br] \"    feature_importances_ : ndarray of shape (n_features,)\" [:br] \"        The impurity-based feature importances.\" [:br] \"        The higher, the more important the feature.\" [:br] \"        The importance of a feature is computed as the (normalized)\" [:br] \"        total reduction of the criterion brought by that feature.  It is also\" [:br] \"        known as the Gini importance.\" [:br] \"\" [:br] \"        Warning: impurity-based feature importances can be misleading for\" [:br] \"        high cardinality features (many unique values). See\" [:br] \"        :func:`sklearn.inspection.permutation_importance` as an alternative.\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        The number of features when ``fit`` is performed.\" [:br] \"\" [:br] \"    n_outputs_ : int\" [:br] \"        The number of outputs when ``fit`` is performed.\" [:br] \"\" [:br] \"    tree_ : Tree\" [:br] \"        The underlying Tree object. Please refer to\" [:br] \"        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\" [:br] \"        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\" [:br] \"        for basic usage of these attributes.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    ExtraTreeRegressor : An extremely randomized tree regressor.\" [:br] \"    sklearn.ensemble.ExtraTreesClassifier : An extra-trees classifier.\" [:br] \"    sklearn.ensemble.ExtraTreesRegressor : An extra-trees regressor.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The default values for the parameters controlling the size of the trees\" [:br] \"    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\" [:br] \"    unpruned trees which can potentially be very large on some data sets. To\" [:br] \"    reduce memory consumption, the complexity and size of the trees should be\" [:br] \"    controlled by setting those parameter values.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"\" [:br] \"    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized trees\\\",\" [:br] \"           Machine Learning, 63(1), 3-42, 2006.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import load_iris\" [:br] \"    >>> from sklearn.model_selection import train_test_split\" [:br] \"    >>> from sklearn.ensemble import BaggingClassifier\" [:br] \"    >>> from sklearn.tree import ExtraTreeClassifier\" [:br] \"    >>> X, y = load_iris(return_X_y=True)\" [:br] \"    >>> X_train, X_test, y_train, y_test = train_test_split(\" [:br] \"    ...    X, y, random_state=0)\" [:br] \"    >>> extra_tree = ExtraTreeClassifier(random_state=0)\" [:br] \"    >>> cls = BaggingClassifier(extra_tree, random_state=0).fit(\" [:br] \"    ...    X_train, y_train)\" [:br] \"    >>> cls.score(X_test, y_test)\" [:br] \"    0.8947...\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/extra-trees-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [19 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :bootstrap |    false |\\n|                :ccp-alpha |    0.000 |\\n|                   :n-jobs |          |\\n|             :random-state |          |\\n|                :oob-score |    false |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     auto |\\n|       :min-impurity-split |          |\\n|               :warm-start |    false |\\n|                :max-depth |          |\\n|             :class-weight |          |\\n|             :n-estimators |      100 |\\n|              :max-samples |          |\\n|                :criterion |     gini |\\n|                  :verbose |        0 |\\n\"]]] [:span (\"\" [:br] \"    An extra-trees classifier.\" [:br] \"\" [:br] \"    This class implements a meta estimator that fits a number of\" [:br] \"    randomized decision trees (a.k.a. extra-trees) on various sub-samples\" [:br] \"    of the dataset and uses averaging to improve the predictive accuracy\" [:br] \"    and control over-fitting.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <forest>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    n_estimators : int, default=100\" [:br] \"        The number of trees in the forest.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"           The default value of ``n_estimators`` changed from 10 to 100\" [:br] \"           in 0.22.\" [:br] \"\" [:br] \"    criterion : {\\\"gini\\\", \\\"entropy\\\"}, default=\\\"gini\\\"\" [:br] \"        The function to measure the quality of a split. Supported criteria are\" [:br] \"        \\\"gini\\\" for the Gini impurity and \\\"entropy\\\" for the information gain.\" [:br] \"\" [:br] \"    max_depth : int, default=None\" [:br] \"        The maximum depth of the tree. If None, then nodes are expanded until\" [:br] \"        all leaves are pure or until all leaves contain less than\" [:br] \"        min_samples_split samples.\" [:br] \"\" [:br] \"    min_samples_split : int or float, default=2\" [:br] \"        The minimum number of samples required to split an internal node:\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_split` as the minimum number.\" [:br] \"        - If float, then `min_samples_split` is a fraction and\" [:br] \"          `ceil(min_samples_split * n_samples)` are the minimum\" [:br] \"          number of samples for each split.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_samples_leaf : int or float, default=1\" [:br] \"        The minimum number of samples required to be at a leaf node.\" [:br] \"        A split point at any depth will only be considered if it leaves at\" [:br] \"        least ``min_samples_leaf`` training samples in each of the left and\" [:br] \"        right branches.  This may have the effect of smoothing the model,\" [:br] \"        especially in regression.\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_leaf` as the minimum number.\" [:br] \"        - If float, then `min_samples_leaf` is a fraction and\" [:br] \"          `ceil(min_samples_leaf * n_samples)` are the minimum\" [:br] \"          number of samples for each node.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_weight_fraction_leaf : float, default=0.0\" [:br] \"        The minimum weighted fraction of the sum total of weights (of all\" [:br] \"        the input samples) required to be at a leaf node. Samples have\" [:br] \"        equal weight when sample_weight is not provided.\" [:br] \"\" [:br] \"    max_features : {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"}, int or float, default=\\\"auto\\\"\" [:br] \"        The number of features to consider when looking for the best split:\" [:br] \"\" [:br] \"        - If int, then consider `max_features` features at each split.\" [:br] \"        - If float, then `max_features` is a fraction and\" [:br] \"          `int(max_features * n_features)` features are considered at each\" [:br] \"          split.\" [:br] \"        - If \\\"auto\\\", then `max_features=sqrt(n_features)`.\" [:br] \"        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\" [:br] \"        - If \\\"log2\\\", then `max_features=log2(n_features)`.\" [:br] \"        - If None, then `max_features=n_features`.\" [:br] \"\" [:br] \"        Note: the search for a split does not stop until at least one\" [:br] \"        valid partition of the node samples is found, even if it requires to\" [:br] \"        effectively inspect more than ``max_features`` features.\" [:br] \"\" [:br] \"    max_leaf_nodes : int, default=None\" [:br] \"        Grow trees with ``max_leaf_nodes`` in best-first fashion.\" [:br] \"        Best nodes are defined as relative reduction in impurity.\" [:br] \"        If None then unlimited number of leaf nodes.\" [:br] \"\" [:br] \"    min_impurity_decrease : float, default=0.0\" [:br] \"        A node will be split if this split induces a decrease of the impurity\" [:br] \"        greater than or equal to this value.\" [:br] \"\" [:br] \"        The weighted impurity decrease equation is the following::\" [:br] \"\" [:br] \"            N_t / N * (impurity - N_t_R / N_t * right_impurity\" [:br] \"                                - N_t_L / N_t * left_impurity)\" [:br] \"\" [:br] \"        where ``N`` is the total number of samples, ``N_t`` is the number of\" [:br] \"        samples at the current node, ``N_t_L`` is the number of samples in the\" [:br] \"        left child, and ``N_t_R`` is the number of samples in the right child.\" [:br] \"\" [:br] \"        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\" [:br] \"        if ``sample_weight`` is passed.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    min_impurity_split : float, default=None\" [:br] \"        Threshold for early stopping in tree growth. A node will split\" [:br] \"        if its impurity is above the threshold, otherwise it is a leaf.\" [:br] \"\" [:br] \"        .. deprecated:: 0.19\" [:br] \"           ``min_impurity_split`` has been deprecated in favor of\" [:br] \"           ``min_impurity_decrease`` in 0.19. The default value of\" [:br] \"           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\" [:br] \"           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\" [:br] \"\" [:br] \"    bootstrap : bool, default=False\" [:br] \"        Whether bootstrap samples are used when building trees. If False, the\" [:br] \"        whole dataset is used to build each tree.\" [:br] \"\" [:br] \"    oob_score : bool, default=False\" [:br] \"        Whether to use out-of-bag samples to estimate\" [:br] \"        the generalization accuracy.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\" [:br] \"        :meth:`decision_path` and :meth:`apply` are all parallelized over the\" [:br] \"        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\" [:br] \"        context. ``-1`` means using all processors. See :term:`Glossary\" [:br] \"        <n_jobs>` for more details.\" [:br] \"\" [:br] \"    random_state : int, RandomState, default=None\" [:br] \"        Controls 3 sources of randomness:\" [:br] \"\" [:br] \"        - the bootstrapping of the samples used when building trees\" [:br] \"          (if ``bootstrap=True``)\" [:br] \"        - the sampling of the features to consider when looking for the best\" [:br] \"          split at each node (if ``max_features < n_features``)\" [:br] \"        - the draw of the splits for each of the `max_features`\" [:br] \"\" [:br] \"        See :term:`Glossary <random_state>` for details.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        Controls the verbosity when fitting and predicting.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to ``True``, reuse the solution of the previous call to fit\" [:br] \"        and add more estimators to the ensemble, otherwise, just fit a whole\" [:br] \"        new forest. See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    class_weight : {\\\"balanced\\\", \\\"balanced_subsample\\\"}, dict or list of dicts,             default=None\" [:br] \"        Weights associated with classes in the form ``{class_label: weight}``.\" [:br] \"        If not given, all classes are supposed to have weight one. For\" [:br] \"        multi-output problems, a list of dicts can be provided in the same\" [:br] \"        order as the columns of y.\" [:br] \"\" [:br] \"        Note that for multioutput (including multilabel) weights should be\" [:br] \"        defined for each class of every column in its own dict. For example,\" [:br] \"        for four-class multilabel classification weights should be\" [:br] \"        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\" [:br] \"        [{1:1}, {2:5}, {3:1}, {4:1}].\" [:br] \"\" [:br] \"        The \\\"balanced\\\" mode uses the values of y to automatically adjust\" [:br] \"        weights inversely proportional to class frequencies in the input data\" [:br] \"        as ``n_samples / (n_classes * np.bincount(y))``\" [:br] \"\" [:br] \"        The \\\"balanced_subsample\\\" mode is the same as \\\"balanced\\\" except that\" [:br] \"        weights are computed based on the bootstrap sample for every tree\" [:br] \"        grown.\" [:br] \"\" [:br] \"        For multi-output, the weights of each column of y will be multiplied.\" [:br] \"\" [:br] \"        Note that these weights will be multiplied with sample_weight (passed\" [:br] \"        through the fit method) if sample_weight is specified.\" [:br] \"\" [:br] \"    ccp_alpha : non-negative float, default=0.0\" [:br] \"        Complexity parameter used for Minimal Cost-Complexity Pruning. The\" [:br] \"        subtree with the largest cost complexity that is smaller than\" [:br] \"        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\" [:br] \"        :ref:`minimal_cost_complexity_pruning` for details.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    max_samples : int or float, default=None\" [:br] \"        If bootstrap is True, the number of samples to draw from X\" [:br] \"        to train each base estimator.\" [:br] \"\" [:br] \"        - If None (default), then draw `X.shape[0]` samples.\" [:br] \"        - If int, then draw `max_samples` samples.\" [:br] \"        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\" [:br] \"          `max_samples` should be in the interval `(0, 1)`.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    base_estimator_ : ExtraTreesClassifier\" [:br] \"        The child estimator template used to create the collection of fitted\" [:br] \"        sub-estimators.\" [:br] \"\" [:br] \"    estimators_ : list of DecisionTreeClassifier\" [:br] \"        The collection of fitted sub-estimators.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,) or a list of such arrays\" [:br] \"        The classes labels (single output problem), or a list of arrays of\" [:br] \"        class labels (multi-output problem).\" [:br] \"\" [:br] \"    n_classes_ : int or list\" [:br] \"        The number of classes (single output problem), or a list containing the\" [:br] \"        number of classes for each output (multi-output problem).\" [:br] \"\" [:br] \"    feature_importances_ : ndarray of shape (n_features,)\" [:br] \"        The impurity-based feature importances.\" [:br] \"        The higher, the more important the feature.\" [:br] \"        The importance of a feature is computed as the (normalized)\" [:br] \"        total reduction of the criterion brought by that feature.  It is also\" [:br] \"        known as the Gini importance.\" [:br] \"\" [:br] \"        Warning: impurity-based feature importances can be misleading for\" [:br] \"        high cardinality features (many unique values). See\" [:br] \"        :func:`sklearn.inspection.permutation_importance` as an alternative.\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        The number of features when ``fit`` is performed.\" [:br] \"\" [:br] \"    n_outputs_ : int\" [:br] \"        The number of outputs when ``fit`` is performed.\" [:br] \"\" [:br] \"    oob_score_ : float\" [:br] \"        Score of the training dataset obtained using an out-of-bag estimate.\" [:br] \"        This attribute exists only when ``oob_score`` is True.\" [:br] \"\" [:br] \"    oob_decision_function_ : ndarray of shape (n_samples, n_classes)\" [:br] \"        Decision function computed with out-of-bag estimate on the training\" [:br] \"        set. If n_estimators is small it might be possible that a data point\" [:br] \"        was never left out during the bootstrap. In this case,\" [:br] \"        `oob_decision_function_` might contain NaN. This attribute exists\" [:br] \"        only when ``oob_score`` is True.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.\" [:br] \"    RandomForestClassifier : Ensemble Classifier based on trees with optimal\" [:br] \"        splits.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The default values for the parameters controlling the size of the trees\" [:br] \"    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\" [:br] \"    unpruned trees which can potentially be very large on some data sets. To\" [:br] \"    reduce memory consumption, the complexity and size of the trees should be\" [:br] \"    controlled by setting those parameter values.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized\" [:br] \"           trees\\\", Machine Learning, 63(1), 3-42, 2006.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.ensemble import ExtraTreesClassifier\" [:br] \"    >>> from sklearn.datasets import make_classification\" [:br] \"    >>> X, y = make_classification(n_features=4, random_state=0)\" [:br] \"    >>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    ExtraTreesClassifier(random_state=0)\" [:br] \"    >>> clf.predict([[0, 0, 0, 0]])\" [:br] \"    array([1])\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/gaussian-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"184px\"}} [:p/markdown \"_unnamed [2 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|        :priors |          |\\n| :var-smoothing |  1.0E-09 |\\n\"]]] [:span (\"\" [:br] \"    Gaussian Naive Bayes (GaussianNB)\" [:br] \"\" [:br] \"    Can perform online updates to model parameters via :meth:`partial_fit`.\" [:br] \"    For details on algorithm used to update feature means and variance online,\" [:br] \"    see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\" [:br] \"\" [:br] \"        http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    priors : array-like of shape (n_classes,)\" [:br] \"        Prior probabilities of the classes. If specified the priors are not\" [:br] \"        adjusted according to the data.\" [:br] \"\" [:br] \"    var_smoothing : float, default=1e-9\" [:br] \"        Portion of the largest variance of all features that is added to\" [:br] \"        variances for calculation stability.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    class_count_ : ndarray of shape (n_classes,)\" [:br] \"        number of training samples observed in each class.\" [:br] \"\" [:br] \"    class_prior_ : ndarray of shape (n_classes,)\" [:br] \"        probability of each class.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        class labels known to the classifier\" [:br] \"\" [:br] \"    epsilon_ : float\" [:br] \"        absolute additive value to variances\" [:br] \"\" [:br] \"    sigma_ : ndarray of shape (n_classes, n_features)\" [:br] \"        variance of each feature per class\" [:br] \"\" [:br] \"    theta_ : ndarray of shape (n_classes, n_features)\" [:br] \"        mean of each feature per class\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\" [:br] \"    >>> Y = np.array([1, 1, 1, 2, 2, 2])\" [:br] \"    >>> from sklearn.naive_bayes import GaussianNB\" [:br] \"    >>> clf = GaussianNB()\" [:br] \"    >>> clf.fit(X, Y)\" [:br] \"    GaussianNB()\" [:br] \"    >>> print(clf.predict([[-0.8, -1]]))\" [:br] \"    [1]\" [:br] \"    >>> clf_pf = GaussianNB()\" [:br] \"    >>> clf_pf.partial_fit(X, Y, np.unique(Y))\" [:br] \"    GaussianNB()\" [:br] \"    >>> print(clf_pf.predict([[-0.8, -1]]))\" [:br] \"    [1]\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/gaussian-process-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [9 2]:\\n\\n|                 :name |      :default |\\n|-----------------------|---------------|\\n|               :kernel |               |\\n|            :optimizer | fmin_l_bfgs_b |\\n|          :multi-class |   one_vs_rest |\\n|               :n-jobs |               |\\n|         :random-state |               |\\n|     :max-iter-predict |           100 |\\n|         :copy-x-train |          true |\\n| :n-restarts-optimizer |             0 |\\n|           :warm-start |         false |\\n\"]]] [:span (\"Gaussian process classification (GPC) based on Laplace approximation.\" [:br] \"\" [:br] \"    The implementation is based on Algorithm 3.1, 3.2, and 5.1 of\" [:br] \"    Gaussian Processes for Machine Learning (GPML) by Rasmussen and\" [:br] \"    Williams.\" [:br] \"\" [:br] \"    Internally, the Laplace approximation is used for approximating the\" [:br] \"    non-Gaussian posterior by a Gaussian.\" [:br] \"\" [:br] \"    Currently, the implementation is restricted to using the logistic link\" [:br] \"    function. For multi-class classification, several binary one-versus rest\" [:br] \"    classifiers are fitted. Note that this class thus does not implement\" [:br] \"    a true multi-class Laplace approximation.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <gaussian_process>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    kernel : kernel instance, default=None\" [:br] \"        The kernel specifying the covariance function of the GP. If None is\" [:br] \"        passed, the kernel \\\"1.0 * RBF(1.0)\\\" is used as default. Note that\" [:br] \"        the kernel's hyperparameters are optimized during fitting.\" [:br] \"\" [:br] \"    optimizer : 'fmin_l_bfgs_b' or callable, default='fmin_l_bfgs_b'\" [:br] \"        Can either be one of the internally supported optimizers for optimizing\" [:br] \"        the kernel's parameters, specified by a string, or an externally\" [:br] \"        defined optimizer passed as a callable. If a callable is passed, it\" [:br] \"        must have the  signature::\" [:br] \"\" [:br] \"            def optimizer(obj_func, initial_theta, bounds):\" [:br] \"                # * 'obj_func' is the objective function to be maximized, which\" [:br] \"                #   takes the hyperparameters theta as parameter and an\" [:br] \"                #   optional flag eval_gradient, which determines if the\" [:br] \"                #   gradient is returned additionally to the function value\" [:br] \"                # * 'initial_theta': the initial value for theta, which can be\" [:br] \"                #   used by local optimizers\" [:br] \"                # * 'bounds': the bounds on the values of theta\" [:br] \"                ....\" [:br] \"                # Returned are the best found hyperparameters theta and\" [:br] \"                # the corresponding value of the target function.\" [:br] \"                return theta_opt, func_min\" [:br] \"\" [:br] \"        Per default, the 'L-BFGS-B' algorithm from scipy.optimize.minimize\" [:br] \"        is used. If None is passed, the kernel's parameters are kept fixed.\" [:br] \"        Available internal optimizers are::\" [:br] \"\" [:br] \"            'fmin_l_bfgs_b'\" [:br] \"\" [:br] \"    n_restarts_optimizer : int, default=0\" [:br] \"        The number of restarts of the optimizer for finding the kernel's\" [:br] \"        parameters which maximize the log-marginal likelihood. The first run\" [:br] \"        of the optimizer is performed from the kernel's initial parameters,\" [:br] \"        the remaining ones (if any) from thetas sampled log-uniform randomly\" [:br] \"        from the space of allowed theta-values. If greater than 0, all bounds\" [:br] \"        must be finite. Note that n_restarts_optimizer=0 implies that one\" [:br] \"        run is performed.\" [:br] \"\" [:br] \"    max_iter_predict : int, default=100\" [:br] \"        The maximum number of iterations in Newton's method for approximating\" [:br] \"        the posterior during predict. Smaller values will reduce computation\" [:br] \"        time at the cost of worse results.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        If warm-starts are enabled, the solution of the last Newton iteration\" [:br] \"        on the Laplace approximation of the posterior mode is used as\" [:br] \"        initialization for the next call of _posterior_mode(). This can speed\" [:br] \"        up convergence when _posterior_mode is called several times on similar\" [:br] \"        problems as in hyperparameter optimization. See :term:`the Glossary\" [:br] \"        <warm_start>`.\" [:br] \"\" [:br] \"    copy_X_train : bool, default=True\" [:br] \"        If True, a persistent copy of the training data is stored in the\" [:br] \"        object. Otherwise, just a reference to the training data is stored,\" [:br] \"        which might cause predictions to change if the data is modified\" [:br] \"        externally.\" [:br] \"\" [:br] \"    random_state : int or RandomState, default=None\" [:br] \"        Determines random number generation used to initialize the centers.\" [:br] \"        Pass an int for reproducible results across multiple function calls.\" [:br] \"        See :term: `Glossary <random_state>`.\" [:br] \"\" [:br] \"    multi_class : {'one_vs_rest', 'one_vs_one'}, default='one_vs_rest'\" [:br] \"        Specifies how multi-class classification problems are handled.\" [:br] \"        Supported are 'one_vs_rest' and 'one_vs_one'. In 'one_vs_rest',\" [:br] \"        one binary Gaussian process classifier is fitted for each class, which\" [:br] \"        is trained to separate this class from the rest. In 'one_vs_one', one\" [:br] \"        binary Gaussian process classifier is fitted for each pair of classes,\" [:br] \"        which is trained to separate these two classes. The predictions of\" [:br] \"        these binary predictors are combined into multi-class predictions.\" [:br] \"        Note that 'one_vs_one' does not support predicting probability\" [:br] \"        estimates.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of jobs to use for the computation.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    kernel_ : kernel instance\" [:br] \"        The kernel used for prediction. In case of binary classification,\" [:br] \"        the structure of the kernel is the same as the one passed as parameter\" [:br] \"        but with optimized hyperparameters. In case of multi-class\" [:br] \"        classification, a CompoundKernel is returned which consists of the\" [:br] \"        different kernels used in the one-versus-rest classifiers.\" [:br] \"\" [:br] \"    log_marginal_likelihood_value_ : float\" [:br] \"        The log-marginal-likelihood of ``self.kernel_.theta``\" [:br] \"\" [:br] \"    classes_ : array-like of shape (n_classes,)\" [:br] \"        Unique class labels.\" [:br] \"\" [:br] \"    n_classes_ : int\" [:br] \"        The number of classes in the training data\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import load_iris\" [:br] \"    >>> from sklearn.gaussian_process import GaussianProcessClassifier\" [:br] \"    >>> from sklearn.gaussian_process.kernels import RBF\" [:br] \"    >>> X, y = load_iris(return_X_y=True)\" [:br] \"    >>> kernel = 1.0 * RBF(1.0)\" [:br] \"    >>> gpc = GaussianProcessClassifier(kernel=kernel,\" [:br] \"    ...         random_state=0).fit(X, y)\" [:br] \"    >>> gpc.score(X, y)\" [:br] \"    0.9866...\" [:br] \"    >>> gpc.predict_proba(X[:2,:])\" [:br] \"    array([[0.83548752, 0.03228706, 0.13222543],\" [:br] \"           [0.79064206, 0.06525643, 0.14410151]])\" [:br] \"\" [:br] \"    .. versionadded:: 0.18\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/gradient-boosting-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [22 2]:\\n\\n|                     :name |     :default |\\n|---------------------------|--------------|\\n|         :n-iter-no-change |              |\\n|            :learning-rate |       0.1000 |\\n| :min-weight-fraction-leaf |        0.000 |\\n|           :max-leaf-nodes |              |\\n|    :min-impurity-decrease |        0.000 |\\n|        :min-samples-split |        2.000 |\\n|                      :tol |    0.0001000 |\\n|                  :presort |   deprecated |\\n|                :subsample |        1.000 |\\n|                :ccp-alpha |        0.000 |\\n|             :random-state |              |\\n|         :min-samples-leaf |            1 |\\n|             :max-features |              |\\n|                     :init |              |\\n|       :min-impurity-split |              |\\n|               :warm-start |        false |\\n|                :max-depth |            3 |\\n|      :validation-fraction |       0.1000 |\\n|             :n-estimators |          100 |\\n|                :criterion | friedman_mse |\\n|                     :loss |     deviance |\\n|                  :verbose |            0 |\\n\"]]] [:span (\"Gradient Boosting for classification.\" [:br] \"\" [:br] \"    GB builds an additive model in a\" [:br] \"    forward stage-wise fashion; it allows for the optimization of\" [:br] \"    arbitrary differentiable loss functions. In each stage ``n_classes_``\" [:br] \"    regression trees are fit on the negative gradient of the\" [:br] \"    binomial or multinomial deviance loss function. Binary classification\" [:br] \"    is a special case where only a single regression tree is induced.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <gradient_boosting>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    loss : {'deviance', 'exponential'}, default='deviance'\" [:br] \"        loss function to be optimized. 'deviance' refers to\" [:br] \"        deviance (= logistic regression) for classification\" [:br] \"        with probabilistic outputs. For loss 'exponential' gradient\" [:br] \"        boosting recovers the AdaBoost algorithm.\" [:br] \"\" [:br] \"    learning_rate : float, default=0.1\" [:br] \"        learning rate shrinks the contribution of each tree by `learning_rate`.\" [:br] \"        There is a trade-off between learning_rate and n_estimators.\" [:br] \"\" [:br] \"    n_estimators : int, default=100\" [:br] \"        The number of boosting stages to perform. Gradient boosting\" [:br] \"        is fairly robust to over-fitting so a large number usually\" [:br] \"        results in better performance.\" [:br] \"\" [:br] \"    subsample : float, default=1.0\" [:br] \"        The fraction of samples to be used for fitting the individual base\" [:br] \"        learners. If smaller than 1.0 this results in Stochastic Gradient\" [:br] \"        Boosting. `subsample` interacts with the parameter `n_estimators`.\" [:br] \"        Choosing `subsample < 1.0` leads to a reduction of variance\" [:br] \"        and an increase in bias.\" [:br] \"\" [:br] \"    criterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\" [:br] \"        The function to measure the quality of a split. Supported criteria\" [:br] \"        are 'friedman_mse' for the mean squared error with improvement\" [:br] \"        score by Friedman, 'mse' for mean squared error, and 'mae' for\" [:br] \"        the mean absolute error. The default value of 'friedman_mse' is\" [:br] \"        generally the best as it can provide a better approximation in\" [:br] \"        some cases.\" [:br] \"\" [:br] \"        .. versionadded:: 0.18\" [:br] \"\" [:br] \"    min_samples_split : int or float, default=2\" [:br] \"        The minimum number of samples required to split an internal node:\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_split` as the minimum number.\" [:br] \"        - If float, then `min_samples_split` is a fraction and\" [:br] \"          `ceil(min_samples_split * n_samples)` are the minimum\" [:br] \"          number of samples for each split.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_samples_leaf : int or float, default=1\" [:br] \"        The minimum number of samples required to be at a leaf node.\" [:br] \"        A split point at any depth will only be considered if it leaves at\" [:br] \"        least ``min_samples_leaf`` training samples in each of the left and\" [:br] \"        right branches.  This may have the effect of smoothing the model,\" [:br] \"        especially in regression.\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_leaf` as the minimum number.\" [:br] \"        - If float, then `min_samples_leaf` is a fraction and\" [:br] \"          `ceil(min_samples_leaf * n_samples)` are the minimum\" [:br] \"          number of samples for each node.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_weight_fraction_leaf : float, default=0.0\" [:br] \"        The minimum weighted fraction of the sum total of weights (of all\" [:br] \"        the input samples) required to be at a leaf node. Samples have\" [:br] \"        equal weight when sample_weight is not provided.\" [:br] \"\" [:br] \"    max_depth : int, default=3\" [:br] \"        maximum depth of the individual regression estimators. The maximum\" [:br] \"        depth limits the number of nodes in the tree. Tune this parameter\" [:br] \"        for best performance; the best value depends on the interaction\" [:br] \"        of the input variables.\" [:br] \"\" [:br] \"    min_impurity_decrease : float, default=0.0\" [:br] \"        A node will be split if this split induces a decrease of the impurity\" [:br] \"        greater than or equal to this value.\" [:br] \"\" [:br] \"        The weighted impurity decrease equation is the following::\" [:br] \"\" [:br] \"            N_t / N * (impurity - N_t_R / N_t * right_impurity\" [:br] \"                                - N_t_L / N_t * left_impurity)\" [:br] \"\" [:br] \"        where ``N`` is the total number of samples, ``N_t`` is the number of\" [:br] \"        samples at the current node, ``N_t_L`` is the number of samples in the\" [:br] \"        left child, and ``N_t_R`` is the number of samples in the right child.\" [:br] \"\" [:br] \"        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\" [:br] \"        if ``sample_weight`` is passed.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    min_impurity_split : float, default=None\" [:br] \"        Threshold for early stopping in tree growth. A node will split\" [:br] \"        if its impurity is above the threshold, otherwise it is a leaf.\" [:br] \"\" [:br] \"        .. deprecated:: 0.19\" [:br] \"           ``min_impurity_split`` has been deprecated in favor of\" [:br] \"           ``min_impurity_decrease`` in 0.19. The default value of\" [:br] \"           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\" [:br] \"           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\" [:br] \"\" [:br] \"    init : estimator or 'zero', default=None\" [:br] \"        An estimator object that is used to compute the initial predictions.\" [:br] \"        ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\" [:br] \"        'zero', the initial raw predictions are set to zero. By default, a\" [:br] \"        ``DummyEstimator`` predicting the classes priors is used.\" [:br] \"\" [:br] \"    random_state : int or RandomState, default=None\" [:br] \"        Controls the random seed given to each Tree estimator at each\" [:br] \"        boosting iteration.\" [:br] \"        In addition, it controls the random permutation of the features at\" [:br] \"        each split (see Notes for more details).\" [:br] \"        It also controls the random spliting of the training data to obtain a\" [:br] \"        validation set if `n_iter_no_change` is not None.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\" [:br] \"        The number of features to consider when looking for the best split:\" [:br] \"\" [:br] \"        - If int, then consider `max_features` features at each split.\" [:br] \"        - If float, then `max_features` is a fraction and\" [:br] \"          `int(max_features * n_features)` features are considered at each\" [:br] \"          split.\" [:br] \"        - If 'auto', then `max_features=sqrt(n_features)`.\" [:br] \"        - If 'sqrt', then `max_features=sqrt(n_features)`.\" [:br] \"        - If 'log2', then `max_features=log2(n_features)`.\" [:br] \"        - If None, then `max_features=n_features`.\" [:br] \"\" [:br] \"        Choosing `max_features < n_features` leads to a reduction of variance\" [:br] \"        and an increase in bias.\" [:br] \"\" [:br] \"        Note: the search for a split does not stop until at least one\" [:br] \"        valid partition of the node samples is found, even if it requires to\" [:br] \"        effectively inspect more than ``max_features`` features.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        Enable verbose output. If 1 then it prints progress and performance\" [:br] \"        once in a while (the more trees the lower the frequency). If greater\" [:br] \"        than 1 then it prints progress and performance for every tree.\" [:br] \"\" [:br] \"    max_leaf_nodes : int, default=None\" [:br] \"        Grow trees with ``max_leaf_nodes`` in best-first fashion.\" [:br] \"        Best nodes are defined as relative reduction in impurity.\" [:br] \"        If None then unlimited number of leaf nodes.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to ``True``, reuse the solution of the previous call to fit\" [:br] \"        and add more estimators to the ensemble, otherwise, just erase the\" [:br] \"        previous solution. See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    presort : deprecated, default='deprecated'\" [:br] \"        This parameter is deprecated and will be removed in v0.24.\" [:br] \"\" [:br] \"        .. deprecated :: 0.22\" [:br] \"\" [:br] \"    validation_fraction : float, default=0.1\" [:br] \"        The proportion of training data to set aside as validation set for\" [:br] \"        early stopping. Must be between 0 and 1.\" [:br] \"        Only used if ``n_iter_no_change`` is set to an integer.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    n_iter_no_change : int, default=None\" [:br] \"        ``n_iter_no_change`` is used to decide if early stopping will be used\" [:br] \"        to terminate training when validation score is not improving. By\" [:br] \"        default it is set to None to disable early stopping. If set to a\" [:br] \"        number, it will set aside ``validation_fraction`` size of the training\" [:br] \"        data as validation and terminate training when validation score is not\" [:br] \"        improving in all of the previous ``n_iter_no_change`` numbers of\" [:br] \"        iterations. The split is stratified.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        Tolerance for the early stopping. When the loss is not improving\" [:br] \"        by at least tol for ``n_iter_no_change`` iterations (if set to a\" [:br] \"        number), the training stops.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    ccp_alpha : non-negative float, default=0.0\" [:br] \"        Complexity parameter used for Minimal Cost-Complexity Pruning. The\" [:br] \"        subtree with the largest cost complexity that is smaller than\" [:br] \"        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\" [:br] \"        :ref:`minimal_cost_complexity_pruning` for details.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    n_estimators_ : int\" [:br] \"        The number of estimators as selected by early stopping (if\" [:br] \"        ``n_iter_no_change`` is specified). Otherwise it is set to\" [:br] \"        ``n_estimators``.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    feature_importances_ : ndarray of shape (n_features,)\" [:br] \"        The impurity-based feature importances.\" [:br] \"        The higher, the more important the feature.\" [:br] \"        The importance of a feature is computed as the (normalized)\" [:br] \"        total reduction of the criterion brought by that feature.  It is also\" [:br] \"        known as the Gini importance.\" [:br] \"\" [:br] \"        Warning: impurity-based feature importances can be misleading for\" [:br] \"        high cardinality features (many unique values). See\" [:br] \"        :func:`sklearn.inspection.permutation_importance` as an alternative.\" [:br] \"\" [:br] \"    oob_improvement_ : ndarray of shape (n_estimators,)\" [:br] \"        The improvement in loss (= deviance) on the out-of-bag samples\" [:br] \"        relative to the previous iteration.\" [:br] \"        ``oob_improvement_[0]`` is the improvement in\" [:br] \"        loss of the first stage over the ``init`` estimator.\" [:br] \"        Only available if ``subsample < 1.0``\" [:br] \"\" [:br] \"    train_score_ : ndarray of shape (n_estimators,)\" [:br] \"        The i-th score ``train_score_[i]`` is the deviance (= loss) of the\" [:br] \"        model at iteration ``i`` on the in-bag sample.\" [:br] \"        If ``subsample == 1`` this is the deviance on the training data.\" [:br] \"\" [:br] \"    loss_ : LossFunction\" [:br] \"        The concrete ``LossFunction`` object.\" [:br] \"\" [:br] \"    init_ : estimator\" [:br] \"        The estimator that provides the initial predictions.\" [:br] \"        Set via the ``init`` argument or ``loss.init_estimator``.\" [:br] \"\" [:br] \"    estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, ``loss_.K``)\" [:br] \"        The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\" [:br] \"        classification, otherwise n_classes.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        The classes labels.\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        The number of data features.\" [:br] \"\" [:br] \"    n_classes_ : int\" [:br] \"        The number of classes.\" [:br] \"\" [:br] \"    max_features_ : int\" [:br] \"        The inferred value of max_features.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The features are always randomly permuted at each split. Therefore,\" [:br] \"    the best found split may vary, even with the same training data and\" [:br] \"    ``max_features=n_features``, if the improvement of the criterion is\" [:br] \"    identical for several splits enumerated during the search of the best\" [:br] \"    split. To obtain a deterministic behaviour during fitting,\" [:br] \"    ``random_state`` has to be fixed.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import make_classification\" [:br] \"    >>> from sklearn.ensemble import GradientBoostingClassifier\" [:br] \"    >>> from sklearn.model_selection import train_test_split\" [:br] \"    >>> X, y = make_classification(random_state=0)\" [:br] \"    >>> X_train, X_test, y_train, y_test = train_test_split(\" [:br] \"    ...     X, y, random_state=0)\" [:br] \"    >>> clf = GradientBoostingClassifier(random_state=0)\" [:br] \"    >>> clf.fit(X_train, y_train)\" [:br] \"    GradientBoostingClassifier(random_state=0)\" [:br] \"    >>> clf.predict(X_test[:2])\" [:br] \"    array([1, 0])\" [:br] \"    >>> clf.score(X_test, y_test)\" [:br] \"    0.88\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    sklearn.ensemble.HistGradientBoostingClassifier,\" [:br] \"    sklearn.tree.DecisionTreeClassifier, RandomForestClassifier\" [:br] \"    AdaBoostClassifier\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    J. Friedman, Greedy Function Approximation: A Gradient Boosting\" [:br] \"    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\" [:br] \"\" [:br] \"    J. Friedman, Stochastic Gradient Boosting, 1999\" [:br] \"\" [:br] \"    T. Hastie, R. Tibshirani and J. Friedman.\" [:br] \"    Elements of Statistical Learning Ed. 2, Springer, 2009.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/hist-gradient-boosting-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [17 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |     10.00 |\\n|       :learning-rate |    0.1000 |\\n|      :max-leaf-nodes |     31.00 |\\n|             :scoring |      loss |\\n|                 :tol | 1.000E-07 |\\n|      :early-stopping |      auto |\\n|            :max-iter |       100 |\\n|        :random-state |           |\\n|            :max-bins |       255 |\\n|    :min-samples-leaf |        20 |\\n|       :monotonic-cst |           |\\n|          :warm-start |     false |\\n|           :max-depth |           |\\n| :validation-fraction |    0.1000 |\\n|                :loss |      auto |\\n|             :verbose |         0 |\\n|  :l-2-regularization |     0.000 |\\n\"]]] [:span (\"Histogram-based Gradient Boosting Classification Tree.\" [:br] \"\" [:br] \"    This estimator is much faster than\" [:br] \"    :class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\" [:br] \"    for big datasets (n_samples >= 10 000).\" [:br] \"\" [:br] \"    This estimator has native support for missing values (NaNs). During\" [:br] \"    training, the tree grower learns at each split point whether samples\" [:br] \"    with missing values should go to the left or right child, based on the\" [:br] \"    potential gain. When predicting, samples with missing values are\" [:br] \"    assigned to the left or right child consequently. If no missing values\" [:br] \"    were encountered for a given feature during training, then samples with\" [:br] \"    missing values are mapped to whichever child has the most samples.\" [:br] \"\" [:br] \"    This implementation is inspired by\" [:br] \"    `LightGBM <https://github.com/Microsoft/LightGBM>`_.\" [:br] \"\" [:br] \"    .. note::\" [:br] \"\" [:br] \"      This estimator is still **experimental** for now: the predictions\" [:br] \"      and the API might change without any deprecation cycle. To use it,\" [:br] \"      you need to explicitly import ``enable_hist_gradient_boosting``::\" [:br] \"\" [:br] \"        >>> # explicitly require this experimental feature\" [:br] \"        >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\" [:br] \"        >>> # now you can import normally from ensemble\" [:br] \"        >>> from sklearn.ensemble import HistGradientBoostingClassifier\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.21\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    loss : {'auto', 'binary_crossentropy', 'categorical_crossentropy'},             optional (default='auto')\" [:br] \"        The loss function to use in the boosting process. 'binary_crossentropy'\" [:br] \"        (also known as logistic loss) is used for binary classification and\" [:br] \"        generalizes to 'categorical_crossentropy' for multiclass\" [:br] \"        classification. 'auto' will automatically choose either loss depending\" [:br] \"        on the nature of the problem.\" [:br] \"    learning_rate : float, optional (default=0.1)\" [:br] \"        The learning rate, also known as *shrinkage*. This is used as a\" [:br] \"        multiplicative factor for the leaves values. Use ``1`` for no\" [:br] \"        shrinkage.\" [:br] \"    max_iter : int, optional (default=100)\" [:br] \"        The maximum number of iterations of the boosting process, i.e. the\" [:br] \"        maximum number of trees for binary classification. For multiclass\" [:br] \"        classification, `n_classes` trees per iteration are built.\" [:br] \"    max_leaf_nodes : int or None, optional (default=31)\" [:br] \"        The maximum number of leaves for each tree. Must be strictly greater\" [:br] \"        than 1. If None, there is no maximum limit.\" [:br] \"    max_depth : int or None, optional (default=None)\" [:br] \"        The maximum depth of each tree. The depth of a tree is the number of\" [:br] \"        edges to go from the root to the deepest leaf.\" [:br] \"        Depth isn't constrained by default.\" [:br] \"    min_samples_leaf : int, optional (default=20)\" [:br] \"        The minimum number of samples per leaf. For small datasets with less\" [:br] \"        than a few hundred samples, it is recommended to lower this value\" [:br] \"        since only very shallow trees would be built.\" [:br] \"    l2_regularization : float, optional (default=0)\" [:br] \"        The L2 regularization parameter. Use 0 for no regularization.\" [:br] \"    max_bins : int, optional (default=255)\" [:br] \"        The maximum number of bins to use for non-missing values. Before\" [:br] \"        training, each feature of the input array `X` is binned into\" [:br] \"        integer-valued bins, which allows for a much faster training stage.\" [:br] \"        Features with a small number of unique values may use less than\" [:br] \"        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\" [:br] \"        is always reserved for missing values. Must be no larger than 255.\" [:br] \"    monotonic_cst : array-like of int of shape (n_features), default=None\" [:br] \"        Indicates the monotonic constraint to enforce on each feature. -1, 1\" [:br] \"        and 0 respectively correspond to a positive constraint, negative\" [:br] \"        constraint and no constraint. Read more in the :ref:`User Guide\" [:br] \"        <monotonic_cst_gbdt>`.\" [:br] \"    warm_start : bool, optional (default=False)\" [:br] \"        When set to ``True``, reuse the solution of the previous call to fit\" [:br] \"        and add more estimators to the ensemble. For results to be valid, the\" [:br] \"        estimator should be re-trained on the same data only.\" [:br] \"        See :term:`the Glossary <warm_start>`.\" [:br] \"    early_stopping : 'auto' or bool (default='auto')\" [:br] \"        If 'auto', early stopping is enabled if the sample size is larger than\" [:br] \"        10000. If True, early stopping is enabled, otherwise early stopping is\" [:br] \"        disabled.\" [:br] \"    scoring : str or callable or None, optional (default='loss')\" [:br] \"        Scoring parameter to use for early stopping. It can be a single\" [:br] \"        string (see :ref:`scoring_parameter`) or a callable (see\" [:br] \"        :ref:`scoring`). If None, the estimator's default scorer\" [:br] \"        is used. If ``scoring='loss'``, early stopping is checked\" [:br] \"        w.r.t the loss value. Only used if early stopping is performed.\" [:br] \"    validation_fraction : int or float or None, optional (default=0.1)\" [:br] \"        Proportion (or absolute size) of training data to set aside as\" [:br] \"        validation data for early stopping. If None, early stopping is done on\" [:br] \"        the training data. Only used if early stopping is performed.\" [:br] \"    n_iter_no_change : int, optional (default=10)\" [:br] \"        Used to determine when to \\\"early stop\\\". The fitting process is\" [:br] \"        stopped when none of the last ``n_iter_no_change`` scores are better\" [:br] \"        than the ``n_iter_no_change - 1`` -th-to-last one, up to some\" [:br] \"        tolerance. Only used if early stopping is performed.\" [:br] \"    tol : float or None, optional (default=1e-7)\" [:br] \"        The absolute tolerance to use when comparing scores. The higher the\" [:br] \"        tolerance, the more likely we are to early stop: higher tolerance\" [:br] \"        means that it will be harder for subsequent iterations to be\" [:br] \"        considered an improvement upon the reference score.\" [:br] \"    verbose: int, optional (default=0)\" [:br] \"        The verbosity level. If not zero, print some information about the\" [:br] \"        fitting process.\" [:br] \"    random_state : int, np.random.RandomStateInstance or None,         optional (default=None)\" [:br] \"        Pseudo-random number generator to control the subsampling in the\" [:br] \"        binning process, and the train/validation data split if early stopping\" [:br] \"        is enabled.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    classes_ : array, shape = (n_classes,)\" [:br] \"        Class labels.\" [:br] \"    n_iter_ : int\" [:br] \"        The number of iterations as selected by early stopping, depending on\" [:br] \"        the `early_stopping` parameter. Otherwise it corresponds to max_iter.\" [:br] \"    n_trees_per_iteration_ : int\" [:br] \"        The number of tree that are built at each iteration. This is equal to 1\" [:br] \"        for binary classification, and to ``n_classes`` for multiclass\" [:br] \"        classification.\" [:br] \"    train_score_ : ndarray, shape (n_iter_+1,)\" [:br] \"        The scores at each iteration on the training data. The first entry\" [:br] \"        is the score of the ensemble before the first iteration. Scores are\" [:br] \"        computed according to the ``scoring`` parameter. If ``scoring`` is\" [:br] \"        not 'loss', scores are computed on a subset of at most 10 000\" [:br] \"        samples. Empty if no early stopping.\" [:br] \"    validation_score_ : ndarray, shape (n_iter_+1,)\" [:br] \"        The scores at each iteration on the held-out validation data. The\" [:br] \"        first entry is the score of the ensemble before the first iteration.\" [:br] \"        Scores are computed according to the ``scoring`` parameter. Empty if\" [:br] \"        no early stopping or if ``validation_fraction`` is None.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> # To use this experimental feature, we need to explicitly ask for it:\" [:br] \"    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\" [:br] \"    >>> from sklearn.ensemble import HistGradientBoostingClassifier\" [:br] \"    >>> from sklearn.datasets import load_iris\" [:br] \"    >>> X, y = load_iris(return_X_y=True)\" [:br] \"    >>> clf = HistGradientBoostingClassifier().fit(X, y)\" [:br] \"    >>> clf.score(X, y)\" [:br] \"    1.0\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/k-neighbors-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :algorithm |      auto |\\n|     :leaf-size |        30 |\\n|        :metric | minkowski |\\n| :metric-params |           |\\n|        :n-jobs |           |\\n|   :n-neighbors |         5 |\\n|             :p |         2 |\\n|       :weights |   uniform |\\n\"]]] [:span (\"Classifier implementing the k-nearest neighbors vote.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <classification>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    n_neighbors : int, default=5\" [:br] \"        Number of neighbors to use by default for :meth:`kneighbors` queries.\" [:br] \"\" [:br] \"    weights : {'uniform', 'distance'} or callable, default='uniform'\" [:br] \"        weight function used in prediction.  Possible values:\" [:br] \"\" [:br] \"        - 'uniform' : uniform weights.  All points in each neighborhood\" [:br] \"          are weighted equally.\" [:br] \"        - 'distance' : weight points by the inverse of their distance.\" [:br] \"          in this case, closer neighbors of a query point will have a\" [:br] \"          greater influence than neighbors which are further away.\" [:br] \"        - [callable] : a user-defined function which accepts an\" [:br] \"          array of distances, and returns an array of the same shape\" [:br] \"          containing the weights.\" [:br] \"\" [:br] \"    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\" [:br] \"        Algorithm used to compute the nearest neighbors:\" [:br] \"\" [:br] \"        - 'ball_tree' will use :class:`BallTree`\" [:br] \"        - 'kd_tree' will use :class:`KDTree`\" [:br] \"        - 'brute' will use a brute-force search.\" [:br] \"        - 'auto' will attempt to decide the most appropriate algorithm\" [:br] \"          based on the values passed to :meth:`fit` method.\" [:br] \"\" [:br] \"        Note: fitting on sparse input will override the setting of\" [:br] \"        this parameter, using brute force.\" [:br] \"\" [:br] \"    leaf_size : int, default=30\" [:br] \"        Leaf size passed to BallTree or KDTree.  This can affect the\" [:br] \"        speed of the construction and query, as well as the memory\" [:br] \"        required to store the tree.  The optimal value depends on the\" [:br] \"        nature of the problem.\" [:br] \"\" [:br] \"    p : int, default=2\" [:br] \"        Power parameter for the Minkowski metric. When p = 1, this is\" [:br] \"        equivalent to using manhattan_distance (l1), and euclidean_distance\" [:br] \"        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\" [:br] \"\" [:br] \"    metric : str or callable, default='minkowski'\" [:br] \"        the distance metric to use for the tree.  The default metric is\" [:br] \"        minkowski, and with p=2 is equivalent to the standard Euclidean\" [:br] \"        metric. See the documentation of :class:`DistanceMetric` for a\" [:br] \"        list of available metrics.\" [:br] \"        If metric is \\\"precomputed\\\", X is assumed to be a distance matrix and\" [:br] \"        must be square during fit. X may be a :term:`sparse graph`,\" [:br] \"        in which case only \\\"nonzero\\\" elements may be considered neighbors.\" [:br] \"\" [:br] \"    metric_params : dict, default=None\" [:br] \"        Additional keyword arguments for the metric function.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of parallel jobs to run for neighbors search.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"        Doesn't affect :meth:`fit` method.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    classes_ : array of shape (n_classes,)\" [:br] \"        Class labels known to the classifier\" [:br] \"\" [:br] \"    effective_metric_ : str or callble\" [:br] \"        The distance metric used. It will be same as the `metric` parameter\" [:br] \"        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\" [:br] \"        'minkowski' and `p` parameter set to 2.\" [:br] \"\" [:br] \"    effective_metric_params_ : dict\" [:br] \"        Additional keyword arguments for the metric function. For most metrics\" [:br] \"        will be same with `metric_params` parameter, but may also contain the\" [:br] \"        `p` parameter value if the `effective_metric_` attribute is set to\" [:br] \"        'minkowski'.\" [:br] \"\" [:br] \"    outputs_2d_ : bool\" [:br] \"        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\" [:br] \"        otherwise True.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> X = [[0], [1], [2], [3]]\" [:br] \"    >>> y = [0, 0, 1, 1]\" [:br] \"    >>> from sklearn.neighbors import KNeighborsClassifier\" [:br] \"    >>> neigh = KNeighborsClassifier(n_neighbors=3)\" [:br] \"    >>> neigh.fit(X, y)\" [:br] \"    KNeighborsClassifier(...)\" [:br] \"    >>> print(neigh.predict([[1.1]]))\" [:br] \"    [0]\" [:br] \"    >>> print(neigh.predict_proba([[0.9]]))\" [:br] \"    [[0.66666667 0.33333333]]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    RadiusNeighborsClassifier\" [:br] \"    KNeighborsRegressor\" [:br] \"    RadiusNeighborsRegressor\" [:br] \"    NearestNeighbors\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\" [:br] \"    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\" [:br] \"\" [:br] \"    .. warning::\" [:br] \"\" [:br] \"       Regarding the Nearest Neighbors algorithms, if it is found that two\" [:br] \"       neighbors, neighbor `k+1` and `k`, have identical distances\" [:br] \"       but different labels, the results will depend on the ordering of the\" [:br] \"       training data.\" [:br] \"\" [:br] \"    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/label-propagation\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"368px\"}} [:p/markdown \"_unnamed [6 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :gamma |       20 |\\n|      :kernel |      rbf |\\n|    :max-iter |     1000 |\\n|      :n-jobs |          |\\n| :n-neighbors |        7 |\\n|         :tol | 0.001000 |\\n\"]]] [:span (\"Label Propagation classifier\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <label_propagation>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    kernel : {'knn', 'rbf'} or callable, default='rbf'\" [:br] \"        String identifier for kernel function to use or the kernel function\" [:br] \"        itself. Only 'rbf' and 'knn' strings are valid inputs. The function\" [:br] \"        passed should take two inputs, each of shape (n_samples, n_features),\" [:br] \"        and return a (n_samples, n_samples) shaped weight matrix.\" [:br] \"\" [:br] \"    gamma : float, default=20\" [:br] \"        Parameter for rbf kernel.\" [:br] \"\" [:br] \"    n_neighbors : int, default=7\" [:br] \"        Parameter for knn kernel which need to be strictly positive.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        Change maximum number of iterations allowed.\" [:br] \"\" [:br] \"    tol : float, 1e-3\" [:br] \"        Convergence tolerance: threshold to consider the system at steady\" [:br] \"        state.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of parallel jobs to run.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    X_ : ndarray of shape (n_samples, n_features)\" [:br] \"        Input array.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        The distinct labels used in classifying instances.\" [:br] \"\" [:br] \"    label_distributions_ : ndarray of shape (n_samples, n_classes)\" [:br] \"        Categorical distribution for each item.\" [:br] \"\" [:br] \"    transduction_ : ndarray of shape (n_samples)\" [:br] \"        Label assigned to each item via the transduction.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        Number of iterations run.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> from sklearn import datasets\" [:br] \"    >>> from sklearn.semi_supervised import LabelPropagation\" [:br] \"    >>> label_prop_model = LabelPropagation()\" [:br] \"    >>> iris = datasets.load_iris()\" [:br] \"    >>> rng = np.random.RandomState(42)\" [:br] \"    >>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3\" [:br] \"    >>> labels = np.copy(iris.target)\" [:br] \"    >>> labels[random_unlabeled_points] = -1\" [:br] \"    >>> label_prop_model.fit(iris.data, labels)\" [:br] \"    LabelPropagation(...)\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data\" [:br] \"    with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon\" [:br] \"    University, 2002 http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    LabelSpreading : Alternate label propagation strategy more robust to noise\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/label-spreading\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [7 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |   0.2000 |\\n|       :gamma |    20.00 |\\n|      :kernel |      rbf |\\n|    :max-iter |       30 |\\n|      :n-jobs |          |\\n| :n-neighbors |        7 |\\n|         :tol | 0.001000 |\\n\"]]] [:span (\"LabelSpreading model for semi-supervised learning\" [:br] \"\" [:br] \"    This model is similar to the basic Label Propagation algorithm,\" [:br] \"    but uses affinity matrix based on the normalized graph Laplacian\" [:br] \"    and soft clamping across the labels.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <label_propagation>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    kernel : {'knn', 'rbf'} or callable, default='rbf'\" [:br] \"        String identifier for kernel function to use or the kernel function\" [:br] \"        itself. Only 'rbf' and 'knn' strings are valid inputs. The function\" [:br] \"        passed should take two inputs, each of shape (n_samples, n_features),\" [:br] \"        and return a (n_samples, n_samples) shaped weight matrix.\" [:br] \"\" [:br] \"    gamma : float, default=20\" [:br] \"      Parameter for rbf kernel.\" [:br] \"\" [:br] \"    n_neighbors : int, default=7\" [:br] \"      Parameter for knn kernel which is a strictly positive integer.\" [:br] \"\" [:br] \"    alpha : float, default=0.2\" [:br] \"      Clamping factor. A value in (0, 1) that specifies the relative amount\" [:br] \"      that an instance should adopt the information from its neighbors as\" [:br] \"      opposed to its initial label.\" [:br] \"      alpha=0 means keeping the initial label information; alpha=1 means\" [:br] \"      replacing all initial information.\" [:br] \"\" [:br] \"    max_iter : int, default=30\" [:br] \"      Maximum number of iterations allowed.\" [:br] \"\" [:br] \"    tol : float, default=1e-3\" [:br] \"      Convergence tolerance: threshold to consider the system at steady\" [:br] \"      state.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of parallel jobs to run.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    X_ : ndarray of shape (n_samples, n_features)\" [:br] \"        Input array.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        The distinct labels used in classifying instances.\" [:br] \"\" [:br] \"    label_distributions_ : ndarray of shape (n_samples, n_classes)\" [:br] \"        Categorical distribution for each item.\" [:br] \"\" [:br] \"    transduction_ : ndarray of shape (n_samples,)\" [:br] \"        Label assigned to each item via the transduction.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        Number of iterations run.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> from sklearn import datasets\" [:br] \"    >>> from sklearn.semi_supervised import LabelSpreading\" [:br] \"    >>> label_prop_model = LabelSpreading()\" [:br] \"    >>> iris = datasets.load_iris()\" [:br] \"    >>> rng = np.random.RandomState(42)\" [:br] \"    >>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3\" [:br] \"    >>> labels = np.copy(iris.target)\" [:br] \"    >>> labels[random_unlabeled_points] = -1\" [:br] \"    >>> label_prop_model.fit(iris.data, labels)\" [:br] \"    LabelSpreading(...)\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston,\" [:br] \"    Bernhard Schoelkopf. Learning with local and global consistency (2004)\" [:br] \"    http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    LabelPropagation : Unregularized graph based semi-supervised learning\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/linear-discriminant-analysis\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"368px\"}} [:p/markdown \"_unnamed [6 2]:\\n\\n|             :name |  :default |\\n|-------------------|-----------|\\n|     :n-components |           |\\n|           :priors |           |\\n|        :shrinkage |           |\\n|           :solver |       svd |\\n| :store-covariance |     false |\\n|              :tol | 0.0001000 |\\n\"]]] [:span (\"Linear Discriminant Analysis\" [:br] \"\" [:br] \"    A classifier with a linear decision boundary, generated by fitting class\" [:br] \"    conditional densities to the data and using Bayes' rule.\" [:br] \"\" [:br] \"    The model fits a Gaussian density to each class, assuming that all classes\" [:br] \"    share the same covariance matrix.\" [:br] \"\" [:br] \"    The fitted model can also be used to reduce the dimensionality of the input\" [:br] \"    by projecting it to the most discriminative directions, using the\" [:br] \"    `transform` method.\" [:br] \"\" [:br] \"    .. versionadded:: 0.17\" [:br] \"       *LinearDiscriminantAnalysis*.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <lda_qda>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    solver : {'svd', 'lsqr', 'eigen'}, default='svd'\" [:br] \"        Solver to use, possible values:\" [:br] \"          - 'svd': Singular value decomposition (default).\" [:br] \"            Does not compute the covariance matrix, therefore this solver is\" [:br] \"            recommended for data with a large number of features.\" [:br] \"          - 'lsqr': Least squares solution, can be combined with shrinkage.\" [:br] \"          - 'eigen': Eigenvalue decomposition, can be combined with shrinkage.\" [:br] \"\" [:br] \"    shrinkage : 'auto' or float, default=None\" [:br] \"        Shrinkage parameter, possible values:\" [:br] \"          - None: no shrinkage (default).\" [:br] \"          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\" [:br] \"          - float between 0 and 1: fixed shrinkage parameter.\" [:br] \"\" [:br] \"        Note that shrinkage works only with 'lsqr' and 'eigen' solvers.\" [:br] \"\" [:br] \"    priors : array-like of shape (n_classes,), default=None\" [:br] \"        The class prior probabilities. By default, the class proportions are\" [:br] \"        inferred from the training data.\" [:br] \"\" [:br] \"    n_components : int, default=None\" [:br] \"        Number of components (<= min(n_classes - 1, n_features)) for\" [:br] \"        dimensionality reduction. If None, will be set to\" [:br] \"        min(n_classes - 1, n_features). This parameter only affects the\" [:br] \"        `transform` method.\" [:br] \"\" [:br] \"    store_covariance : bool, default=False\" [:br] \"        If True, explicitely compute the weighted within-class covariance\" [:br] \"        matrix when solver is 'svd'. The matrix is always computed\" [:br] \"        and stored for the other solvers.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"\" [:br] \"    tol : float, default=1.0e-4\" [:br] \"        Absolute threshold for a singular value of X to be considered\" [:br] \"        significant, used to estimate the rank of X. Dimensions whose\" [:br] \"        singular values are non-significant are discarded. Only used if\" [:br] \"        solver is 'svd'.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : ndarray of shape (n_features,) or (n_classes, n_features)\" [:br] \"        Weight vector(s).\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape (n_classes,)\" [:br] \"        Intercept term.\" [:br] \"\" [:br] \"    covariance_ : array-like of shape (n_features, n_features)\" [:br] \"        Weighted within-class covariance matrix. It corresponds to\" [:br] \"        `sum_k prior_k * C_k` where `C_k` is the covariance matrix of the\" [:br] \"        samples in class `k`. The `C_k` are estimated using the (potentially\" [:br] \"        shrunk) biased estimator of covariance. If solver is 'svd', only\" [:br] \"        exists when `store_covariance` is True.\" [:br] \"\" [:br] \"    explained_variance_ratio_ : ndarray of shape (n_components,)\" [:br] \"        Percentage of variance explained by each of the selected components.\" [:br] \"        If ``n_components`` is not set then all components are stored and the\" [:br] \"        sum of explained variances is equal to 1.0. Only available when eigen\" [:br] \"        or svd solver is used.\" [:br] \"\" [:br] \"    means_ : array-like of shape (n_classes, n_features)\" [:br] \"        Class-wise means.\" [:br] \"\" [:br] \"    priors_ : array-like of shape (n_classes,)\" [:br] \"        Class priors (sum to 1).\" [:br] \"\" [:br] \"    scalings_ : array-like of shape (rank, n_classes - 1)\" [:br] \"        Scaling of the features in the space spanned by the class centroids.\" [:br] \"        Only available for 'svd' and 'eigen' solvers.\" [:br] \"\" [:br] \"    xbar_ : array-like of shape (n_features,)\" [:br] \"        Overall mean. Only present if solver is 'svd'.\" [:br] \"\" [:br] \"    classes_ : array-like of shape (n_classes,)\" [:br] \"        Unique class labels.\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis: Quadratic\" [:br] \"        Discriminant Analysis\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\" [:br] \"    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\" [:br] \"    >>> y = np.array([1, 1, 1, 2, 2, 2])\" [:br] \"    >>> clf = LinearDiscriminantAnalysis()\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    LinearDiscriminantAnalysis()\" [:br] \"    >>> print(clf.predict([[-0.8, -1]]))\" [:br] \"    [1]\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/linear-svc\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [12 2]:\\n\\n|              :name |      :default |\\n|--------------------|---------------|\\n|               :tol |     0.0001000 |\\n| :intercept-scaling |         1.000 |\\n|       :multi-class |           ovr |\\n|           :penalty |            l2 |\\n|                 :c |         1.000 |\\n|          :max-iter |          1000 |\\n|      :random-state |               |\\n|              :dual |          true |\\n|     :fit-intercept |          true |\\n|      :class-weight |               |\\n|              :loss | squared_hinge |\\n|           :verbose |             0 |\\n\"]]] [:span (\"Linear Support Vector Classification.\" [:br] \"\" [:br] \"    Similar to SVC with parameter kernel='linear', but implemented in terms of\" [:br] \"    liblinear rather than libsvm, so it has more flexibility in the choice of\" [:br] \"    penalties and loss functions and should scale better to large numbers of\" [:br] \"    samples.\" [:br] \"\" [:br] \"    This class supports both dense and sparse input and the multiclass support\" [:br] \"    is handled according to a one-vs-the-rest scheme.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <svm_classification>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    penalty : {'l1', 'l2'}, default='l2'\" [:br] \"        Specifies the norm used in the penalization. The 'l2'\" [:br] \"        penalty is the standard used in SVC. The 'l1' leads to ``coef_``\" [:br] \"        vectors that are sparse.\" [:br] \"\" [:br] \"    loss : {'hinge', 'squared_hinge'}, default='squared_hinge'\" [:br] \"        Specifies the loss function. 'hinge' is the standard SVM loss\" [:br] \"        (used e.g. by the SVC class) while 'squared_hinge' is the\" [:br] \"        square of the hinge loss.\" [:br] \"\" [:br] \"    dual : bool, default=True\" [:br] \"        Select the algorithm to either solve the dual or primal\" [:br] \"        optimization problem. Prefer dual=False when n_samples > n_features.\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        Tolerance for stopping criteria.\" [:br] \"\" [:br] \"    C : float, default=1.0\" [:br] \"        Regularization parameter. The strength of the regularization is\" [:br] \"        inversely proportional to C. Must be strictly positive.\" [:br] \"\" [:br] \"    multi_class : {'ovr', 'crammer_singer'}, default='ovr'\" [:br] \"        Determines the multi-class strategy if `y` contains more than\" [:br] \"        two classes.\" [:br] \"        ``\\\"ovr\\\"`` trains n_classes one-vs-rest classifiers, while\" [:br] \"        ``\\\"crammer_singer\\\"`` optimizes a joint objective over all classes.\" [:br] \"        While `crammer_singer` is interesting from a theoretical perspective\" [:br] \"        as it is consistent, it is seldom used in practice as it rarely leads\" [:br] \"        to better accuracy and is more expensive to compute.\" [:br] \"        If ``\\\"crammer_singer\\\"`` is chosen, the options loss, penalty and dual\" [:br] \"        will be ignored.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be already centered).\" [:br] \"\" [:br] \"    intercept_scaling : float, default=1\" [:br] \"        When self.fit_intercept is True, instance vector x becomes\" [:br] \"        ``[x, self.intercept_scaling]``,\" [:br] \"        i.e. a \\\"synthetic\\\" feature with constant value equals to\" [:br] \"        intercept_scaling is appended to the instance vector.\" [:br] \"        The intercept becomes intercept_scaling * synthetic feature weight\" [:br] \"        Note! the synthetic feature weight is subject to l1/l2 regularization\" [:br] \"        as all other features.\" [:br] \"        To lessen the effect of regularization on synthetic feature weight\" [:br] \"        (and therefore on the intercept) intercept_scaling has to be increased.\" [:br] \"\" [:br] \"    class_weight : dict or 'balanced', default=None\" [:br] \"        Set the parameter C of class i to ``class_weight[i]*C`` for\" [:br] \"        SVC. If not given, all classes are supposed to have\" [:br] \"        weight one.\" [:br] \"        The \\\"balanced\\\" mode uses the values of y to automatically adjust\" [:br] \"        weights inversely proportional to class frequencies in the input data\" [:br] \"        as ``n_samples / (n_classes * np.bincount(y))``.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        Enable verbose output. Note that this setting takes advantage of a\" [:br] \"        per-process runtime setting in liblinear that, if enabled, may not work\" [:br] \"        properly in a multithreaded context.\" [:br] \"\" [:br] \"    random_state : int or RandomState instance, default=None\" [:br] \"        Controls the pseudo random number generation for shuffling the data for\" [:br] \"        the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\" [:br] \"        underlying implementation of :class:`LinearSVC` is not random and\" [:br] \"        ``random_state`` has no effect on the results.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        The maximum number of iterations to be run.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)\" [:br] \"        Weights assigned to the features (coefficients in the primal\" [:br] \"        problem). This is only available in the case of a linear kernel.\" [:br] \"\" [:br] \"        ``coef_`` is a readonly property derived from ``raw_coef_`` that\" [:br] \"        follows the internal memory layout of liblinear.\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\" [:br] \"        Constants in decision function.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        The unique classes labels.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        Maximum number of iterations run across all classes.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    SVC\" [:br] \"        Implementation of Support Vector Machine classifier using libsvm:\" [:br] \"        the kernel can be non-linear but its SMO algorithm does not\" [:br] \"        scale to large number of samples as LinearSVC does.\" [:br] \"\" [:br] \"        Furthermore SVC multi-class mode is implemented using one\" [:br] \"        vs one scheme while LinearSVC uses one vs the rest. It is\" [:br] \"        possible to implement one vs the rest with SVC by using the\" [:br] \"        :class:`sklearn.multiclass.OneVsRestClassifier` wrapper.\" [:br] \"\" [:br] \"        Finally SVC can fit dense data without memory copy if the input\" [:br] \"        is C-contiguous. Sparse data will still incur memory copy though.\" [:br] \"\" [:br] \"    sklearn.linear_model.SGDClassifier\" [:br] \"        SGDClassifier can optimize the same cost function as LinearSVC\" [:br] \"        by adjusting the penalty and loss parameters. In addition it requires\" [:br] \"        less memory, allows incremental (online) learning, and implements\" [:br] \"        various loss functions and regularization regimes.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The underlying C implementation uses a random number generator to\" [:br] \"    select features when fitting the model. It is thus not uncommon\" [:br] \"    to have slightly different results for the same input data. If\" [:br] \"    that happens, try with a smaller ``tol`` parameter.\" [:br] \"\" [:br] \"    The underlying implementation, liblinear, uses a sparse internal\" [:br] \"    representation for the data that will incur a memory copy.\" [:br] \"\" [:br] \"    Predict output may not match that of standalone liblinear in certain\" [:br] \"    cases. See :ref:`differences from liblinear <liblinear_differences>`\" [:br] \"    in the narrative documentation.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    `LIBLINEAR: A Library for Large Linear Classification\" [:br] \"    <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.svm import LinearSVC\" [:br] \"    >>> from sklearn.pipeline import make_pipeline\" [:br] \"    >>> from sklearn.preprocessing import StandardScaler\" [:br] \"    >>> from sklearn.datasets import make_classification\" [:br] \"    >>> X, y = make_classification(n_features=4, random_state=0)\" [:br] \"    >>> clf = make_pipeline(StandardScaler(),\" [:br] \"    ...                     LinearSVC(random_state=0, tol=1e-5))\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    Pipeline(steps=[('standardscaler', StandardScaler()),\" [:br] \"                    ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\" [:br] \"\" [:br] \"    >>> print(clf.named_steps['linearsvc'].coef_)\" [:br] \"    [[0.141...   0.526... 0.679... 0.493...]]\" [:br] \"\" [:br] \"    >>> print(clf.named_steps['linearsvc'].intercept_)\" [:br] \"    [0.1693...]\" [:br] \"    >>> print(clf.predict([[0, 0, 0, 0]]))\" [:br] \"    [1]\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/logistic-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [15 2]:\\n\\n|              :name |  :default |\\n|--------------------|-----------|\\n|               :tol | 0.0001000 |\\n| :intercept-scaling |     1.000 |\\n|       :multi-class |      auto |\\n|            :solver |     lbfgs |\\n|           :penalty |        l2 |\\n|                 :c |     1.000 |\\n|          :max-iter |       100 |\\n|            :n-jobs |           |\\n|      :random-state |           |\\n|              :dual |     false |\\n|     :fit-intercept |      true |\\n|        :warm-start |     false |\\n|         :l-1-ratio |           |\\n|      :class-weight |           |\\n|           :verbose |         0 |\\n\"]]] [:span (\"\" [:br] \"    Logistic Regression (aka logit, MaxEnt) classifier.\" [:br] \"\" [:br] \"    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\" [:br] \"    scheme if the 'multi_class' option is set to 'ovr', and uses the\" [:br] \"    cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\" [:br] \"    (Currently the 'multinomial' option is supported only by the 'lbfgs',\" [:br] \"    'sag', 'saga' and 'newton-cg' solvers.)\" [:br] \"\" [:br] \"    This class implements regularized logistic regression using the\" [:br] \"    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\" [:br] \"    that regularization is applied by default**. It can handle both dense\" [:br] \"    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\" [:br] \"    floats for optimal performance; any other input format will be converted\" [:br] \"    (and copied).\" [:br] \"\" [:br] \"    The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\" [:br] \"    with primal formulation, or no regularization. The 'liblinear' solver\" [:br] \"    supports both L1 and L2 regularization, with a dual formulation only for\" [:br] \"    the L2 penalty. The Elastic-Net regularization is only supported by the\" [:br] \"    'saga' solver.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <logistic_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\" [:br] \"        Used to specify the norm used in the penalization. The 'newton-cg',\" [:br] \"        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\" [:br] \"        only supported by the 'saga' solver. If 'none' (not supported by the\" [:br] \"        liblinear solver), no regularization is applied.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"           l1 penalty with SAGA solver (allowing 'multinomial' + L1)\" [:br] \"\" [:br] \"    dual : bool, default=False\" [:br] \"        Dual or primal formulation. Dual formulation is only implemented for\" [:br] \"        l2 penalty with liblinear solver. Prefer dual=False when\" [:br] \"        n_samples > n_features.\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        Tolerance for stopping criteria.\" [:br] \"\" [:br] \"    C : float, default=1.0\" [:br] \"        Inverse of regularization strength; must be a positive float.\" [:br] \"        Like in support vector machines, smaller values specify stronger\" [:br] \"        regularization.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Specifies if a constant (a.k.a. bias or intercept) should be\" [:br] \"        added to the decision function.\" [:br] \"\" [:br] \"    intercept_scaling : float, default=1\" [:br] \"        Useful only when the solver 'liblinear' is used\" [:br] \"        and self.fit_intercept is set to True. In this case, x becomes\" [:br] \"        [x, self.intercept_scaling],\" [:br] \"        i.e. a \\\"synthetic\\\" feature with constant value equal to\" [:br] \"        intercept_scaling is appended to the instance vector.\" [:br] \"        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\" [:br] \"\" [:br] \"        Note! the synthetic feature weight is subject to l1/l2 regularization\" [:br] \"        as all other features.\" [:br] \"        To lessen the effect of regularization on synthetic feature weight\" [:br] \"        (and therefore on the intercept) intercept_scaling has to be increased.\" [:br] \"\" [:br] \"    class_weight : dict or 'balanced', default=None\" [:br] \"        Weights associated with classes in the form ``{class_label: weight}``.\" [:br] \"        If not given, all classes are supposed to have weight one.\" [:br] \"\" [:br] \"        The \\\"balanced\\\" mode uses the values of y to automatically adjust\" [:br] \"        weights inversely proportional to class frequencies in the input data\" [:br] \"        as ``n_samples / (n_classes * np.bincount(y))``.\" [:br] \"\" [:br] \"        Note that these weights will be multiplied with sample_weight (passed\" [:br] \"        through the fit method) if sample_weight is specified.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"           *class_weight='balanced'*\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\" [:br] \"        data. See :term:`Glossary <random_state>` for details.\" [:br] \"\" [:br] \"    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\" [:br] \"\" [:br] \"        Algorithm to use in the optimization problem.\" [:br] \"\" [:br] \"        - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\" [:br] \"          'saga' are faster for large ones.\" [:br] \"        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\" [:br] \"          handle multinomial loss; 'liblinear' is limited to one-versus-rest\" [:br] \"          schemes.\" [:br] \"        - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\" [:br] \"        - 'liblinear' and 'saga' also handle L1 penalty\" [:br] \"        - 'saga' also supports 'elasticnet' penalty\" [:br] \"        - 'liblinear' does not support setting ``penalty='none'``\" [:br] \"\" [:br] \"        Note that 'sag' and 'saga' fast convergence is only guaranteed on\" [:br] \"        features with approximately the same scale. You can\" [:br] \"        preprocess the data with a scaler from sklearn.preprocessing.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"           Stochastic Average Gradient descent solver.\" [:br] \"        .. versionadded:: 0.19\" [:br] \"           SAGA solver.\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"            The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\" [:br] \"\" [:br] \"    max_iter : int, default=100\" [:br] \"        Maximum number of iterations taken for the solvers to converge.\" [:br] \"\" [:br] \"    multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\" [:br] \"        If the option chosen is 'ovr', then a binary problem is fit for each\" [:br] \"        label. For 'multinomial' the loss minimised is the multinomial loss fit\" [:br] \"        across the entire probability distribution, *even when the data is\" [:br] \"        binary*. 'multinomial' is unavailable when solver='liblinear'.\" [:br] \"        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\" [:br] \"        and otherwise selects 'multinomial'.\" [:br] \"\" [:br] \"        .. versionadded:: 0.18\" [:br] \"           Stochastic Average Gradient descent solver for 'multinomial' case.\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"            Default changed from 'ovr' to 'auto' in 0.22.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        For the liblinear and lbfgs solvers set verbose to any positive\" [:br] \"        number for verbosity.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to True, reuse the solution of the previous call to fit as\" [:br] \"        initialization, otherwise, just erase the previous solution.\" [:br] \"        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        Number of CPU cores used when parallelizing over classes if\" [:br] \"        multi_class='ovr'\\\". This parameter is ignored when the ``solver`` is\" [:br] \"        set to 'liblinear' regardless of whether 'multi_class' is specified or\" [:br] \"        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\" [:br] \"        context. ``-1`` means using all processors.\" [:br] \"        See :term:`Glossary <n_jobs>` for more details.\" [:br] \"\" [:br] \"    l1_ratio : float, default=None\" [:br] \"        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\" [:br] \"        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\" [:br] \"        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\" [:br] \"        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\" [:br] \"        combination of L1 and L2.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes, )\" [:br] \"        A list of class labels known to the classifier.\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\" [:br] \"        Coefficient of the features in the decision function.\" [:br] \"\" [:br] \"        `coef_` is of shape (1, n_features) when the given problem is binary.\" [:br] \"        In particular, when `multi_class='multinomial'`, `coef_` corresponds\" [:br] \"        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape (1,) or (n_classes,)\" [:br] \"        Intercept (a.k.a. bias) added to the decision function.\" [:br] \"\" [:br] \"        If `fit_intercept` is set to False, the intercept is set to zero.\" [:br] \"        `intercept_` is of shape (1,) when the given problem is binary.\" [:br] \"        In particular, when `multi_class='multinomial'`, `intercept_`\" [:br] \"        corresponds to outcome 1 (True) and `-intercept_` corresponds to\" [:br] \"        outcome 0 (False).\" [:br] \"\" [:br] \"    n_iter_ : ndarray of shape (n_classes,) or (1, )\" [:br] \"        Actual number of iterations for all classes. If binary or multinomial,\" [:br] \"        it returns only 1 element. For liblinear solver, only the maximum\" [:br] \"        number of iteration across all classes is given.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.20\" [:br] \"\" [:br] \"            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\" [:br] \"            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    SGDClassifier : Incrementally trained logistic regression (when given\" [:br] \"        the parameter ``loss=\\\"log\\\"``).\" [:br] \"    LogisticRegressionCV : Logistic regression with built-in cross validation.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The underlying C implementation uses a random number generator to\" [:br] \"    select features when fitting the model. It is thus not uncommon,\" [:br] \"    to have slightly different results for the same input data. If\" [:br] \"    that happens, try with a smaller tol parameter.\" [:br] \"\" [:br] \"    Predict output may not match that of standalone liblinear in certain\" [:br] \"    cases. See :ref:`differences from liblinear <liblinear_differences>`\" [:br] \"    in the narrative documentation.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"\" [:br] \"    L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\" [:br] \"        Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\" [:br] \"        http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\" [:br] \"\" [:br] \"    LIBLINEAR -- A Library for Large Linear Classification\" [:br] \"        https://www.csie.ntu.edu.tw/~cjlin/liblinear/\" [:br] \"\" [:br] \"    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\" [:br] \"        Minimizing Finite Sums with the Stochastic Average Gradient\" [:br] \"        https://hal.inria.fr/hal-00860051/document\" [:br] \"\" [:br] \"    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\" [:br] \"        SAGA: A Fast Incremental Gradient Method With Support\" [:br] \"        for Non-Strongly Convex Composite Objectives\" [:br] \"        https://arxiv.org/abs/1407.0202\" [:br] \"\" [:br] \"    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\" [:br] \"        methods for logistic regression and maximum entropy models.\" [:br] \"        Machine Learning 85(1-2):41-75.\" [:br] \"        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import load_iris\" [:br] \"    >>> from sklearn.linear_model import LogisticRegression\" [:br] \"    >>> X, y = load_iris(return_X_y=True)\" [:br] \"    >>> clf = LogisticRegression(random_state=0).fit(X, y)\" [:br] \"    >>> clf.predict(X[:2, :])\" [:br] \"    array([0, 0])\" [:br] \"    >>> clf.predict_proba(X[:2, :])\" [:br] \"    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\" [:br] \"           [9.7...e-01, 2.8...e-02, ...e-08]])\" [:br] \"    >>> clf.score(X, y)\" [:br] \"    0.97...\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/logistic-regression-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [17 2]:\\n\\n|              :name |  :default |\\n|--------------------|-----------|\\n|             :refit |      true |\\n|           :scoring |           |\\n|               :tol | 0.0001000 |\\n| :intercept-scaling |     1.000 |\\n|       :multi-class |      auto |\\n|            :solver |     lbfgs |\\n|           :penalty |        l2 |\\n|          :max-iter |       100 |\\n|            :n-jobs |           |\\n|      :random-state |           |\\n|              :dual |     false |\\n|     :fit-intercept |      true |\\n|                :cv |           |\\n|                :cs |        10 |\\n|      :class-weight |           |\\n|           :verbose |         0 |\\n|        :l-1-ratios |           |\\n\"]]] [:span (\"Logistic Regression CV (aka logit, MaxEnt) classifier.\" [:br] \"\" [:br] \"    See glossary entry for :term:`cross-validation estimator`.\" [:br] \"\" [:br] \"    This class implements logistic regression using liblinear, newton-cg, sag\" [:br] \"    of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\" [:br] \"    regularization with primal formulation. The liblinear solver supports both\" [:br] \"    L1 and L2 regularization, with a dual formulation only for the L2 penalty.\" [:br] \"    Elastic-Net penalty is only supported by the saga solver.\" [:br] \"\" [:br] \"    For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\" [:br] \"    is selected by the cross-validator\" [:br] \"    :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\" [:br] \"    using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\" [:br] \"    solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <logistic_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    Cs : int or list of floats, default=10\" [:br] \"        Each of the values in Cs describes the inverse of regularization\" [:br] \"        strength. If Cs is as an int, then a grid of Cs values are chosen\" [:br] \"        in a logarithmic scale between 1e-4 and 1e4.\" [:br] \"        Like in support vector machines, smaller values specify stronger\" [:br] \"        regularization.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Specifies if a constant (a.k.a. bias or intercept) should be\" [:br] \"        added to the decision function.\" [:br] \"\" [:br] \"    cv : int or cross-validation generator, default=None\" [:br] \"        The default cross-validation generator used is Stratified K-Folds.\" [:br] \"        If an integer is provided, then it is the number of folds used.\" [:br] \"        See the module :mod:`sklearn.model_selection` module for the\" [:br] \"        list of possible cross-validation objects.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"            ``cv`` default value if None changed from 3-fold to 5-fold.\" [:br] \"\" [:br] \"    dual : bool, default=False\" [:br] \"        Dual or primal formulation. Dual formulation is only implemented for\" [:br] \"        l2 penalty with liblinear solver. Prefer dual=False when\" [:br] \"        n_samples > n_features.\" [:br] \"\" [:br] \"    penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\" [:br] \"        Used to specify the norm used in the penalization. The 'newton-cg',\" [:br] \"        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\" [:br] \"        only supported by the 'saga' solver.\" [:br] \"\" [:br] \"    scoring : str or callable, default=None\" [:br] \"        A string (see model evaluation documentation) or\" [:br] \"        a scorer callable object / function with signature\" [:br] \"        ``scorer(estimator, X, y)``. For a list of scoring functions\" [:br] \"        that can be used, look at :mod:`sklearn.metrics`. The\" [:br] \"        default scoring option used is 'accuracy'.\" [:br] \"\" [:br] \"    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\" [:br] \"\" [:br] \"        Algorithm to use in the optimization problem.\" [:br] \"\" [:br] \"        - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\" [:br] \"          'saga' are faster for large ones.\" [:br] \"        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\" [:br] \"          handle multinomial loss; 'liblinear' is limited to one-versus-rest\" [:br] \"          schemes.\" [:br] \"        - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\" [:br] \"          'liblinear' and 'saga' handle L1 penalty.\" [:br] \"        - 'liblinear' might be slower in LogisticRegressionCV because it does\" [:br] \"          not handle warm-starting.\" [:br] \"\" [:br] \"        Note that 'sag' and 'saga' fast convergence is only guaranteed on\" [:br] \"        features with approximately the same scale. You can preprocess the data\" [:br] \"        with a scaler from sklearn.preprocessing.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"           Stochastic Average Gradient descent solver.\" [:br] \"        .. versionadded:: 0.19\" [:br] \"           SAGA solver.\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        Tolerance for stopping criteria.\" [:br] \"\" [:br] \"    max_iter : int, default=100\" [:br] \"        Maximum number of iterations of the optimization algorithm.\" [:br] \"\" [:br] \"    class_weight : dict or 'balanced', default=None\" [:br] \"        Weights associated with classes in the form ``{class_label: weight}``.\" [:br] \"        If not given, all classes are supposed to have weight one.\" [:br] \"\" [:br] \"        The \\\"balanced\\\" mode uses the values of y to automatically adjust\" [:br] \"        weights inversely proportional to class frequencies in the input data\" [:br] \"        as ``n_samples / (n_classes * np.bincount(y))``.\" [:br] \"\" [:br] \"        Note that these weights will be multiplied with sample_weight (passed\" [:br] \"        through the fit method) if sample_weight is specified.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"           class_weight == 'balanced'\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        Number of CPU cores used during the cross-validation loop.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\" [:br] \"        positive number for verbosity.\" [:br] \"\" [:br] \"    refit : bool, default=True\" [:br] \"        If set to True, the scores are averaged across all folds, and the\" [:br] \"        coefs and the C that corresponds to the best score is taken, and a\" [:br] \"        final refit is done using these parameters.\" [:br] \"        Otherwise the coefs, intercepts and C that correspond to the\" [:br] \"        best scores across folds are averaged.\" [:br] \"\" [:br] \"    intercept_scaling : float, default=1\" [:br] \"        Useful only when the solver 'liblinear' is used\" [:br] \"        and self.fit_intercept is set to True. In this case, x becomes\" [:br] \"        [x, self.intercept_scaling],\" [:br] \"        i.e. a \\\"synthetic\\\" feature with constant value equal to\" [:br] \"        intercept_scaling is appended to the instance vector.\" [:br] \"        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\" [:br] \"\" [:br] \"        Note! the synthetic feature weight is subject to l1/l2 regularization\" [:br] \"        as all other features.\" [:br] \"        To lessen the effect of regularization on synthetic feature weight\" [:br] \"        (and therefore on the intercept) intercept_scaling has to be increased.\" [:br] \"\" [:br] \"    multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\" [:br] \"        If the option chosen is 'ovr', then a binary problem is fit for each\" [:br] \"        label. For 'multinomial' the loss minimised is the multinomial loss fit\" [:br] \"        across the entire probability distribution, *even when the data is\" [:br] \"        binary*. 'multinomial' is unavailable when solver='liblinear'.\" [:br] \"        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\" [:br] \"        and otherwise selects 'multinomial'.\" [:br] \"\" [:br] \"        .. versionadded:: 0.18\" [:br] \"           Stochastic Average Gradient descent solver for 'multinomial' case.\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"            Default changed from 'ovr' to 'auto' in 0.22.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\" [:br] \"        Note that this only applies to the solver and not the cross-validation\" [:br] \"        generator. See :term:`Glossary <random_state>` for details.\" [:br] \"\" [:br] \"    l1_ratios : list of float, default=None\" [:br] \"        The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\" [:br] \"        Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\" [:br] \"        using ``penalty='l2'``, while 1 is equivalent to using\" [:br] \"        ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\" [:br] \"        of L1 and L2.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    classes_ : ndarray of shape (n_classes, )\" [:br] \"        A list of class labels known to the classifier.\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\" [:br] \"        Coefficient of the features in the decision function.\" [:br] \"\" [:br] \"        `coef_` is of shape (1, n_features) when the given problem\" [:br] \"        is binary.\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape (1,) or (n_classes,)\" [:br] \"        Intercept (a.k.a. bias) added to the decision function.\" [:br] \"\" [:br] \"        If `fit_intercept` is set to False, the intercept is set to zero.\" [:br] \"        `intercept_` is of shape(1,) when the problem is binary.\" [:br] \"\" [:br] \"    Cs_ : ndarray of shape (n_cs)\" [:br] \"        Array of C i.e. inverse of regularization parameter values used\" [:br] \"        for cross-validation.\" [:br] \"\" [:br] \"    l1_ratios_ : ndarray of shape (n_l1_ratios)\" [:br] \"        Array of l1_ratios used for cross-validation. If no l1_ratio is used\" [:br] \"        (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\" [:br] \"\" [:br] \"    coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\" [:br] \"        dict with classes as the keys, and the path of coefficients obtained\" [:br] \"        during cross-validating across each fold and then across each Cs\" [:br] \"        after doing an OvR for the corresponding class as values.\" [:br] \"        If the 'multi_class' option is set to 'multinomial', then\" [:br] \"        the coefs_paths are the coefficients corresponding to each class.\" [:br] \"        Each dict value has shape ``(n_folds, n_cs, n_features)`` or\" [:br] \"        ``(n_folds, n_cs, n_features + 1)`` depending on whether the\" [:br] \"        intercept is fit or not. If ``penalty='elasticnet'``, the shape is\" [:br] \"        ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\" [:br] \"        ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\" [:br] \"\" [:br] \"    scores_ : dict\" [:br] \"        dict with classes as the keys, and the values as the\" [:br] \"        grid of scores obtained during cross-validating each fold, after doing\" [:br] \"        an OvR for the corresponding class. If the 'multi_class' option\" [:br] \"        given is 'multinomial' then the same scores are repeated across\" [:br] \"        all classes, since this is the multinomial class. Each dict value\" [:br] \"        has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if\" [:br] \"        ``penalty='elasticnet'``.\" [:br] \"\" [:br] \"    C_ : ndarray of shape (n_classes,) or (n_classes - 1,)\" [:br] \"        Array of C that maps to the best scores across every class. If refit is\" [:br] \"        set to False, then for each class, the best C is the average of the\" [:br] \"        C's that correspond to the best scores for each fold.\" [:br] \"        `C_` is of shape(n_classes,) when the problem is binary.\" [:br] \"\" [:br] \"    l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\" [:br] \"        Array of l1_ratio that maps to the best scores across every class. If\" [:br] \"        refit is set to False, then for each class, the best l1_ratio is the\" [:br] \"        average of the l1_ratio's that correspond to the best scores for each\" [:br] \"        fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\" [:br] \"\" [:br] \"    n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\" [:br] \"        Actual number of iterations for all classes, folds and Cs.\" [:br] \"        In the binary or multinomial cases, the first dimension is equal to 1.\" [:br] \"        If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\" [:br] \"        n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\" [:br] \"\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import load_iris\" [:br] \"    >>> from sklearn.linear_model import LogisticRegressionCV\" [:br] \"    >>> X, y = load_iris(return_X_y=True)\" [:br] \"    >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\" [:br] \"    >>> clf.predict(X[:2, :])\" [:br] \"    array([0, 0])\" [:br] \"    >>> clf.predict_proba(X[:2, :]).shape\" [:br] \"    (2, 3)\" [:br] \"    >>> clf.score(X, y)\" [:br] \"    0.98...\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    LogisticRegression\" [:br] \"\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/mlp-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [23 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |        10 |\\n|       :learning-rate |  constant |\\n|          :activation |      relu |\\n|  :hidden-layer-sizes |     [100] |\\n|                 :tol | 0.0001000 |\\n|              :beta-2 |    0.9990 |\\n|      :early-stopping |     false |\\n|  :nesterovs-momentum |      true |\\n|          :batch-size |      auto |\\n|              :solver |      adam |\\n|             :shuffle |      true |\\n|             :power-t |    0.5000 |\\n|             :max-fun |     15000 |\\n|              :beta-1 |    0.9000 |\\n|            :max-iter |       200 |\\n|        :random-state |           |\\n|            :momentum |    0.9000 |\\n|  :learning-rate-init |  0.001000 |\\n|               :alpha | 0.0001000 |\\n|          :warm-start |     false |\\n| :validation-fraction |    0.1000 |\\n|             :verbose |     false |\\n|             :epsilon | 1.000E-08 |\\n\"]]] [:span (\"Multi-layer Perceptron classifier.\" [:br] \"\" [:br] \"    This model optimizes the log-loss function using LBFGS or stochastic\" [:br] \"    gradient descent.\" [:br] \"\" [:br] \"    .. versionadded:: 0.18\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    hidden_layer_sizes : tuple, length = n_layers - 2, default=(100,)\" [:br] \"        The ith element represents the number of neurons in the ith\" [:br] \"        hidden layer.\" [:br] \"\" [:br] \"    activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\" [:br] \"        Activation function for the hidden layer.\" [:br] \"\" [:br] \"        - 'identity', no-op activation, useful to implement linear bottleneck,\" [:br] \"          returns f(x) = x\" [:br] \"\" [:br] \"        - 'logistic', the logistic sigmoid function,\" [:br] \"          returns f(x) = 1 / (1 + exp(-x)).\" [:br] \"\" [:br] \"        - 'tanh', the hyperbolic tan function,\" [:br] \"          returns f(x) = tanh(x).\" [:br] \"\" [:br] \"        - 'relu', the rectified linear unit function,\" [:br] \"          returns f(x) = max(0, x)\" [:br] \"\" [:br] \"    solver : {'lbfgs', 'sgd', 'adam'}, default='adam'\" [:br] \"        The solver for weight optimization.\" [:br] \"\" [:br] \"        - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\" [:br] \"\" [:br] \"        - 'sgd' refers to stochastic gradient descent.\" [:br] \"\" [:br] \"        - 'adam' refers to a stochastic gradient-based optimizer proposed\" [:br] \"          by Kingma, Diederik, and Jimmy Ba\" [:br] \"\" [:br] \"        Note: The default solver 'adam' works pretty well on relatively\" [:br] \"        large datasets (with thousands of training samples or more) in terms of\" [:br] \"        both training time and validation score.\" [:br] \"        For small datasets, however, 'lbfgs' can converge faster and perform\" [:br] \"        better.\" [:br] \"\" [:br] \"    alpha : float, default=0.0001\" [:br] \"        L2 penalty (regularization term) parameter.\" [:br] \"\" [:br] \"    batch_size : int, default='auto'\" [:br] \"        Size of minibatches for stochastic optimizers.\" [:br] \"        If the solver is 'lbfgs', the classifier will not use minibatch.\" [:br] \"        When set to \\\"auto\\\", `batch_size=min(200, n_samples)`\" [:br] \"\" [:br] \"    learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\" [:br] \"        Learning rate schedule for weight updates.\" [:br] \"\" [:br] \"        - 'constant' is a constant learning rate given by\" [:br] \"          'learning_rate_init'.\" [:br] \"\" [:br] \"        - 'invscaling' gradually decreases the learning rate at each\" [:br] \"          time step 't' using an inverse scaling exponent of 'power_t'.\" [:br] \"          effective_learning_rate = learning_rate_init / pow(t, power_t)\" [:br] \"\" [:br] \"        - 'adaptive' keeps the learning rate constant to\" [:br] \"          'learning_rate_init' as long as training loss keeps decreasing.\" [:br] \"          Each time two consecutive epochs fail to decrease training loss by at\" [:br] \"          least tol, or fail to increase validation score by at least tol if\" [:br] \"          'early_stopping' is on, the current learning rate is divided by 5.\" [:br] \"\" [:br] \"        Only used when ``solver='sgd'``.\" [:br] \"\" [:br] \"    learning_rate_init : double, default=0.001\" [:br] \"        The initial learning rate used. It controls the step-size\" [:br] \"        in updating the weights. Only used when solver='sgd' or 'adam'.\" [:br] \"\" [:br] \"    power_t : double, default=0.5\" [:br] \"        The exponent for inverse scaling learning rate.\" [:br] \"        It is used in updating effective learning rate when the learning_rate\" [:br] \"        is set to 'invscaling'. Only used when solver='sgd'.\" [:br] \"\" [:br] \"    max_iter : int, default=200\" [:br] \"        Maximum number of iterations. The solver iterates until convergence\" [:br] \"        (determined by 'tol') or this number of iterations. For stochastic\" [:br] \"        solvers ('sgd', 'adam'), note that this determines the number of epochs\" [:br] \"        (how many times each data point will be used), not the number of\" [:br] \"        gradient steps.\" [:br] \"\" [:br] \"    shuffle : bool, default=True\" [:br] \"        Whether to shuffle samples in each iteration. Only used when\" [:br] \"        solver='sgd' or 'adam'.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Determines random number generation for weights and bias\" [:br] \"        initialization, train-test split if early stopping is used, and batch\" [:br] \"        sampling when solver='sgd' or 'adam'.\" [:br] \"        Pass an int for reproducible results across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        Tolerance for the optimization. When the loss or score is not improving\" [:br] \"        by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\" [:br] \"        unless ``learning_rate`` is set to 'adaptive', convergence is\" [:br] \"        considered to be reached and training stops.\" [:br] \"\" [:br] \"    verbose : bool, default=False\" [:br] \"        Whether to print progress messages to stdout.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to True, reuse the solution of the previous\" [:br] \"        call to fit as initialization, otherwise, just erase the\" [:br] \"        previous solution. See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    momentum : float, default=0.9\" [:br] \"        Momentum for gradient descent update. Should be between 0 and 1. Only\" [:br] \"        used when solver='sgd'.\" [:br] \"\" [:br] \"    nesterovs_momentum : boolean, default=True\" [:br] \"        Whether to use Nesterov's momentum. Only used when solver='sgd' and\" [:br] \"        momentum > 0.\" [:br] \"\" [:br] \"    early_stopping : bool, default=False\" [:br] \"        Whether to use early stopping to terminate training when validation\" [:br] \"        score is not improving. If set to true, it will automatically set\" [:br] \"        aside 10% of training data as validation and terminate training when\" [:br] \"        validation score is not improving by at least tol for\" [:br] \"        ``n_iter_no_change`` consecutive epochs. The split is stratified,\" [:br] \"        except in a multilabel setting.\" [:br] \"        Only effective when solver='sgd' or 'adam'\" [:br] \"\" [:br] \"    validation_fraction : float, default=0.1\" [:br] \"        The proportion of training data to set aside as validation set for\" [:br] \"        early stopping. Must be between 0 and 1.\" [:br] \"        Only used if early_stopping is True\" [:br] \"\" [:br] \"    beta_1 : float, default=0.9\" [:br] \"        Exponential decay rate for estimates of first moment vector in adam,\" [:br] \"        should be in [0, 1). Only used when solver='adam'\" [:br] \"\" [:br] \"    beta_2 : float, default=0.999\" [:br] \"        Exponential decay rate for estimates of second moment vector in adam,\" [:br] \"        should be in [0, 1). Only used when solver='adam'\" [:br] \"\" [:br] \"    epsilon : float, default=1e-8\" [:br] \"        Value for numerical stability in adam. Only used when solver='adam'\" [:br] \"\" [:br] \"    n_iter_no_change : int, default=10\" [:br] \"        Maximum number of epochs to not meet ``tol`` improvement.\" [:br] \"        Only effective when solver='sgd' or 'adam'\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    max_fun : int, default=15000\" [:br] \"        Only used when solver='lbfgs'. Maximum number of loss function calls.\" [:br] \"        The solver iterates until convergence (determined by 'tol'), number\" [:br] \"        of iterations reaches max_iter, or this number of loss function calls.\" [:br] \"        Note that number of loss function calls will be greater than or equal\" [:br] \"        to the number of iterations for the `MLPClassifier`.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    classes_ : ndarray or list of ndarray of shape (n_classes,)\" [:br] \"        Class labels for each output.\" [:br] \"\" [:br] \"    loss_ : float\" [:br] \"        The current loss computed with the loss function.\" [:br] \"\" [:br] \"    coefs_ : list, length n_layers - 1\" [:br] \"        The ith element in the list represents the weight matrix corresponding\" [:br] \"        to layer i.\" [:br] \"\" [:br] \"    intercepts_ : list, length n_layers - 1\" [:br] \"        The ith element in the list represents the bias vector corresponding to\" [:br] \"        layer i + 1.\" [:br] \"\" [:br] \"    n_iter_ : int,\" [:br] \"        The number of iterations the solver has ran.\" [:br] \"\" [:br] \"    n_layers_ : int\" [:br] \"        Number of layers.\" [:br] \"\" [:br] \"    n_outputs_ : int\" [:br] \"        Number of outputs.\" [:br] \"\" [:br] \"    out_activation_ : string\" [:br] \"        Name of the output activation function.\" [:br] \"\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.neural_network import MLPClassifier\" [:br] \"    >>> from sklearn.datasets import make_classification\" [:br] \"    >>> from sklearn.model_selection import train_test_split\" [:br] \"    >>> X, y = make_classification(n_samples=100, random_state=1)\" [:br] \"    >>> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\" [:br] \"    ...                                                     random_state=1)\" [:br] \"    >>> clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\" [:br] \"    >>> clf.predict_proba(X_test[:1])\" [:br] \"    array([[0.038..., 0.961...]])\" [:br] \"    >>> clf.predict(X_test[:5, :])\" [:br] \"    array([1, 0, 1, 0, 1])\" [:br] \"    >>> clf.score(X_test, y_test)\" [:br] \"    0.8...\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    MLPClassifier trains iteratively since at each time step\" [:br] \"    the partial derivatives of the loss function with respect to the model\" [:br] \"    parameters are computed to update the parameters.\" [:br] \"\" [:br] \"    It can also have a regularization term added to the loss function\" [:br] \"    that shrinks model parameters to prevent overfitting.\" [:br] \"\" [:br] \"    This implementation works with data represented as dense numpy arrays or\" [:br] \"    sparse scipy arrays of floating point values.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    Hinton, Geoffrey E.\" [:br] \"        \\\"Connectionist learning procedures.\\\" Artificial intelligence 40.1\" [:br] \"        (1989): 185-234.\" [:br] \"\" [:br] \"    Glorot, Xavier, and Yoshua Bengio. \\\"Understanding the difficulty of\" [:br] \"        training deep feedforward neural networks.\\\" International Conference\" [:br] \"        on Artificial Intelligence and Statistics. 2010.\" [:br] \"\" [:br] \"    He, Kaiming, et al. \\\"Delving deep into rectifiers: Surpassing human-level\" [:br] \"        performance on imagenet classification.\\\" arXiv preprint\" [:br] \"        arXiv:1502.01852 (2015).\" [:br] \"\" [:br] \"    Kingma, Diederik, and Jimmy Ba. \\\"Adam: A method for stochastic\" [:br] \"        optimization.\\\" arXiv preprint arXiv:1412.6980 (2014).\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/multinomial-nb\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"230px\"}} [:p/markdown \"_unnamed [3 2]:\\n\\n|        :name | :default |\\n|--------------|----------|\\n|       :alpha |    1.000 |\\n| :class-prior |          |\\n|   :fit-prior |     true |\\n\"]]] [:span (\"\" [:br] \"    Naive Bayes classifier for multinomial models\" [:br] \"\" [:br] \"    The multinomial Naive Bayes classifier is suitable for classification with\" [:br] \"    discrete features (e.g., word counts for text classification). The\" [:br] \"    multinomial distribution normally requires integer feature counts. However,\" [:br] \"    in practice, fractional counts such as tf-idf may also work.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : float, default=1.0\" [:br] \"        Additive (Laplace/Lidstone) smoothing parameter\" [:br] \"        (0 for no smoothing).\" [:br] \"\" [:br] \"    fit_prior : bool, default=True\" [:br] \"        Whether to learn class prior probabilities or not.\" [:br] \"        If false, a uniform prior will be used.\" [:br] \"\" [:br] \"    class_prior : array-like of shape (n_classes,), default=None\" [:br] \"        Prior probabilities of the classes. If specified the priors are not\" [:br] \"        adjusted according to the data.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    class_count_ : ndarray of shape (n_classes,)\" [:br] \"        Number of samples encountered for each class during fitting. This\" [:br] \"        value is weighted by the sample weight when provided.\" [:br] \"\" [:br] \"    class_log_prior_ : ndarray of shape (n_classes, )\" [:br] \"        Smoothed empirical log probability for each class.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        Class labels known to the classifier\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (n_classes, n_features)\" [:br] \"        Mirrors ``feature_log_prob_`` for interpreting MultinomialNB\" [:br] \"        as a linear model.\" [:br] \"\" [:br] \"    feature_count_ : ndarray of shape (n_classes, n_features)\" [:br] \"        Number of samples encountered for each (class, feature)\" [:br] \"        during fitting. This value is weighted by the sample weight when\" [:br] \"        provided.\" [:br] \"\" [:br] \"    feature_log_prob_ : ndarray of shape (n_classes, n_features)\" [:br] \"        Empirical log probability of features\" [:br] \"        given a class, ``P(x_i|y)``.\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape (n_classes, )\" [:br] \"        Mirrors ``class_log_prior_`` for interpreting MultinomialNB\" [:br] \"        as a linear model.\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        Number of features of each sample.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> rng = np.random.RandomState(1)\" [:br] \"    >>> X = rng.randint(5, size=(6, 100))\" [:br] \"    >>> y = np.array([1, 2, 3, 4, 5, 6])\" [:br] \"    >>> from sklearn.naive_bayes import MultinomialNB\" [:br] \"    >>> clf = MultinomialNB()\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    MultinomialNB()\" [:br] \"    >>> print(clf.predict(X[2:3]))\" [:br] \"    [3]\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    For the rationale behind the names `coef_` and `intercept_`, i.e.\" [:br] \"    naive Bayes as a linear classifier, see J. Rennie et al. (2003),\" [:br] \"    Tackling the poor assumptions of naive Bayes text classifiers, ICML.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\" [:br] \"    Information Retrieval. Cambridge University Press, pp. 234-265.\" [:br] \"    https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/nearest-centroid\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"184px\"}} [:p/markdown \"_unnamed [2 2]:\\n\\n|             :name |  :default |\\n|-------------------|-----------|\\n|           :metric | euclidean |\\n| :shrink-threshold |           |\\n\"]]] [:span (\"Nearest centroid classifier.\" [:br] \"\" [:br] \"    Each class is represented by its centroid, with test samples classified to\" [:br] \"    the class with the nearest centroid.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <nearest_centroid_classifier>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    metric : str or callable\" [:br] \"        The metric to use when calculating distance between instances in a\" [:br] \"        feature array. If metric is a string or callable, it must be one of\" [:br] \"        the options allowed by metrics.pairwise.pairwise_distances for its\" [:br] \"        metric parameter.\" [:br] \"        The centroids for the samples corresponding to each class is the point\" [:br] \"        from which the sum of the distances (according to the metric) of all\" [:br] \"        samples that belong to that particular class are minimized.\" [:br] \"        If the \\\"manhattan\\\" metric is provided, this centroid is the median and\" [:br] \"        for all other metrics, the centroid is now set to be the mean.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.19\" [:br] \"            ``metric='precomputed'`` was deprecated and now raises an error\" [:br] \"\" [:br] \"    shrink_threshold : float, default=None\" [:br] \"        Threshold for shrinking centroids to remove features.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    centroids_ : array-like of shape (n_classes, n_features)\" [:br] \"        Centroid of each class.\" [:br] \"\" [:br] \"    classes_ : array of shape (n_classes,)\" [:br] \"        The unique classes labels.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.neighbors import NearestCentroid\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\" [:br] \"    >>> y = np.array([1, 1, 1, 2, 2, 2])\" [:br] \"    >>> clf = NearestCentroid()\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    NearestCentroid()\" [:br] \"    >>> print(clf.predict([[-0.8, -1]]))\" [:br] \"    [1]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    sklearn.neighbors.KNeighborsClassifier: nearest neighbors classifier\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    When used for text classification with tf-idf vectors, this classifier is\" [:br] \"    also known as the Rocchio classifier.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of\" [:br] \"    multiple cancer types by shrunken centroids of gene expression. Proceedings\" [:br] \"    of the National Academy of Sciences of the United States of America,\" [:br] \"    99(10), 6567-6572. The National Academy of Sciences.\" [:br] \"\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/nu-svc\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [15 2]:\\n\\n|                    :name | :default |\\n|--------------------------|----------|\\n|              :break-ties |    false |\\n|                  :kernel |      rbf |\\n|                   :gamma |    scale |\\n|                  :degree |        3 |\\n| :decision-function-shape |      ovr |\\n|             :probability |    false |\\n|                     :tol | 0.001000 |\\n|                      :nu |   0.5000 |\\n|               :shrinking |     true |\\n|                :max-iter |       -1 |\\n|            :random-state |          |\\n|                  :coef-0 |    0.000 |\\n|            :class-weight |          |\\n|              :cache-size |      200 |\\n|                 :verbose |    false |\\n\"]]] [:span (\"Nu-Support Vector Classification.\" [:br] \"\" [:br] \"    Similar to SVC but uses a parameter to control the number of support\" [:br] \"    vectors.\" [:br] \"\" [:br] \"    The implementation is based on libsvm.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <svm_classification>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    nu : float, default=0.5\" [:br] \"        An upper bound on the fraction of margin errors (see :ref:`User Guide\" [:br] \"        <nu_svc>`) and a lower bound of the fraction of support vectors.\" [:br] \"        Should be in the interval (0, 1].\" [:br] \"\" [:br] \"    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\" [:br] \"         Specifies the kernel type to be used in the algorithm.\" [:br] \"         It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\" [:br] \"         a callable.\" [:br] \"         If none is given, 'rbf' will be used. If a callable is given it is\" [:br] \"         used to precompute the kernel matrix.\" [:br] \"\" [:br] \"    degree : int, default=3\" [:br] \"        Degree of the polynomial kernel function ('poly').\" [:br] \"        Ignored by all other kernels.\" [:br] \"\" [:br] \"    gamma : {'scale', 'auto'} or float, default='scale'\" [:br] \"        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\" [:br] \"\" [:br] \"        - if ``gamma='scale'`` (default) is passed then it uses\" [:br] \"          1 / (n_features * X.var()) as value of gamma,\" [:br] \"        - if 'auto', uses 1 / n_features.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"           The default value of ``gamma`` changed from 'auto' to 'scale'.\" [:br] \"\" [:br] \"    coef0 : float, default=0.0\" [:br] \"        Independent term in kernel function.\" [:br] \"        It is only significant in 'poly' and 'sigmoid'.\" [:br] \"\" [:br] \"    shrinking : bool, default=True\" [:br] \"        Whether to use the shrinking heuristic.\" [:br] \"        See the :ref:`User Guide <shrinking_svm>`.\" [:br] \"\" [:br] \"    probability : bool, default=False\" [:br] \"        Whether to enable probability estimates. This must be enabled prior\" [:br] \"        to calling `fit`, will slow down that method as it internally uses\" [:br] \"        5-fold cross-validation, and `predict_proba` may be inconsistent with\" [:br] \"        `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\" [:br] \"\" [:br] \"    tol : float, default=1e-3\" [:br] \"        Tolerance for stopping criterion.\" [:br] \"\" [:br] \"    cache_size : float, default=200\" [:br] \"        Specify the size of the kernel cache (in MB).\" [:br] \"\" [:br] \"    class_weight : {dict, 'balanced'}, default=None\" [:br] \"        Set the parameter C of class i to class_weight[i]*C for\" [:br] \"        SVC. If not given, all classes are supposed to have\" [:br] \"        weight one. The \\\"balanced\\\" mode uses the values of y to automatically\" [:br] \"        adjust weights inversely proportional to class frequencies as\" [:br] \"        ``n_samples / (n_classes * np.bincount(y))``\" [:br] \"\" [:br] \"    verbose : bool, default=False\" [:br] \"        Enable verbose output. Note that this setting takes advantage of a\" [:br] \"        per-process runtime setting in libsvm that, if enabled, may not work\" [:br] \"        properly in a multithreaded context.\" [:br] \"\" [:br] \"    max_iter : int, default=-1\" [:br] \"        Hard limit on iterations within solver, or -1 for no limit.\" [:br] \"\" [:br] \"    decision_function_shape : {'ovo', 'ovr'}, default='ovr'\" [:br] \"        Whether to return a one-vs-rest ('ovr') decision function of shape\" [:br] \"        (n_samples, n_classes) as all other classifiers, or the original\" [:br] \"        one-vs-one ('ovo') decision function of libsvm which has shape\" [:br] \"        (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\" [:br] \"        ('ovo') is always used as multi-class strategy. The parameter is\" [:br] \"        ignored for binary classification.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.19\" [:br] \"            decision_function_shape is 'ovr' by default.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"           *decision_function_shape='ovr'* is recommended.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.17\" [:br] \"           Deprecated *decision_function_shape='ovo' and None*.\" [:br] \"\" [:br] \"    break_ties : bool, default=False\" [:br] \"        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\" [:br] \"        :term:`predict` will break ties according to the confidence values of\" [:br] \"        :term:`decision_function`; otherwise the first class among the tied\" [:br] \"        classes is returned. Please note that breaking ties comes at a\" [:br] \"        relatively high computational cost compared to a simple predict.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    random_state : int or RandomState instance, default=None\" [:br] \"        Controls the pseudo random number generation for shuffling the data for\" [:br] \"        probability estimates. Ignored when `probability` is False.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    support_ : ndarray of shape (n_SV,)\" [:br] \"        Indices of support vectors.\" [:br] \"\" [:br] \"    support_vectors_ : ndarray of shape (n_SV, n_features)\" [:br] \"        Support vectors.\" [:br] \"\" [:br] \"    n_support_ : ndarray of shape (n_class), dtype=int32\" [:br] \"        Number of support vectors for each class.\" [:br] \"\" [:br] \"    dual_coef_ : ndarray of shape (n_class-1, n_SV)\" [:br] \"        Dual coefficients of the support vector in the decision\" [:br] \"        function (see :ref:`sgd_mathematical_formulation`), multiplied by\" [:br] \"        their targets.\" [:br] \"        For multiclass, coefficient for all 1-vs-1 classifiers.\" [:br] \"        The layout of the coefficients in the multiclass case is somewhat\" [:br] \"        non-trivial. See the :ref:`multi-class section of the User Guide\" [:br] \"        <svm_multi_class>` for details.\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (n_class * (n_class-1) / 2, n_features)\" [:br] \"        Weights assigned to the features (coefficients in the primal\" [:br] \"        problem). This is only available in the case of a linear kernel.\" [:br] \"\" [:br] \"        `coef_` is readonly property derived from `dual_coef_` and\" [:br] \"        `support_vectors_`.\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape (n_class * (n_class-1) / 2,)\" [:br] \"        Constants in decision function.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        The unique classes labels.\" [:br] \"\" [:br] \"    fit_status_ : int\" [:br] \"        0 if correctly fitted, 1 if the algorithm did not converge.\" [:br] \"\" [:br] \"    probA_ : ndarray of shape (n_class * (n_class-1) / 2,)\" [:br] \"    probB_ : ndarray of shape (n_class * (n_class-1) / 2,)\" [:br] \"        If `probability=True`, it corresponds to the parameters learned in\" [:br] \"        Platt scaling to produce probability estimates from decision values.\" [:br] \"        If `probability=False`, it's an empty array. Platt scaling uses the\" [:br] \"        logistic function\" [:br] \"        ``1 / (1 + exp(decision_value * probA_ + probB_))``\" [:br] \"        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\" [:br] \"        more information on the multiclass case and training procedure see\" [:br] \"        section 8 of [1]_.\" [:br] \"\" [:br] \"    class_weight_ : ndarray of shape (n_class,)\" [:br] \"        Multipliers of parameter C of each class.\" [:br] \"        Computed based on the ``class_weight`` parameter.\" [:br] \"\" [:br] \"    shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\" [:br] \"        Array dimensions of training vector ``X``.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\" [:br] \"    >>> y = np.array([1, 1, 2, 2])\" [:br] \"    >>> from sklearn.pipeline import make_pipeline\" [:br] \"    >>> from sklearn.preprocessing import StandardScaler\" [:br] \"    >>> from sklearn.svm import NuSVC\" [:br] \"    >>> clf = make_pipeline(StandardScaler(), NuSVC())\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    Pipeline(steps=[('standardscaler', StandardScaler()), ('nusvc', NuSVC())])\" [:br] \"    >>> print(clf.predict([[-0.8, -1]]))\" [:br] \"    [1]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    SVC\" [:br] \"        Support Vector Machine for classification using libsvm.\" [:br] \"\" [:br] \"    LinearSVC\" [:br] \"        Scalable linear Support Vector Machine for classification using\" [:br] \"        liblinear.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    .. [1] `LIBSVM: A Library for Support Vector Machines\" [:br] \"        <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\" [:br] \"\" [:br] \"    .. [2] `Platt, John (1999). \\\"Probabilistic outputs for support vector\" [:br] \"        machines and comparison to regularizedlikelihood methods.\\\"\" [:br] \"        <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/passive-aggressive-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [15 2]:\\n\\n|                :name | :default |\\n|----------------------|----------|\\n|    :n-iter-no-change |        5 |\\n|             :average |    false |\\n|                 :tol | 0.001000 |\\n|      :early-stopping |    false |\\n|             :shuffle |     true |\\n|                   :c |    1.000 |\\n|            :max-iter |     1000 |\\n|              :n-jobs |          |\\n|        :random-state |          |\\n|       :fit-intercept |     true |\\n|          :warm-start |    false |\\n| :validation-fraction |   0.1000 |\\n|        :class-weight |          |\\n|                :loss |    hinge |\\n|             :verbose |        0 |\\n\"]]] [:span (\"Passive Aggressive Classifier\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <passive_aggressive>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"\" [:br] \"    C : float\" [:br] \"        Maximum step size (regularization). Defaults to 1.0.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=False\" [:br] \"        Whether the intercept should be estimated or not. If False, the\" [:br] \"        data is assumed to be already centered.\" [:br] \"\" [:br] \"    max_iter : int, optional (default=1000)\" [:br] \"        The maximum number of passes over the training data (aka epochs).\" [:br] \"        It only impacts the behavior in the ``fit`` method, and not the\" [:br] \"        :meth:`partial_fit` method.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    tol : float or None, optional (default=1e-3)\" [:br] \"        The stopping criterion. If it is not None, the iterations will stop\" [:br] \"        when (loss > previous_loss - tol).\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    early_stopping : bool, default=False\" [:br] \"        Whether to use early stopping to terminate training when validation.\" [:br] \"        score is not improving. If set to True, it will automatically set aside\" [:br] \"        a stratified fraction of training data as validation and terminate\" [:br] \"        training when validation score is not improving by at least tol for\" [:br] \"        n_iter_no_change consecutive epochs.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    validation_fraction : float, default=0.1\" [:br] \"        The proportion of training data to set aside as validation set for\" [:br] \"        early stopping. Must be between 0 and 1.\" [:br] \"        Only used if early_stopping is True.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    n_iter_no_change : int, default=5\" [:br] \"        Number of iterations with no improvement to wait before early stopping.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    shuffle : bool, default=True\" [:br] \"        Whether or not the training data should be shuffled after each epoch.\" [:br] \"\" [:br] \"    verbose : integer, optional\" [:br] \"        The verbosity level\" [:br] \"\" [:br] \"    loss : string, optional\" [:br] \"        The loss function to be used:\" [:br] \"        hinge: equivalent to PA-I in the reference paper.\" [:br] \"        squared_hinge: equivalent to PA-II in the reference paper.\" [:br] \"\" [:br] \"    n_jobs : int or None, optional (default=None)\" [:br] \"        The number of CPUs to use to do the OVA (One Versus All, for\" [:br] \"        multi-class problems) computation.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Used to shuffle the training data, when ``shuffle`` is set to\" [:br] \"        ``True``. Pass an int for reproducible output across multiple\" [:br] \"        function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    warm_start : bool, optional\" [:br] \"        When set to True, reuse the solution of the previous call to fit as\" [:br] \"        initialization, otherwise, just erase the previous solution.\" [:br] \"        See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"        Repeatedly calling fit or partial_fit when warm_start is True can\" [:br] \"        result in a different solution than when calling fit a single time\" [:br] \"        because of the way the data is shuffled.\" [:br] \"\" [:br] \"    class_weight : dict, {class_label: weight} or \\\"balanced\\\" or None, optional\" [:br] \"        Preset for the class_weight fit parameter.\" [:br] \"\" [:br] \"        Weights associated with classes. If not given, all classes\" [:br] \"        are supposed to have weight one.\" [:br] \"\" [:br] \"        The \\\"balanced\\\" mode uses the values of y to automatically adjust\" [:br] \"        weights inversely proportional to class frequencies in the input data\" [:br] \"        as ``n_samples / (n_classes * np.bincount(y))``\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"           parameter *class_weight* to automatically weight samples.\" [:br] \"\" [:br] \"    average : bool or int, optional\" [:br] \"        When set to True, computes the averaged SGD weights and stores the\" [:br] \"        result in the ``coef_`` attribute. If set to an int greater than 1,\" [:br] \"        averaging will begin once the total number of samples seen reaches\" [:br] \"        average. So average=10 will begin averaging after seeing 10 samples.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"           parameter *average* to use weights averaging in SGD\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\" [:br] \"        Weights assigned to the features.\" [:br] \"\" [:br] \"    intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\" [:br] \"        Constants in decision function.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        The actual number of iterations to reach the stopping criterion.\" [:br] \"        For multiclass fits, it is the maximum over every binary fit.\" [:br] \"\" [:br] \"    classes_ : array of shape (n_classes,)\" [:br] \"        The unique classes labels.\" [:br] \"\" [:br] \"    t_ : int\" [:br] \"        Number of weight updates performed during training.\" [:br] \"        Same as ``(n_iter_ * n_samples)``.\" [:br] \"\" [:br] \"    loss_function_ : callable\" [:br] \"        Loss function used by the algorithm.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.linear_model import PassiveAggressiveClassifier\" [:br] \"    >>> from sklearn.datasets import make_classification\" [:br] \"\" [:br] \"    >>> X, y = make_classification(n_features=4, random_state=0)\" [:br] \"    >>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,\" [:br] \"    ... tol=1e-3)\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    PassiveAggressiveClassifier(random_state=0)\" [:br] \"    >>> print(clf.coef_)\" [:br] \"    [[0.26642044 0.45070924 0.67251877 0.64185414]]\" [:br] \"    >>> print(clf.intercept_)\" [:br] \"    [1.84127814]\" [:br] \"    >>> print(clf.predict([[0, 0, 0, 0]]))\" [:br] \"    [1]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"\" [:br] \"    SGDClassifier\" [:br] \"    Perceptron\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    Online Passive-Aggressive Algorithms\" [:br] \"    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\" [:br] \"    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\" [:br] \"\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/perceptron\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [15 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |     5.000 |\\n|                 :tol |  0.001000 |\\n|      :early-stopping |     false |\\n|               :eta-0 |     1.000 |\\n|             :shuffle |      true |\\n|             :penalty |           |\\n|            :max-iter |      1000 |\\n|              :n-jobs |           |\\n|        :random-state |         0 |\\n|       :fit-intercept |      true |\\n|               :alpha | 0.0001000 |\\n|          :warm-start |     false |\\n| :validation-fraction |    0.1000 |\\n|        :class-weight |           |\\n|             :verbose |         0 |\\n\"]]] [:span (\"Perceptron\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <perceptron>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"\" [:br] \"    penalty : {'l2','l1','elasticnet'}, default=None\" [:br] \"        The penalty (aka regularization term) to be used.\" [:br] \"\" [:br] \"    alpha : float, default=0.0001\" [:br] \"        Constant that multiplies the regularization term if regularization is\" [:br] \"        used.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether the intercept should be estimated or not. If False, the\" [:br] \"        data is assumed to be already centered.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        The maximum number of passes over the training data (aka epochs).\" [:br] \"        It only impacts the behavior in the ``fit`` method, and not the\" [:br] \"        :meth:`partial_fit` method.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    tol : float, default=1e-3\" [:br] \"        The stopping criterion. If it is not None, the iterations will stop\" [:br] \"        when (loss > previous_loss - tol).\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    shuffle : bool, default=True\" [:br] \"        Whether or not the training data should be shuffled after each epoch.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        The verbosity level\" [:br] \"\" [:br] \"    eta0 : double, default=1\" [:br] \"        Constant by which the updates are multiplied.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of CPUs to use to do the OVA (One Versus All, for\" [:br] \"        multi-class problems) computation.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Used to shuffle the training data, when ``shuffle`` is set to\" [:br] \"        ``True``. Pass an int for reproducible output across multiple\" [:br] \"        function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    early_stopping : bool, default=False\" [:br] \"        Whether to use early stopping to terminate training when validation.\" [:br] \"        score is not improving. If set to True, it will automatically set aside\" [:br] \"        a stratified fraction of training data as validation and terminate\" [:br] \"        training when validation score is not improving by at least tol for\" [:br] \"        n_iter_no_change consecutive epochs.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    validation_fraction : float, default=0.1\" [:br] \"        The proportion of training data to set aside as validation set for\" [:br] \"        early stopping. Must be between 0 and 1.\" [:br] \"        Only used if early_stopping is True.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    n_iter_no_change : int, default=5\" [:br] \"        Number of iterations with no improvement to wait before early stopping.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    class_weight : dict, {class_label: weight} or \\\"balanced\\\", default=None\" [:br] \"        Preset for the class_weight fit parameter.\" [:br] \"\" [:br] \"        Weights associated with classes. If not given, all classes\" [:br] \"        are supposed to have weight one.\" [:br] \"\" [:br] \"        The \\\"balanced\\\" mode uses the values of y to automatically adjust\" [:br] \"        weights inversely proportional to class frequencies in the input data\" [:br] \"        as ``n_samples / (n_classes * np.bincount(y))``\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to True, reuse the solution of the previous call to fit as\" [:br] \"        initialization, otherwise, just erase the previous solution. See\" [:br] \"        :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : ndarray of shape = [1, n_features] if n_classes == 2 else         [n_classes, n_features]\" [:br] \"        Weights assigned to the features.\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape = [1] if n_classes == 2 else [n_classes]\" [:br] \"        Constants in decision function.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        The actual number of iterations to reach the stopping criterion.\" [:br] \"        For multiclass fits, it is the maximum over every binary fit.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        The unique classes labels.\" [:br] \"\" [:br] \"    t_ : int\" [:br] \"        Number of weight updates performed during training.\" [:br] \"        Same as ``(n_iter_ * n_samples)``.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"\" [:br] \"    ``Perceptron`` is a classification algorithm which shares the same\" [:br] \"    underlying implementation with ``SGDClassifier``. In fact,\" [:br] \"    ``Perceptron()`` is equivalent to `SGDClassifier(loss=\\\"perceptron\\\",\" [:br] \"    eta0=1, learning_rate=\\\"constant\\\", penalty=None)`.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import load_digits\" [:br] \"    >>> from sklearn.linear_model import Perceptron\" [:br] \"    >>> X, y = load_digits(return_X_y=True)\" [:br] \"    >>> clf = Perceptron(tol=1e-3, random_state=0)\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    Perceptron()\" [:br] \"    >>> clf.score(X, y)\" [:br] \"    0.939...\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"\" [:br] \"    SGDClassifier\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"\" [:br] \"    https://en.wikipedia.org/wiki/Perceptron and references therein.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/quadratic-discriminant-analysis\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"276px\"}} [:p/markdown \"_unnamed [4 2]:\\n\\n|             :name |  :default |\\n|-------------------|-----------|\\n|           :priors |           |\\n|        :reg-param |     0.000 |\\n| :store-covariance |     false |\\n|              :tol | 0.0001000 |\\n\"]]] [:span (\"Quadratic Discriminant Analysis\" [:br] \"\" [:br] \"    A classifier with a quadratic decision boundary, generated\" [:br] \"    by fitting class conditional densities to the data\" [:br] \"    and using Bayes' rule.\" [:br] \"\" [:br] \"    The model fits a Gaussian density to each class.\" [:br] \"\" [:br] \"    .. versionadded:: 0.17\" [:br] \"       *QuadraticDiscriminantAnalysis*\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <lda_qda>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    priors : ndarray of shape (n_classes,), default=None\" [:br] \"        Class priors. By default, the class proportions are inferred from the\" [:br] \"        training data.\" [:br] \"\" [:br] \"    reg_param : float, default=0.0\" [:br] \"        Regularizes the per-class covariance estimates by transforming S2 as\" [:br] \"        ``S2 = (1 - reg_param) * S2 + reg_param * np.eye(n_features)``,\" [:br] \"        where S2 corresponds to the `scaling_` attribute of a given class.\" [:br] \"\" [:br] \"    store_covariance : bool, default=False\" [:br] \"        If True, the class covariance matrices are explicitely computed and\" [:br] \"        stored in the `self.covariance_` attribute.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"\" [:br] \"    tol : float, default=1.0e-4\" [:br] \"        Absolute threshold for a singular value to be considered significant,\" [:br] \"        used to estimate the rank of `Xk` where `Xk` is the centered matrix\" [:br] \"        of samples in class k. This parameter does not affect the\" [:br] \"        predictions. It only controls a warning that is raised when features\" [:br] \"        are considered to be colinear.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    covariance_ : list of len n_classes of ndarray             of shape (n_features, n_features)\" [:br] \"        For each class, gives the covariance matrix estimated using the\" [:br] \"        samples of that class. The estimations are unbiased. Only present if\" [:br] \"        `store_covariance` is True.\" [:br] \"\" [:br] \"    means_ : array-like of shape (n_classes, n_features)\" [:br] \"        Class-wise means.\" [:br] \"\" [:br] \"    priors_ : array-like of shape (n_classes,)\" [:br] \"        Class priors (sum to 1).\" [:br] \"\" [:br] \"    rotations_ : list of len n_classes of ndarray of shape (n_features, n_k)\" [:br] \"        For each class k an array of shape (n_features, n_k), where\" [:br] \"        ``n_k = min(n_features, number of elements in class k)``\" [:br] \"        It is the rotation of the Gaussian distribution, i.e. its\" [:br] \"        principal axis. It corresponds to `V`, the matrix of eigenvectors\" [:br] \"        coming from the SVD of `Xk = U S Vt` where `Xk` is the centered\" [:br] \"        matrix of samples from class k.\" [:br] \"\" [:br] \"    scalings_ : list of len n_classes of ndarray of shape (n_k,)\" [:br] \"        For each class, contains the scaling of\" [:br] \"        the Gaussian distributions along its principal axes, i.e. the\" [:br] \"        variance in the rotated coordinate system. It corresponds to `S^2 /\" [:br] \"        (n_samples - 1)`, where `S` is the diagonal matrix of singular values\" [:br] \"        from the SVD of `Xk`, where `Xk` is the centered matrix of samples\" [:br] \"        from class k.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        Unique class labels.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\" [:br] \"    >>> y = np.array([1, 1, 1, 2, 2, 2])\" [:br] \"    >>> clf = QuadraticDiscriminantAnalysis()\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    QuadraticDiscriminantAnalysis()\" [:br] \"    >>> print(clf.predict([[-0.8, -1]]))\" [:br] \"    [1]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    sklearn.discriminant_analysis.LinearDiscriminantAnalysis: Linear\" [:br] \"        Discriminant Analysis\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/radius-neighbors-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [9 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|       :weights |   uniform |\\n|             :p |         2 |\\n|     :leaf-size |        30 |\\n| :metric-params |           |\\n|        :radius |     1.000 |\\n| :outlier-label |           |\\n|     :algorithm |      auto |\\n|        :n-jobs |           |\\n|        :metric | minkowski |\\n\"]]] [:span (\"Classifier implementing a vote among neighbors within a given radius\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <classification>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    radius : float, default=1.0\" [:br] \"        Range of parameter space to use by default for :meth:`radius_neighbors`\" [:br] \"        queries.\" [:br] \"\" [:br] \"    weights : {'uniform', 'distance'} or callable, default='uniform'\" [:br] \"        weight function used in prediction.  Possible values:\" [:br] \"\" [:br] \"        - 'uniform' : uniform weights.  All points in each neighborhood\" [:br] \"          are weighted equally.\" [:br] \"        - 'distance' : weight points by the inverse of their distance.\" [:br] \"          in this case, closer neighbors of a query point will have a\" [:br] \"          greater influence than neighbors which are further away.\" [:br] \"        - [callable] : a user-defined function which accepts an\" [:br] \"          array of distances, and returns an array of the same shape\" [:br] \"          containing the weights.\" [:br] \"\" [:br] \"        Uniform weights are used by default.\" [:br] \"\" [:br] \"    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\" [:br] \"        Algorithm used to compute the nearest neighbors:\" [:br] \"\" [:br] \"        - 'ball_tree' will use :class:`BallTree`\" [:br] \"        - 'kd_tree' will use :class:`KDTree`\" [:br] \"        - 'brute' will use a brute-force search.\" [:br] \"        - 'auto' will attempt to decide the most appropriate algorithm\" [:br] \"          based on the values passed to :meth:`fit` method.\" [:br] \"\" [:br] \"        Note: fitting on sparse input will override the setting of\" [:br] \"        this parameter, using brute force.\" [:br] \"\" [:br] \"    leaf_size : int, default=30\" [:br] \"        Leaf size passed to BallTree or KDTree.  This can affect the\" [:br] \"        speed of the construction and query, as well as the memory\" [:br] \"        required to store the tree.  The optimal value depends on the\" [:br] \"        nature of the problem.\" [:br] \"\" [:br] \"    p : int, default=2\" [:br] \"        Power parameter for the Minkowski metric. When p = 1, this is\" [:br] \"        equivalent to using manhattan_distance (l1), and euclidean_distance\" [:br] \"        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\" [:br] \"\" [:br] \"    metric : str or callable, default='minkowski'\" [:br] \"        the distance metric to use for the tree.  The default metric is\" [:br] \"        minkowski, and with p=2 is equivalent to the standard Euclidean\" [:br] \"        metric. See the documentation of :class:`DistanceMetric` for a\" [:br] \"        list of available metrics.\" [:br] \"        If metric is \\\"precomputed\\\", X is assumed to be a distance matrix and\" [:br] \"        must be square during fit. X may be a :term:`sparse graph`,\" [:br] \"        in which case only \\\"nonzero\\\" elements may be considered neighbors.\" [:br] \"\" [:br] \"    outlier_label : {manual label, 'most_frequent'}, default=None\" [:br] \"        label for outlier samples (samples with no neighbors in given radius).\" [:br] \"\" [:br] \"        - manual label: str or int label (should be the same type as y)\" [:br] \"          or list of manual labels if multi-output is used.\" [:br] \"        - 'most_frequent' : assign the most frequent label of y to outliers.\" [:br] \"        - None : when any outlier is detected, ValueError will be raised.\" [:br] \"\" [:br] \"    metric_params : dict, default=None\" [:br] \"        Additional keyword arguments for the metric function.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of parallel jobs to run for neighbors search.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        Class labels known to the classifier.\" [:br] \"\" [:br] \"    effective_metric_ : str or callble\" [:br] \"        The distance metric used. It will be same as the `metric` parameter\" [:br] \"        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\" [:br] \"        'minkowski' and `p` parameter set to 2.\" [:br] \"\" [:br] \"    effective_metric_params_ : dict\" [:br] \"        Additional keyword arguments for the metric function. For most metrics\" [:br] \"        will be same with `metric_params` parameter, but may also contain the\" [:br] \"        `p` parameter value if the `effective_metric_` attribute is set to\" [:br] \"        'minkowski'.\" [:br] \"\" [:br] \"    outputs_2d_ : bool\" [:br] \"        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\" [:br] \"        otherwise True.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> X = [[0], [1], [2], [3]]\" [:br] \"    >>> y = [0, 0, 1, 1]\" [:br] \"    >>> from sklearn.neighbors import RadiusNeighborsClassifier\" [:br] \"    >>> neigh = RadiusNeighborsClassifier(radius=1.0)\" [:br] \"    >>> neigh.fit(X, y)\" [:br] \"    RadiusNeighborsClassifier(...)\" [:br] \"    >>> print(neigh.predict([[1.5]]))\" [:br] \"    [0]\" [:br] \"    >>> print(neigh.predict_proba([[1.0]]))\" [:br] \"    [[0.66666667 0.33333333]]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    KNeighborsClassifier\" [:br] \"    RadiusNeighborsRegressor\" [:br] \"    KNeighborsRegressor\" [:br] \"    NearestNeighbors\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\" [:br] \"    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\" [:br] \"\" [:br] \"    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/random-forest-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [19 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :bootstrap |     true |\\n|                :ccp-alpha |    0.000 |\\n|                   :n-jobs |          |\\n|             :random-state |          |\\n|                :oob-score |    false |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     auto |\\n|       :min-impurity-split |          |\\n|               :warm-start |    false |\\n|                :max-depth |          |\\n|             :class-weight |          |\\n|             :n-estimators |      100 |\\n|              :max-samples |          |\\n|                :criterion |     gini |\\n|                  :verbose |        0 |\\n\"]]] [:span (\"\" [:br] \"    A random forest classifier.\" [:br] \"\" [:br] \"    A random forest is a meta estimator that fits a number of decision tree\" [:br] \"    classifiers on various sub-samples of the dataset and uses averaging to\" [:br] \"    improve the predictive accuracy and control over-fitting.\" [:br] \"    The sub-sample size is controlled with the `max_samples` parameter if\" [:br] \"    `bootstrap=True` (default), otherwise the whole dataset is used to build\" [:br] \"    each tree.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <forest>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    n_estimators : int, default=100\" [:br] \"        The number of trees in the forest.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"           The default value of ``n_estimators`` changed from 10 to 100\" [:br] \"           in 0.22.\" [:br] \"\" [:br] \"    criterion : {\\\"gini\\\", \\\"entropy\\\"}, default=\\\"gini\\\"\" [:br] \"        The function to measure the quality of a split. Supported criteria are\" [:br] \"        \\\"gini\\\" for the Gini impurity and \\\"entropy\\\" for the information gain.\" [:br] \"        Note: this parameter is tree-specific.\" [:br] \"\" [:br] \"    max_depth : int, default=None\" [:br] \"        The maximum depth of the tree. If None, then nodes are expanded until\" [:br] \"        all leaves are pure or until all leaves contain less than\" [:br] \"        min_samples_split samples.\" [:br] \"\" [:br] \"    min_samples_split : int or float, default=2\" [:br] \"        The minimum number of samples required to split an internal node:\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_split` as the minimum number.\" [:br] \"        - If float, then `min_samples_split` is a fraction and\" [:br] \"          `ceil(min_samples_split * n_samples)` are the minimum\" [:br] \"          number of samples for each split.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_samples_leaf : int or float, default=1\" [:br] \"        The minimum number of samples required to be at a leaf node.\" [:br] \"        A split point at any depth will only be considered if it leaves at\" [:br] \"        least ``min_samples_leaf`` training samples in each of the left and\" [:br] \"        right branches.  This may have the effect of smoothing the model,\" [:br] \"        especially in regression.\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_leaf` as the minimum number.\" [:br] \"        - If float, then `min_samples_leaf` is a fraction and\" [:br] \"          `ceil(min_samples_leaf * n_samples)` are the minimum\" [:br] \"          number of samples for each node.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_weight_fraction_leaf : float, default=0.0\" [:br] \"        The minimum weighted fraction of the sum total of weights (of all\" [:br] \"        the input samples) required to be at a leaf node. Samples have\" [:br] \"        equal weight when sample_weight is not provided.\" [:br] \"\" [:br] \"    max_features : {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"}, int or float, default=\\\"auto\\\"\" [:br] \"        The number of features to consider when looking for the best split:\" [:br] \"\" [:br] \"        - If int, then consider `max_features` features at each split.\" [:br] \"        - If float, then `max_features` is a fraction and\" [:br] \"          `int(max_features * n_features)` features are considered at each\" [:br] \"          split.\" [:br] \"        - If \\\"auto\\\", then `max_features=sqrt(n_features)`.\" [:br] \"        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)` (same as \\\"auto\\\").\" [:br] \"        - If \\\"log2\\\", then `max_features=log2(n_features)`.\" [:br] \"        - If None, then `max_features=n_features`.\" [:br] \"\" [:br] \"        Note: the search for a split does not stop until at least one\" [:br] \"        valid partition of the node samples is found, even if it requires to\" [:br] \"        effectively inspect more than ``max_features`` features.\" [:br] \"\" [:br] \"    max_leaf_nodes : int, default=None\" [:br] \"        Grow trees with ``max_leaf_nodes`` in best-first fashion.\" [:br] \"        Best nodes are defined as relative reduction in impurity.\" [:br] \"        If None then unlimited number of leaf nodes.\" [:br] \"\" [:br] \"    min_impurity_decrease : float, default=0.0\" [:br] \"        A node will be split if this split induces a decrease of the impurity\" [:br] \"        greater than or equal to this value.\" [:br] \"\" [:br] \"        The weighted impurity decrease equation is the following::\" [:br] \"\" [:br] \"            N_t / N * (impurity - N_t_R / N_t * right_impurity\" [:br] \"                                - N_t_L / N_t * left_impurity)\" [:br] \"\" [:br] \"        where ``N`` is the total number of samples, ``N_t`` is the number of\" [:br] \"        samples at the current node, ``N_t_L`` is the number of samples in the\" [:br] \"        left child, and ``N_t_R`` is the number of samples in the right child.\" [:br] \"\" [:br] \"        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\" [:br] \"        if ``sample_weight`` is passed.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    min_impurity_split : float, default=None\" [:br] \"        Threshold for early stopping in tree growth. A node will split\" [:br] \"        if its impurity is above the threshold, otherwise it is a leaf.\" [:br] \"\" [:br] \"        .. deprecated:: 0.19\" [:br] \"           ``min_impurity_split`` has been deprecated in favor of\" [:br] \"           ``min_impurity_decrease`` in 0.19. The default value of\" [:br] \"           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\" [:br] \"           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\" [:br] \"\" [:br] \"\" [:br] \"    bootstrap : bool, default=True\" [:br] \"        Whether bootstrap samples are used when building trees. If False, the\" [:br] \"        whole dataset is used to build each tree.\" [:br] \"\" [:br] \"    oob_score : bool, default=False\" [:br] \"        Whether to use out-of-bag samples to estimate\" [:br] \"        the generalization accuracy.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\" [:br] \"        :meth:`decision_path` and :meth:`apply` are all parallelized over the\" [:br] \"        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\" [:br] \"        context. ``-1`` means using all processors. See :term:`Glossary\" [:br] \"        <n_jobs>` for more details.\" [:br] \"\" [:br] \"    random_state : int or RandomState, default=None\" [:br] \"        Controls both the randomness of the bootstrapping of the samples used\" [:br] \"        when building trees (if ``bootstrap=True``) and the sampling of the\" [:br] \"        features to consider when looking for the best split at each node\" [:br] \"        (if ``max_features < n_features``).\" [:br] \"        See :term:`Glossary <random_state>` for details.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        Controls the verbosity when fitting and predicting.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to ``True``, reuse the solution of the previous call to fit\" [:br] \"        and add more estimators to the ensemble, otherwise, just fit a whole\" [:br] \"        new forest. See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    class_weight : {\\\"balanced\\\", \\\"balanced_subsample\\\"}, dict or list of dicts,             default=None\" [:br] \"        Weights associated with classes in the form ``{class_label: weight}``.\" [:br] \"        If not given, all classes are supposed to have weight one. For\" [:br] \"        multi-output problems, a list of dicts can be provided in the same\" [:br] \"        order as the columns of y.\" [:br] \"\" [:br] \"        Note that for multioutput (including multilabel) weights should be\" [:br] \"        defined for each class of every column in its own dict. For example,\" [:br] \"        for four-class multilabel classification weights should be\" [:br] \"        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\" [:br] \"        [{1:1}, {2:5}, {3:1}, {4:1}].\" [:br] \"\" [:br] \"        The \\\"balanced\\\" mode uses the values of y to automatically adjust\" [:br] \"        weights inversely proportional to class frequencies in the input data\" [:br] \"        as ``n_samples / (n_classes * np.bincount(y))``\" [:br] \"\" [:br] \"        The \\\"balanced_subsample\\\" mode is the same as \\\"balanced\\\" except that\" [:br] \"        weights are computed based on the bootstrap sample for every tree\" [:br] \"        grown.\" [:br] \"\" [:br] \"        For multi-output, the weights of each column of y will be multiplied.\" [:br] \"\" [:br] \"        Note that these weights will be multiplied with sample_weight (passed\" [:br] \"        through the fit method) if sample_weight is specified.\" [:br] \"\" [:br] \"    ccp_alpha : non-negative float, default=0.0\" [:br] \"        Complexity parameter used for Minimal Cost-Complexity Pruning. The\" [:br] \"        subtree with the largest cost complexity that is smaller than\" [:br] \"        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\" [:br] \"        :ref:`minimal_cost_complexity_pruning` for details.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    max_samples : int or float, default=None\" [:br] \"        If bootstrap is True, the number of samples to draw from X\" [:br] \"        to train each base estimator.\" [:br] \"\" [:br] \"        - If None (default), then draw `X.shape[0]` samples.\" [:br] \"        - If int, then draw `max_samples` samples.\" [:br] \"        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\" [:br] \"          `max_samples` should be in the interval `(0, 1)`.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    base_estimator_ : DecisionTreeClassifier\" [:br] \"        The child estimator template used to create the collection of fitted\" [:br] \"        sub-estimators.\" [:br] \"\" [:br] \"    estimators_ : list of DecisionTreeClassifier\" [:br] \"        The collection of fitted sub-estimators.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,) or a list of such arrays\" [:br] \"        The classes labels (single output problem), or a list of arrays of\" [:br] \"        class labels (multi-output problem).\" [:br] \"\" [:br] \"    n_classes_ : int or list\" [:br] \"        The number of classes (single output problem), or a list containing the\" [:br] \"        number of classes for each output (multi-output problem).\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        The number of features when ``fit`` is performed.\" [:br] \"\" [:br] \"    n_outputs_ : int\" [:br] \"        The number of outputs when ``fit`` is performed.\" [:br] \"\" [:br] \"    feature_importances_ : ndarray of shape (n_features,)\" [:br] \"        The impurity-based feature importances.\" [:br] \"        The higher, the more important the feature.\" [:br] \"        The importance of a feature is computed as the (normalized)\" [:br] \"        total reduction of the criterion brought by that feature.  It is also\" [:br] \"        known as the Gini importance.\" [:br] \"\" [:br] \"        Warning: impurity-based feature importances can be misleading for\" [:br] \"        high cardinality features (many unique values). See\" [:br] \"        :func:`sklearn.inspection.permutation_importance` as an alternative.\" [:br] \"\" [:br] \"    oob_score_ : float\" [:br] \"        Score of the training dataset obtained using an out-of-bag estimate.\" [:br] \"        This attribute exists only when ``oob_score`` is True.\" [:br] \"\" [:br] \"    oob_decision_function_ : ndarray of shape (n_samples, n_classes)\" [:br] \"        Decision function computed with out-of-bag estimate on the training\" [:br] \"        set. If n_estimators is small it might be possible that a data point\" [:br] \"        was never left out during the bootstrap. In this case,\" [:br] \"        `oob_decision_function_` might contain NaN. This attribute exists\" [:br] \"        only when ``oob_score`` is True.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    DecisionTreeClassifier, ExtraTreesClassifier\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The default values for the parameters controlling the size of the trees\" [:br] \"    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\" [:br] \"    unpruned trees which can potentially be very large on some data sets. To\" [:br] \"    reduce memory consumption, the complexity and size of the trees should be\" [:br] \"    controlled by setting those parameter values.\" [:br] \"\" [:br] \"    The features are always randomly permuted at each split. Therefore,\" [:br] \"    the best found split may vary, even with the same training data,\" [:br] \"    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\" [:br] \"    of the criterion is identical for several splits enumerated during the\" [:br] \"    search of the best split. To obtain a deterministic behaviour during\" [:br] \"    fitting, ``random_state`` has to be fixed.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    .. [1] L. Breiman, \\\"Random Forests\\\", Machine Learning, 45(1), 5-32, 2001.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.ensemble import RandomForestClassifier\" [:br] \"    >>> from sklearn.datasets import make_classification\" [:br] \"    >>> X, y = make_classification(n_samples=1000, n_features=4,\" [:br] \"    ...                            n_informative=2, n_redundant=0,\" [:br] \"    ...                            random_state=0, shuffle=False)\" [:br] \"    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    RandomForestClassifier(...)\" [:br] \"    >>> print(clf.predict([[0, 0, 0, 0]]))\" [:br] \"    [1]\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/ridge-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [9 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|     :normalize |    false |\\n|           :tol | 0.001000 |\\n|        :solver |     auto |\\n|      :max-iter |          |\\n|  :random-state |          |\\n|        :copy-x |     true |\\n| :fit-intercept |     true |\\n|         :alpha |    1.000 |\\n|  :class-weight |          |\\n\"]]] [:span (\"Classifier using Ridge regression.\" [:br] \"\" [:br] \"    This classifier first converts the target values into ``{-1, 1}`` and\" [:br] \"    then treats the problem as a regression task (multi-output regression in\" [:br] \"    the multiclass case).\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <ridge_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : float, default=1.0\" [:br] \"        Regularization strength; must be a positive float. Regularization\" [:br] \"        improves the conditioning of the problem and reduces the variance of\" [:br] \"        the estimates. Larger values specify stronger regularization.\" [:br] \"        Alpha corresponds to ``1 / (2C)`` in other linear models such as\" [:br] \"        :class:`~sklearn.linear_model.LogisticRegression` or\" [:br] \"        :class:`sklearn.svm.LinearSVC`.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether to calculate the intercept for this model. If set to false, no\" [:br] \"        intercept will be used in calculations (e.g. data is expected to be\" [:br] \"        already centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If True, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    max_iter : int, default=None\" [:br] \"        Maximum number of iterations for conjugate gradient solver.\" [:br] \"        The default value is determined by scipy.sparse.linalg.\" [:br] \"\" [:br] \"    tol : float, default=1e-3\" [:br] \"        Precision of the solution.\" [:br] \"\" [:br] \"    class_weight : dict or 'balanced', default=None\" [:br] \"        Weights associated with classes in the form ``{class_label: weight}``.\" [:br] \"        If not given, all classes are supposed to have weight one.\" [:br] \"\" [:br] \"        The \\\"balanced\\\" mode uses the values of y to automatically adjust\" [:br] \"        weights inversely proportional to class frequencies in the input data\" [:br] \"        as ``n_samples / (n_classes * np.bincount(y))``.\" [:br] \"\" [:br] \"    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\" [:br] \"        Solver to use in the computational routines:\" [:br] \"\" [:br] \"        - 'auto' chooses the solver automatically based on the type of data.\" [:br] \"\" [:br] \"        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\" [:br] \"          coefficients. More stable for singular matrices than 'cholesky'.\" [:br] \"\" [:br] \"        - 'cholesky' uses the standard scipy.linalg.solve function to\" [:br] \"          obtain a closed-form solution.\" [:br] \"\" [:br] \"        - 'sparse_cg' uses the conjugate gradient solver as found in\" [:br] \"          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\" [:br] \"          more appropriate than 'cholesky' for large-scale data\" [:br] \"          (possibility to set `tol` and `max_iter`).\" [:br] \"\" [:br] \"        - 'lsqr' uses the dedicated regularized least-squares routine\" [:br] \"          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\" [:br] \"          procedure.\" [:br] \"\" [:br] \"        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\" [:br] \"          its unbiased and more flexible version named SAGA. Both methods\" [:br] \"          use an iterative procedure, and are often faster than other solvers\" [:br] \"          when both n_samples and n_features are large. Note that 'sag' and\" [:br] \"          'saga' fast convergence is only guaranteed on features with\" [:br] \"          approximately the same scale. You can preprocess the data with a\" [:br] \"          scaler from sklearn.preprocessing.\" [:br] \"\" [:br] \"          .. versionadded:: 0.17\" [:br] \"             Stochastic Average Gradient descent solver.\" [:br] \"          .. versionadded:: 0.19\" [:br] \"           SAGA solver.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\" [:br] \"        See :term:`Glossary <random_state>` for details.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\" [:br] \"        Coefficient of the features in the decision function.\" [:br] \"\" [:br] \"        ``coef_`` is of shape (1, n_features) when the given problem is binary.\" [:br] \"\" [:br] \"    intercept_ : float or ndarray of shape (n_targets,)\" [:br] \"        Independent term in decision function. Set to 0.0 if\" [:br] \"        ``fit_intercept = False``.\" [:br] \"\" [:br] \"    n_iter_ : None or ndarray of shape (n_targets,)\" [:br] \"        Actual number of iterations for each target. Available only for\" [:br] \"        sag and lsqr solvers. Other solvers will return None.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        The classes labels.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    Ridge : Ridge regression.\" [:br] \"    RidgeClassifierCV :  Ridge classifier with built-in cross validation.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    For multi-class classification, n_class classifiers are trained in\" [:br] \"    a one-versus-all approach. Concretely, this is implemented by taking\" [:br] \"    advantage of the multi-variate response support in Ridge.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import load_breast_cancer\" [:br] \"    >>> from sklearn.linear_model import RidgeClassifier\" [:br] \"    >>> X, y = load_breast_cancer(return_X_y=True)\" [:br] \"    >>> clf = RidgeClassifier().fit(X, y)\" [:br] \"    >>> clf.score(X, y)\" [:br] \"    0.9595...\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/ridge-classifier-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [7 2]:\\n\\n|            :name |                    :default |\\n|------------------|-----------------------------|\\n|          :alphas | #tech.v3.tensor<float64>[3] |\\n|                  | [0.1000 1.000 10.00]        |\\n|    :class-weight |                             |\\n|              :cv |                             |\\n|   :fit-intercept |                        true |\\n|       :normalize |                       false |\\n|         :scoring |                             |\\n| :store-cv-values |                       false |\\n\"]]] [:span (\"Ridge classifier with built-in cross-validation.\" [:br] \"\" [:br] \"    See glossary entry for :term:`cross-validation estimator`.\" [:br] \"\" [:br] \"    By default, it performs Generalized Cross-Validation, which is a form of\" [:br] \"    efficient Leave-One-Out cross-validation. Currently, only the n_features >\" [:br] \"    n_samples case is handled efficiently.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <ridge_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alphas : ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\" [:br] \"        Array of alpha values to try.\" [:br] \"        Regularization strength; must be a positive float. Regularization\" [:br] \"        improves the conditioning of the problem and reduces the variance of\" [:br] \"        the estimates. Larger values specify stronger regularization.\" [:br] \"        Alpha corresponds to ``1 / (2C)`` in other linear models such as\" [:br] \"        :class:`~sklearn.linear_model.LogisticRegression` or\" [:br] \"        :class:`sklearn.svm.LinearSVC`.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    scoring : string, callable, default=None\" [:br] \"        A string (see model evaluation documentation) or\" [:br] \"        a scorer callable object / function with signature\" [:br] \"        ``scorer(estimator, X, y)``.\" [:br] \"\" [:br] \"    cv : int, cross-validation generator or an iterable, default=None\" [:br] \"        Determines the cross-validation splitting strategy.\" [:br] \"        Possible inputs for cv are:\" [:br] \"\" [:br] \"        - None, to use the efficient Leave-One-Out cross-validation\" [:br] \"        - integer, to specify the number of folds.\" [:br] \"        - :term:`CV splitter`,\" [:br] \"        - An iterable yielding (train, test) splits as arrays of indices.\" [:br] \"\" [:br] \"        Refer :ref:`User Guide <cross_validation>` for the various\" [:br] \"        cross-validation strategies that can be used here.\" [:br] \"\" [:br] \"    class_weight : dict or 'balanced', default=None\" [:br] \"        Weights associated with classes in the form ``{class_label: weight}``.\" [:br] \"        If not given, all classes are supposed to have weight one.\" [:br] \"\" [:br] \"        The \\\"balanced\\\" mode uses the values of y to automatically adjust\" [:br] \"        weights inversely proportional to class frequencies in the input data\" [:br] \"        as ``n_samples / (n_classes * np.bincount(y))``\" [:br] \"\" [:br] \"    store_cv_values : bool, default=False\" [:br] \"        Flag indicating if the cross-validation values corresponding to\" [:br] \"        each alpha should be stored in the ``cv_values_`` attribute (see\" [:br] \"        below). This flag is only compatible with ``cv=None`` (i.e. using\" [:br] \"        Generalized Cross-Validation).\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    cv_values_ : ndarray of shape (n_samples, n_targets, n_alphas), optional\" [:br] \"        Cross-validation values for each alpha (if ``store_cv_values=True`` and\" [:br] \"        ``cv=None``). After ``fit()`` has been called, this attribute will\" [:br] \"        contain the mean squared errors (by default) or the values of the\" [:br] \"        ``{loss,score}_func`` function (if provided in the constructor). This\" [:br] \"        attribute exists only when ``store_cv_values`` is True.\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (1, n_features) or (n_targets, n_features)\" [:br] \"        Coefficient of the features in the decision function.\" [:br] \"\" [:br] \"        ``coef_`` is of shape (1, n_features) when the given problem is binary.\" [:br] \"\" [:br] \"    intercept_ : float or ndarray of shape (n_targets,)\" [:br] \"        Independent term in decision function. Set to 0.0 if\" [:br] \"        ``fit_intercept = False``.\" [:br] \"\" [:br] \"    alpha_ : float\" [:br] \"        Estimated regularization parameter.\" [:br] \"\" [:br] \"    best_score_ : float\" [:br] \"        Score of base estimator with best alpha.\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        The classes labels.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import load_breast_cancer\" [:br] \"    >>> from sklearn.linear_model import RidgeClassifierCV\" [:br] \"    >>> X, y = load_breast_cancer(return_X_y=True)\" [:br] \"    >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\" [:br] \"    >>> clf.score(X, y)\" [:br] \"    0.9630...\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    Ridge : Ridge regression\" [:br] \"    RidgeClassifier : Ridge classifier\" [:br] \"    RidgeCV : Ridge regression with built-in cross validation\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    For multi-class classification, n_class classifiers are trained in\" [:br] \"    a one-versus-all approach. Concretely, this is implemented by taking\" [:br] \"    advantage of the multi-variate response support in Ridge.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/sgd-classifier\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [21 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |         5 |\\n|       :learning-rate |   optimal |\\n|             :average |     false |\\n|                 :tol |  0.001000 |\\n|      :early-stopping |     false |\\n|               :eta-0 |     0.000 |\\n|             :shuffle |      true |\\n|             :penalty |        l2 |\\n|             :power-t |    0.5000 |\\n|            :max-iter |      1000 |\\n|              :n-jobs |           |\\n|        :random-state |           |\\n|       :fit-intercept |      true |\\n|               :alpha | 0.0001000 |\\n|          :warm-start |     false |\\n|           :l-1-ratio |    0.1500 |\\n| :validation-fraction |    0.1000 |\\n|        :class-weight |           |\\n|                :loss |     hinge |\\n|             :verbose |         0 |\\n|             :epsilon |    0.1000 |\\n\"]]] [:span (\"Linear classifiers (SVM, logistic regression, etc.) with SGD training.\" [:br] \"\" [:br] \"    This estimator implements regularized linear models with stochastic\" [:br] \"    gradient descent (SGD) learning: the gradient of the loss is estimated\" [:br] \"    each sample at a time and the model is updated along the way with a\" [:br] \"    decreasing strength schedule (aka learning rate). SGD allows minibatch\" [:br] \"    (online/out-of-core) learning via the `partial_fit` method.\" [:br] \"    For best results using the default learning rate schedule, the data should\" [:br] \"    have zero mean and unit variance.\" [:br] \"\" [:br] \"    This implementation works with data represented as dense or sparse arrays\" [:br] \"    of floating point values for the features. The model it fits can be\" [:br] \"    controlled with the loss parameter; by default, it fits a linear support\" [:br] \"    vector machine (SVM).\" [:br] \"\" [:br] \"    The regularizer is a penalty added to the loss function that shrinks model\" [:br] \"    parameters towards the zero vector using either the squared euclidean norm\" [:br] \"    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\" [:br] \"    parameter update crosses the 0.0 value because of the regularizer, the\" [:br] \"    update is truncated to 0.0 to allow for learning sparse models and achieve\" [:br] \"    online feature selection.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <sgd>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    loss : str, default='hinge'\" [:br] \"        The loss function to be used. Defaults to 'hinge', which gives a\" [:br] \"        linear SVM.\" [:br] \"\" [:br] \"        The possible options are 'hinge', 'log', 'modified_huber',\" [:br] \"        'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',\" [:br] \"        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\" [:br] \"\" [:br] \"        The 'log' loss gives logistic regression, a probabilistic classifier.\" [:br] \"        'modified_huber' is another smooth loss that brings tolerance to\" [:br] \"        outliers as well as probability estimates.\" [:br] \"        'squared_hinge' is like hinge but is quadratically penalized.\" [:br] \"        'perceptron' is the linear loss used by the perceptron algorithm.\" [:br] \"        The other losses are designed for regression but can be useful in\" [:br] \"        classification as well; see\" [:br] \"        :class:`~sklearn.linear_model.SGDRegressor` for a description.\" [:br] \"\" [:br] \"        More details about the losses formulas can be found in the\" [:br] \"        :ref:`User Guide <sgd_mathematical_formulation>`.\" [:br] \"\" [:br] \"    penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\" [:br] \"        The penalty (aka regularization term) to be used. Defaults to 'l2'\" [:br] \"        which is the standard regularizer for linear SVM models. 'l1' and\" [:br] \"        'elasticnet' might bring sparsity to the model (feature selection)\" [:br] \"        not achievable with 'l2'.\" [:br] \"\" [:br] \"    alpha : float, default=0.0001\" [:br] \"        Constant that multiplies the regularization term. The higher the\" [:br] \"        value, the stronger the regularization.\" [:br] \"        Also used to compute the learning rate when set to `learning_rate` is\" [:br] \"        set to 'optimal'.\" [:br] \"\" [:br] \"    l1_ratio : float, default=0.15\" [:br] \"        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\" [:br] \"        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\" [:br] \"        Only used if `penalty` is 'elasticnet'.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether the intercept should be estimated or not. If False, the\" [:br] \"        data is assumed to be already centered.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        The maximum number of passes over the training data (aka epochs).\" [:br] \"        It only impacts the behavior in the ``fit`` method, and not the\" [:br] \"        :meth:`partial_fit` method.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    tol : float, default=1e-3\" [:br] \"        The stopping criterion. If it is not None, training will stop\" [:br] \"        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\" [:br] \"        epochs.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    shuffle : bool, default=True\" [:br] \"        Whether or not the training data should be shuffled after each epoch.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        The verbosity level.\" [:br] \"\" [:br] \"    epsilon : float, default=0.1\" [:br] \"        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\" [:br] \"        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\" [:br] \"        For 'huber', determines the threshold at which it becomes less\" [:br] \"        important to get the prediction exactly right.\" [:br] \"        For epsilon-insensitive, any differences between the current prediction\" [:br] \"        and the correct label are ignored if they are less than this threshold.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of CPUs to use to do the OVA (One Versus All, for\" [:br] \"        multi-class problems) computation.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Used for shuffling the data, when ``shuffle`` is set to ``True``.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    learning_rate : str, default='optimal'\" [:br] \"        The learning rate schedule:\" [:br] \"\" [:br] \"        - 'constant': `eta = eta0`\" [:br] \"        - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\" [:br] \"          where t0 is chosen by a heuristic proposed by Leon Bottou.\" [:br] \"        - 'invscaling': `eta = eta0 / pow(t, power_t)`\" [:br] \"        - 'adaptive': eta = eta0, as long as the training keeps decreasing.\" [:br] \"          Each time n_iter_no_change consecutive epochs fail to decrease the\" [:br] \"          training loss by tol or fail to increase validation score by tol if\" [:br] \"          early_stopping is True, the current learning rate is divided by 5.\" [:br] \"\" [:br] \"            .. versionadded:: 0.20\" [:br] \"                Added 'adaptive' option\" [:br] \"\" [:br] \"    eta0 : double, default=0.0\" [:br] \"        The initial learning rate for the 'constant', 'invscaling' or\" [:br] \"        'adaptive' schedules. The default value is 0.0 as eta0 is not used by\" [:br] \"        the default schedule 'optimal'.\" [:br] \"\" [:br] \"    power_t : double, default=0.5\" [:br] \"        The exponent for inverse scaling learning rate [default 0.5].\" [:br] \"\" [:br] \"    early_stopping : bool, default=False\" [:br] \"        Whether to use early stopping to terminate training when validation\" [:br] \"        score is not improving. If set to True, it will automatically set aside\" [:br] \"        a stratified fraction of training data as validation and terminate\" [:br] \"        training when validation score returned by the `score` method is not\" [:br] \"        improving by at least tol for n_iter_no_change consecutive epochs.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"            Added 'early_stopping' option\" [:br] \"\" [:br] \"    validation_fraction : float, default=0.1\" [:br] \"        The proportion of training data to set aside as validation set for\" [:br] \"        early stopping. Must be between 0 and 1.\" [:br] \"        Only used if `early_stopping` is True.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"            Added 'validation_fraction' option\" [:br] \"\" [:br] \"    n_iter_no_change : int, default=5\" [:br] \"        Number of iterations with no improvement to wait before early stopping.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"            Added 'n_iter_no_change' option\" [:br] \"\" [:br] \"    class_weight : dict, {class_label: weight} or \\\"balanced\\\", default=None\" [:br] \"        Preset for the class_weight fit parameter.\" [:br] \"\" [:br] \"        Weights associated with classes. If not given, all classes\" [:br] \"        are supposed to have weight one.\" [:br] \"\" [:br] \"        The \\\"balanced\\\" mode uses the values of y to automatically adjust\" [:br] \"        weights inversely proportional to class frequencies in the input data\" [:br] \"        as ``n_samples / (n_classes * np.bincount(y))``.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to True, reuse the solution of the previous call to fit as\" [:br] \"        initialization, otherwise, just erase the previous solution.\" [:br] \"        See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"        Repeatedly calling fit or partial_fit when warm_start is True can\" [:br] \"        result in a different solution than when calling fit a single time\" [:br] \"        because of the way the data is shuffled.\" [:br] \"        If a dynamic learning rate is used, the learning rate is adapted\" [:br] \"        depending on the number of samples already seen. Calling ``fit`` resets\" [:br] \"        this counter, while ``partial_fit`` will result in increasing the\" [:br] \"        existing counter.\" [:br] \"\" [:br] \"    average : bool or int, default=False\" [:br] \"        When set to True, computes the averaged SGD weights accross all\" [:br] \"        updates and stores the result in the ``coef_`` attribute. If set to\" [:br] \"        an int greater than 1, averaging will begin once the total number of\" [:br] \"        samples seen reaches `average`. So ``average=10`` will begin\" [:br] \"        averaging after seeing 10 samples.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\" [:br] \"        Weights assigned to the features.\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\" [:br] \"        Constants in decision function.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        The actual number of iterations before reaching the stopping criterion.\" [:br] \"        For multiclass fits, it is the maximum over every binary fit.\" [:br] \"\" [:br] \"    loss_function_ : concrete ``LossFunction``\" [:br] \"\" [:br] \"    classes_ : array of shape (n_classes,)\" [:br] \"\" [:br] \"    t_ : int\" [:br] \"        Number of weight updates performed during training.\" [:br] \"        Same as ``(n_iter_ * n_samples)``.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    sklearn.svm.LinearSVC: Linear support vector classification.\" [:br] \"    LogisticRegression: Logistic regression.\" [:br] \"    Perceptron: Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\" [:br] \"        ``SGDClassifier(loss=\\\"perceptron\\\", eta0=1, learning_rate=\\\"constant\\\",\" [:br] \"        penalty=None)``.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> from sklearn.linear_model import SGDClassifier\" [:br] \"    >>> from sklearn.preprocessing import StandardScaler\" [:br] \"    >>> from sklearn.pipeline import make_pipeline\" [:br] \"    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\" [:br] \"    >>> Y = np.array([1, 1, 2, 2])\" [:br] \"    >>> # Always scale the input. The most convenient way is to use a pipeline.\" [:br] \"    >>> clf = make_pipeline(StandardScaler(),\" [:br] \"    ...                     SGDClassifier(max_iter=1000, tol=1e-3))\" [:br] \"    >>> clf.fit(X, Y)\" [:br] \"    Pipeline(steps=[('standardscaler', StandardScaler()),\" [:br] \"                    ('sgdclassifier', SGDClassifier())])\" [:br] \"    >>> print(clf.predict([[-0.8, -1]]))\" [:br] \"    [1]\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.classification/svc\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [15 2]:\\n\\n|                    :name | :default |\\n|--------------------------|----------|\\n|              :break-ties |    false |\\n|                  :kernel |      rbf |\\n|                   :gamma |    scale |\\n|                  :degree |        3 |\\n| :decision-function-shape |      ovr |\\n|             :probability |    false |\\n|                     :tol | 0.001000 |\\n|               :shrinking |     true |\\n|                       :c |    1.000 |\\n|                :max-iter |       -1 |\\n|            :random-state |          |\\n|                  :coef-0 |    0.000 |\\n|            :class-weight |          |\\n|              :cache-size |      200 |\\n|                 :verbose |    false |\\n\"]]] [:span (\"C-Support Vector Classification.\" [:br] \"\" [:br] \"    The implementation is based on libsvm. The fit time scales at least\" [:br] \"    quadratically with the number of samples and may be impractical\" [:br] \"    beyond tens of thousands of samples. For large datasets\" [:br] \"    consider using :class:`sklearn.svm.LinearSVC` or\" [:br] \"    :class:`sklearn.linear_model.SGDClassifier` instead, possibly after a\" [:br] \"    :class:`sklearn.kernel_approximation.Nystroem` transformer.\" [:br] \"\" [:br] \"    The multiclass support is handled according to a one-vs-one scheme.\" [:br] \"\" [:br] \"    For details on the precise mathematical formulation of the provided\" [:br] \"    kernel functions and how `gamma`, `coef0` and `degree` affect each\" [:br] \"    other, see the corresponding section in the narrative documentation:\" [:br] \"    :ref:`svm_kernels`.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <svm_classification>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    C : float, default=1.0\" [:br] \"        Regularization parameter. The strength of the regularization is\" [:br] \"        inversely proportional to C. Must be strictly positive. The penalty\" [:br] \"        is a squared l2 penalty.\" [:br] \"\" [:br] \"    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\" [:br] \"        Specifies the kernel type to be used in the algorithm.\" [:br] \"        It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\" [:br] \"        a callable.\" [:br] \"        If none is given, 'rbf' will be used. If a callable is given it is\" [:br] \"        used to pre-compute the kernel matrix from data matrices; that matrix\" [:br] \"        should be an array of shape ``(n_samples, n_samples)``.\" [:br] \"\" [:br] \"    degree : int, default=3\" [:br] \"        Degree of the polynomial kernel function ('poly').\" [:br] \"        Ignored by all other kernels.\" [:br] \"\" [:br] \"    gamma : {'scale', 'auto'} or float, default='scale'\" [:br] \"        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\" [:br] \"\" [:br] \"        - if ``gamma='scale'`` (default) is passed then it uses\" [:br] \"          1 / (n_features * X.var()) as value of gamma,\" [:br] \"        - if 'auto', uses 1 / n_features.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"           The default value of ``gamma`` changed from 'auto' to 'scale'.\" [:br] \"\" [:br] \"    coef0 : float, default=0.0\" [:br] \"        Independent term in kernel function.\" [:br] \"        It is only significant in 'poly' and 'sigmoid'.\" [:br] \"\" [:br] \"    shrinking : bool, default=True\" [:br] \"        Whether to use the shrinking heuristic.\" [:br] \"        See the :ref:`User Guide <shrinking_svm>`.\" [:br] \"\" [:br] \"    probability : bool, default=False\" [:br] \"        Whether to enable probability estimates. This must be enabled prior\" [:br] \"        to calling `fit`, will slow down that method as it internally uses\" [:br] \"        5-fold cross-validation, and `predict_proba` may be inconsistent with\" [:br] \"        `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\" [:br] \"\" [:br] \"    tol : float, default=1e-3\" [:br] \"        Tolerance for stopping criterion.\" [:br] \"\" [:br] \"    cache_size : float, default=200\" [:br] \"        Specify the size of the kernel cache (in MB).\" [:br] \"\" [:br] \"    class_weight : dict or 'balanced', default=None\" [:br] \"        Set the parameter C of class i to class_weight[i]*C for\" [:br] \"        SVC. If not given, all classes are supposed to have\" [:br] \"        weight one.\" [:br] \"        The \\\"balanced\\\" mode uses the values of y to automatically adjust\" [:br] \"        weights inversely proportional to class frequencies in the input data\" [:br] \"        as ``n_samples / (n_classes * np.bincount(y))``\" [:br] \"\" [:br] \"    verbose : bool, default=False\" [:br] \"        Enable verbose output. Note that this setting takes advantage of a\" [:br] \"        per-process runtime setting in libsvm that, if enabled, may not work\" [:br] \"        properly in a multithreaded context.\" [:br] \"\" [:br] \"    max_iter : int, default=-1\" [:br] \"        Hard limit on iterations within solver, or -1 for no limit.\" [:br] \"\" [:br] \"    decision_function_shape : {'ovo', 'ovr'}, default='ovr'\" [:br] \"        Whether to return a one-vs-rest ('ovr') decision function of shape\" [:br] \"        (n_samples, n_classes) as all other classifiers, or the original\" [:br] \"        one-vs-one ('ovo') decision function of libsvm which has shape\" [:br] \"        (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\" [:br] \"        ('ovo') is always used as multi-class strategy. The parameter is\" [:br] \"        ignored for binary classification.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.19\" [:br] \"            decision_function_shape is 'ovr' by default.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"           *decision_function_shape='ovr'* is recommended.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.17\" [:br] \"           Deprecated *decision_function_shape='ovo' and None*.\" [:br] \"\" [:br] \"    break_ties : bool, default=False\" [:br] \"        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\" [:br] \"        :term:`predict` will break ties according to the confidence values of\" [:br] \"        :term:`decision_function`; otherwise the first class among the tied\" [:br] \"        classes is returned. Please note that breaking ties comes at a\" [:br] \"        relatively high computational cost compared to a simple predict.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    random_state : int or RandomState instance, default=None\" [:br] \"        Controls the pseudo random number generation for shuffling the data for\" [:br] \"        probability estimates. Ignored when `probability` is False.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    support_ : ndarray of shape (n_SV,)\" [:br] \"        Indices of support vectors.\" [:br] \"\" [:br] \"    support_vectors_ : ndarray of shape (n_SV, n_features)\" [:br] \"        Support vectors.\" [:br] \"\" [:br] \"    n_support_ : ndarray of shape (n_class,), dtype=int32\" [:br] \"        Number of support vectors for each class.\" [:br] \"\" [:br] \"    dual_coef_ : ndarray of shape (n_class-1, n_SV)\" [:br] \"        Dual coefficients of the support vector in the decision\" [:br] \"        function (see :ref:`sgd_mathematical_formulation`), multiplied by\" [:br] \"        their targets.\" [:br] \"        For multiclass, coefficient for all 1-vs-1 classifiers.\" [:br] \"        The layout of the coefficients in the multiclass case is somewhat\" [:br] \"        non-trivial. See the :ref:`multi-class section of the User Guide\" [:br] \"        <svm_multi_class>` for details.\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (n_class * (n_class-1) / 2, n_features)\" [:br] \"        Weights assigned to the features (coefficients in the primal\" [:br] \"        problem). This is only available in the case of a linear kernel.\" [:br] \"\" [:br] \"        `coef_` is a readonly property derived from `dual_coef_` and\" [:br] \"        `support_vectors_`.\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape (n_class * (n_class-1) / 2,)\" [:br] \"        Constants in decision function.\" [:br] \"\" [:br] \"    fit_status_ : int\" [:br] \"        0 if correctly fitted, 1 otherwise (will raise warning)\" [:br] \"\" [:br] \"    classes_ : ndarray of shape (n_classes,)\" [:br] \"        The classes labels.\" [:br] \"\" [:br] \"    probA_ : ndarray of shape (n_class * (n_class-1) / 2)\" [:br] \"    probB_ : ndarray of shape (n_class * (n_class-1) / 2)\" [:br] \"        If `probability=True`, it corresponds to the parameters learned in\" [:br] \"        Platt scaling to produce probability estimates from decision values.\" [:br] \"        If `probability=False`, it's an empty array. Platt scaling uses the\" [:br] \"        logistic function\" [:br] \"        ``1 / (1 + exp(decision_value * probA_ + probB_))``\" [:br] \"        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\" [:br] \"        more information on the multiclass case and training procedure see\" [:br] \"        section 8 of [1]_.\" [:br] \"\" [:br] \"    class_weight_ : ndarray of shape (n_class,)\" [:br] \"        Multipliers of parameter C for each class.\" [:br] \"        Computed based on the ``class_weight`` parameter.\" [:br] \"\" [:br] \"    shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\" [:br] \"        Array dimensions of training vector ``X``.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> from sklearn.pipeline import make_pipeline\" [:br] \"    >>> from sklearn.preprocessing import StandardScaler\" [:br] \"    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\" [:br] \"    >>> y = np.array([1, 1, 2, 2])\" [:br] \"    >>> from sklearn.svm import SVC\" [:br] \"    >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    Pipeline(steps=[('standardscaler', StandardScaler()),\" [:br] \"                    ('svc', SVC(gamma='auto'))])\" [:br] \"\" [:br] \"    >>> print(clf.predict([[-0.8, -1]]))\" [:br] \"    [1]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    SVR\" [:br] \"        Support Vector Machine for Regression implemented using libsvm.\" [:br] \"\" [:br] \"    LinearSVC\" [:br] \"        Scalable Linear Support Vector Machine for classification\" [:br] \"        implemented using liblinear. Check the See also section of\" [:br] \"        LinearSVC for more comparison element.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    .. [1] `LIBSVM: A Library for Support Vector Machines\" [:br] \"        <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\" [:br] \"\" [:br] \"    .. [2] `Platt, John (1999). \\\"Probabilistic outputs for support vector\" [:br] \"        machines and comparison to regularizedlikelihood methods.\\\"\" [:br] \"        <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\" [:br] \"    \")] [:hr]])], \"97\" [:div [:p] nil nil ([:div [:h3 \":sklearn.regression/ada-boost-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"322px\"}} [:p/markdown \"_unnamed [5 2]:\\n\\n|           :name | :default |\\n|-----------------|----------|\\n| :base-estimator |          |\\n|  :learning-rate |    1.000 |\\n|           :loss |   linear |\\n|   :n-estimators |       50 |\\n|   :random-state |          |\\n\"]]] [:span (\"An AdaBoost regressor.\" [:br] \"\" [:br] \"    An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\" [:br] \"    regressor on the original dataset and then fits additional copies of the\" [:br] \"    regressor on the same dataset but where the weights of instances are\" [:br] \"    adjusted according to the error of the current prediction. As such,\" [:br] \"    subsequent regressors focus more on difficult cases.\" [:br] \"\" [:br] \"    This class implements the algorithm known as AdaBoost.R2 [2].\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <adaboost>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.14\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    base_estimator : object, default=None\" [:br] \"        The base estimator from which the boosted ensemble is built.\" [:br] \"        If ``None``, then the base estimator is\" [:br] \"        ``DecisionTreeRegressor(max_depth=3)``.\" [:br] \"\" [:br] \"    n_estimators : int, default=50\" [:br] \"        The maximum number of estimators at which boosting is terminated.\" [:br] \"        In case of perfect fit, the learning procedure is stopped early.\" [:br] \"\" [:br] \"    learning_rate : float, default=1.\" [:br] \"        Learning rate shrinks the contribution of each regressor by\" [:br] \"        ``learning_rate``. There is a trade-off between ``learning_rate`` and\" [:br] \"        ``n_estimators``.\" [:br] \"\" [:br] \"    loss : {'linear', 'square', 'exponential'}, default='linear'\" [:br] \"        The loss function to use when updating the weights after each\" [:br] \"        boosting iteration.\" [:br] \"\" [:br] \"    random_state : int or RandomState, default=None\" [:br] \"        Controls the random seed given at each `base_estimator` at each\" [:br] \"        boosting iteration.\" [:br] \"        Thus, it is only used when `base_estimator` exposes a `random_state`.\" [:br] \"        In addition, it controls the bootstrap of the weights used to train the\" [:br] \"        `base_estimator` at each boosting iteration.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    base_estimator_ : estimator\" [:br] \"        The base estimator from which the ensemble is grown.\" [:br] \"\" [:br] \"    estimators_ : list of classifiers\" [:br] \"        The collection of fitted sub-estimators.\" [:br] \"\" [:br] \"    estimator_weights_ : ndarray of floats\" [:br] \"        Weights for each estimator in the boosted ensemble.\" [:br] \"\" [:br] \"    estimator_errors_ : ndarray of floats\" [:br] \"        Regression error for each estimator in the boosted ensemble.\" [:br] \"\" [:br] \"    feature_importances_ : ndarray of shape (n_features,)\" [:br] \"        The impurity-based feature importances if supported by the\" [:br] \"        ``base_estimator`` (when based on decision trees).\" [:br] \"\" [:br] \"        Warning: impurity-based feature importances can be misleading for\" [:br] \"        high cardinality features (many unique values). See\" [:br] \"        :func:`sklearn.inspection.permutation_importance` as an alternative.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.ensemble import AdaBoostRegressor\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> X, y = make_regression(n_features=4, n_informative=2,\" [:br] \"    ...                        random_state=0, shuffle=False)\" [:br] \"    >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\" [:br] \"    >>> regr.fit(X, y)\" [:br] \"    AdaBoostRegressor(n_estimators=100, random_state=0)\" [:br] \"    >>> regr.predict([[0, 0, 0, 0]])\" [:br] \"    array([4.7972...])\" [:br] \"    >>> regr.score(X, y)\" [:br] \"    0.9771...\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    AdaBoostClassifier, GradientBoostingRegressor,\" [:br] \"    sklearn.tree.DecisionTreeRegressor\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    .. [1] Y. Freund, R. Schapire, \\\"A Decision-Theoretic Generalization of\" [:br] \"           on-Line Learning and an Application to Boosting\\\", 1995.\" [:br] \"\" [:br] \"    .. [2] H. Drucker, \\\"Improving Regressors using Boosting Techniques\\\", 1997.\" [:br] \"\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/ard-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [12 2]:\\n\\n|             :name |  :default |\\n|-------------------|-----------|\\n|        :normalize |     false |\\n|              :tol |  0.001000 |\\n|          :alpha-2 | 1.000E-06 |\\n| :threshold-lambda | 1.000E+04 |\\n|         :lambda-1 | 1.000E-06 |\\n|           :copy-x |      true |\\n|         :lambda-2 | 1.000E-06 |\\n|    :fit-intercept |      true |\\n|          :alpha-1 | 1.000E-06 |\\n|           :n-iter |       300 |\\n|          :verbose |     false |\\n|    :compute-score |     false |\\n\"]]] [:span (\"Bayesian ARD regression.\" [:br] \"\" [:br] \"    Fit the weights of a regression model, using an ARD prior. The weights of\" [:br] \"    the regression model are assumed to be in Gaussian distributions.\" [:br] \"    Also estimate the parameters lambda (precisions of the distributions of the\" [:br] \"    weights) and alpha (precision of the distribution of the noise).\" [:br] \"    The estimation is done by an iterative procedures (Evidence Maximization)\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <bayesian_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    n_iter : int, default=300\" [:br] \"        Maximum number of iterations.\" [:br] \"\" [:br] \"    tol : float, default=1e-3\" [:br] \"        Stop the algorithm if w has converged.\" [:br] \"\" [:br] \"    alpha_1 : float, default=1e-6\" [:br] \"        Hyper-parameter : shape parameter for the Gamma distribution prior\" [:br] \"        over the alpha parameter.\" [:br] \"\" [:br] \"    alpha_2 : float, default=1e-6\" [:br] \"        Hyper-parameter : inverse scale parameter (rate parameter) for the\" [:br] \"        Gamma distribution prior over the alpha parameter.\" [:br] \"\" [:br] \"    lambda_1 : float, default=1e-6\" [:br] \"        Hyper-parameter : shape parameter for the Gamma distribution prior\" [:br] \"        over the lambda parameter.\" [:br] \"\" [:br] \"    lambda_2 : float, default=1e-6\" [:br] \"        Hyper-parameter : inverse scale parameter (rate parameter) for the\" [:br] \"        Gamma distribution prior over the lambda parameter.\" [:br] \"\" [:br] \"    compute_score : bool, default=False\" [:br] \"        If True, compute the objective function at each step of the model.\" [:br] \"\" [:br] \"    threshold_lambda : float, default=10 000\" [:br] \"        threshold for removing (pruning) weights with high precision from\" [:br] \"        the computation.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If True, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    verbose : bool, default=False\" [:br] \"        Verbose mode when fitting the model.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array-like of shape (n_features,)\" [:br] \"        Coefficients of the regression model (mean of distribution)\" [:br] \"\" [:br] \"    alpha_ : float\" [:br] \"       estimated precision of the noise.\" [:br] \"\" [:br] \"    lambda_ : array-like of shape (n_features,)\" [:br] \"       estimated precisions of the weights.\" [:br] \"\" [:br] \"    sigma_ : array-like of shape (n_features, n_features)\" [:br] \"        estimated variance-covariance matrix of the weights\" [:br] \"\" [:br] \"    scores_ : float\" [:br] \"        if computed, value of the objective function (to be maximized)\" [:br] \"\" [:br] \"    intercept_ : float\" [:br] \"        Independent term in decision function. Set to 0.0 if\" [:br] \"        ``fit_intercept = False``.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn import linear_model\" [:br] \"    >>> clf = linear_model.ARDRegression()\" [:br] \"    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\" [:br] \"    ARDRegression()\" [:br] \"    >>> clf.predict([[1, 1]])\" [:br] \"    array([1.])\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    For an example, see :ref:`examples/linear_model/plot_ard.py\" [:br] \"    <sphx_glr_auto_examples_linear_model_plot_ard.py>`.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\" [:br] \"    competition, ASHRAE Transactions, 1994.\" [:br] \"\" [:br] \"    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\" [:br] \"    http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\" [:br] \"    Their beta is our ``self.alpha_``\" [:br] \"    Their alpha is our ``self.lambda_``\" [:br] \"    ARD is a little different than the slide: only dimensions/features for\" [:br] \"    which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\" [:br] \"    discarded.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/bagging-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [11 2]:\\n\\n|               :name | :default |\\n|---------------------|----------|\\n|          :bootstrap |     true |\\n| :bootstrap-features |    false |\\n|             :n-jobs |          |\\n|       :random-state |          |\\n|          :oob-score |    false |\\n|     :base-estimator |          |\\n|       :max-features |    1.000 |\\n|         :warm-start |    false |\\n|       :n-estimators |       10 |\\n|        :max-samples |    1.000 |\\n|            :verbose |        0 |\\n\"]]] [:span (\"A Bagging regressor.\" [:br] \"\" [:br] \"    A Bagging regressor is an ensemble meta-estimator that fits base\" [:br] \"    regressors each on random subsets of the original dataset and then\" [:br] \"    aggregate their individual predictions (either by voting or by averaging)\" [:br] \"    to form a final prediction. Such a meta-estimator can typically be used as\" [:br] \"    a way to reduce the variance of a black-box estimator (e.g., a decision\" [:br] \"    tree), by introducing randomization into its construction procedure and\" [:br] \"    then making an ensemble out of it.\" [:br] \"\" [:br] \"    This algorithm encompasses several works from the literature. When random\" [:br] \"    subsets of the dataset are drawn as random subsets of the samples, then\" [:br] \"    this algorithm is known as Pasting [1]_. If samples are drawn with\" [:br] \"    replacement, then the method is known as Bagging [2]_. When random subsets\" [:br] \"    of the dataset are drawn as random subsets of the features, then the method\" [:br] \"    is known as Random Subspaces [3]_. Finally, when base estimators are built\" [:br] \"    on subsets of both samples and features, then the method is known as\" [:br] \"    Random Patches [4]_.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <bagging>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.15\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    base_estimator : object, default=None\" [:br] \"        The base estimator to fit on random subsets of the dataset.\" [:br] \"        If None, then the base estimator is a decision tree.\" [:br] \"\" [:br] \"    n_estimators : int, default=10\" [:br] \"        The number of base estimators in the ensemble.\" [:br] \"\" [:br] \"    max_samples : int or float, default=1.0\" [:br] \"        The number of samples to draw from X to train each base estimator (with\" [:br] \"        replacement by default, see `bootstrap` for more details).\" [:br] \"\" [:br] \"        - If int, then draw `max_samples` samples.\" [:br] \"        - If float, then draw `max_samples * X.shape[0]` samples.\" [:br] \"\" [:br] \"    max_features : int or float, default=1.0\" [:br] \"        The number of features to draw from X to train each base estimator (\" [:br] \"        without replacement by default, see `bootstrap_features` for more\" [:br] \"        details).\" [:br] \"\" [:br] \"        - If int, then draw `max_features` features.\" [:br] \"        - If float, then draw `max_features * X.shape[1]` features.\" [:br] \"\" [:br] \"    bootstrap : bool, default=True\" [:br] \"        Whether samples are drawn with replacement. If False, sampling\" [:br] \"        without replacement is performed.\" [:br] \"\" [:br] \"    bootstrap_features : bool, default=False\" [:br] \"        Whether features are drawn with replacement.\" [:br] \"\" [:br] \"    oob_score : bool, default=False\" [:br] \"        Whether to use out-of-bag samples to estimate\" [:br] \"        the generalization error.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to True, reuse the solution of the previous call to fit\" [:br] \"        and add more estimators to the ensemble, otherwise, just fit\" [:br] \"        a whole new ensemble. See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of jobs to run in parallel for both :meth:`fit` and\" [:br] \"        :meth:`predict`. ``None`` means 1 unless in a\" [:br] \"        :obj:`joblib.parallel_backend` context. ``-1`` means using all\" [:br] \"        processors. See :term:`Glossary <n_jobs>` for more details.\" [:br] \"\" [:br] \"    random_state : int or RandomState, default=None\" [:br] \"        Controls the random resampling of the original dataset\" [:br] \"        (sample wise and feature wise).\" [:br] \"        If the base estimator accepts a `random_state` attribute, a different\" [:br] \"        seed is generated for each instance in the ensemble.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        Controls the verbosity when fitting and predicting.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    base_estimator_ : estimator\" [:br] \"        The base estimator from which the ensemble is grown.\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        The number of features when :meth:`fit` is performed.\" [:br] \"\" [:br] \"    estimators_ : list of estimators\" [:br] \"        The collection of fitted sub-estimators.\" [:br] \"\" [:br] \"    estimators_samples_ : list of arrays\" [:br] \"        The subset of drawn samples (i.e., the in-bag samples) for each base\" [:br] \"        estimator. Each subset is defined by an array of the indices selected.\" [:br] \"\" [:br] \"    estimators_features_ : list of arrays\" [:br] \"        The subset of drawn features for each base estimator.\" [:br] \"\" [:br] \"    oob_score_ : float\" [:br] \"        Score of the training dataset obtained using an out-of-bag estimate.\" [:br] \"        This attribute exists only when ``oob_score`` is True.\" [:br] \"\" [:br] \"    oob_prediction_ : ndarray of shape (n_samples,)\" [:br] \"        Prediction computed with out-of-bag estimate on the training\" [:br] \"        set. If n_estimators is small it might be possible that a data point\" [:br] \"        was never left out during the bootstrap. In this case,\" [:br] \"        `oob_prediction_` might contain NaN. This attribute exists only\" [:br] \"        when ``oob_score`` is True.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.svm import SVR\" [:br] \"    >>> from sklearn.ensemble import BaggingRegressor\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> X, y = make_regression(n_samples=100, n_features=4,\" [:br] \"    ...                        n_informative=2, n_targets=1,\" [:br] \"    ...                        random_state=0, shuffle=False)\" [:br] \"    >>> regr = BaggingRegressor(base_estimator=SVR(),\" [:br] \"    ...                         n_estimators=10, random_state=0).fit(X, y)\" [:br] \"    >>> regr.predict([[0, 0, 0, 0]])\" [:br] \"    array([-2.8720...])\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"\" [:br] \"    .. [1] L. Breiman, \\\"Pasting small votes for classification in large\" [:br] \"           databases and on-line\\\", Machine Learning, 36(1), 85-103, 1999.\" [:br] \"\" [:br] \"    .. [2] L. Breiman, \\\"Bagging predictors\\\", Machine Learning, 24(2), 123-140,\" [:br] \"           1996.\" [:br] \"\" [:br] \"    .. [3] T. Ho, \\\"The random subspace method for constructing decision\" [:br] \"           forests\\\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\" [:br] \"           1998.\" [:br] \"\" [:br] \"    .. [4] G. Louppe and P. Geurts, \\\"Ensembles on Random Patches\\\", Machine\" [:br] \"           Learning and Knowledge Discovery in Databases, 346-361, 2012.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/bayesian-ridge\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [13 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|           :tol |  0.001000 |\\n|       :alpha-2 | 1.000E-06 |\\n|      :lambda-1 | 1.000E-06 |\\n|        :copy-x |      true |\\n|      :lambda-2 | 1.000E-06 |\\n|    :alpha-init |           |\\n| :fit-intercept |      true |\\n|       :alpha-1 | 1.000E-06 |\\n|   :lambda-init |           |\\n|        :n-iter |       300 |\\n|       :verbose |     false |\\n| :compute-score |     false |\\n\"]]] [:span (\"Bayesian ridge regression.\" [:br] \"\" [:br] \"    Fit a Bayesian ridge model. See the Notes section for details on this\" [:br] \"    implementation and the optimization of the regularization parameters\" [:br] \"    lambda (precision of the weights) and alpha (precision of the noise).\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <bayesian_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    n_iter : int, default=300\" [:br] \"        Maximum number of iterations. Should be greater than or equal to 1.\" [:br] \"\" [:br] \"    tol : float, default=1e-3\" [:br] \"        Stop the algorithm if w has converged.\" [:br] \"\" [:br] \"    alpha_1 : float, default=1e-6\" [:br] \"        Hyper-parameter : shape parameter for the Gamma distribution prior\" [:br] \"        over the alpha parameter.\" [:br] \"\" [:br] \"    alpha_2 : float, default=1e-6\" [:br] \"        Hyper-parameter : inverse scale parameter (rate parameter) for the\" [:br] \"        Gamma distribution prior over the alpha parameter.\" [:br] \"\" [:br] \"    lambda_1 : float, default=1e-6\" [:br] \"        Hyper-parameter : shape parameter for the Gamma distribution prior\" [:br] \"        over the lambda parameter.\" [:br] \"\" [:br] \"    lambda_2 : float, default=1e-6\" [:br] \"        Hyper-parameter : inverse scale parameter (rate parameter) for the\" [:br] \"        Gamma distribution prior over the lambda parameter.\" [:br] \"\" [:br] \"    alpha_init : float, default=None\" [:br] \"        Initial value for alpha (precision of the noise).\" [:br] \"        If not set, alpha_init is 1/Var(y).\" [:br] \"\" [:br] \"            .. versionadded:: 0.22\" [:br] \"\" [:br] \"    lambda_init : float, default=None\" [:br] \"        Initial value for lambda (precision of the weights).\" [:br] \"        If not set, lambda_init is 1.\" [:br] \"\" [:br] \"            .. versionadded:: 0.22\" [:br] \"\" [:br] \"    compute_score : bool, default=False\" [:br] \"        If True, compute the log marginal likelihood at each iteration of the\" [:br] \"        optimization.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether to calculate the intercept for this model.\" [:br] \"        The intercept is not treated as a probabilistic parameter\" [:br] \"        and thus has no associated variance. If set\" [:br] \"        to False, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If True, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    verbose : bool, default=False\" [:br] \"        Verbose mode when fitting the model.\" [:br] \"\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array-like of shape (n_features,)\" [:br] \"        Coefficients of the regression model (mean of distribution)\" [:br] \"\" [:br] \"    intercept_ : float\" [:br] \"        Independent term in decision function. Set to 0.0 if\" [:br] \"        ``fit_intercept = False``.\" [:br] \"\" [:br] \"    alpha_ : float\" [:br] \"       Estimated precision of the noise.\" [:br] \"\" [:br] \"    lambda_ : float\" [:br] \"       Estimated precision of the weights.\" [:br] \"\" [:br] \"    sigma_ : array-like of shape (n_features, n_features)\" [:br] \"        Estimated variance-covariance matrix of the weights\" [:br] \"\" [:br] \"    scores_ : array-like of shape (n_iter_+1,)\" [:br] \"        If computed_score is True, value of the log marginal likelihood (to be\" [:br] \"        maximized) at each iteration of the optimization. The array starts\" [:br] \"        with the value of the log marginal likelihood obtained for the initial\" [:br] \"        values of alpha and lambda and ends with the value obtained for the\" [:br] \"        estimated alpha and lambda.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        The actual number of iterations to reach the stopping criterion.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn import linear_model\" [:br] \"    >>> clf = linear_model.BayesianRidge()\" [:br] \"    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\" [:br] \"    BayesianRidge()\" [:br] \"    >>> clf.predict([[1, 1]])\" [:br] \"    array([1.])\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    There exist several strategies to perform Bayesian ridge regression. This\" [:br] \"    implementation is based on the algorithm described in Appendix A of\" [:br] \"    (Tipping, 2001) where updates of the regularization parameters are done as\" [:br] \"    suggested in (MacKay, 1992). Note that according to A New\" [:br] \"    View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\" [:br] \"    update rules do not guarantee that the marginal likelihood is increasing\" [:br] \"    between two consecutive iterations of the optimization.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\" [:br] \"    Vol. 4, No. 3, 1992.\" [:br] \"\" [:br] \"    M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\" [:br] \"    Journal of Machine Learning Research, Vol. 1, 2001.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/cca\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"322px\"}} [:p/markdown \"_unnamed [5 2]:\\n\\n|         :name |  :default |\\n|---------------|-----------|\\n|         :copy |      true |\\n|     :max-iter |       500 |\\n| :n-components |         2 |\\n|        :scale |      true |\\n|          :tol | 1.000E-06 |\\n\"]]] [:span (\"CCA Canonical Correlation Analysis.\" [:br] \"\" [:br] \"    CCA inherits from PLS with mode=\\\"B\\\" and deflation_mode=\\\"canonical\\\".\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <cross_decomposition>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    n_components : int, (default 2).\" [:br] \"        number of components to keep.\" [:br] \"\" [:br] \"    scale : boolean, (default True)\" [:br] \"        whether to scale the data?\" [:br] \"\" [:br] \"    max_iter : an integer, (default 500)\" [:br] \"        the maximum number of iterations of the NIPALS inner loop\" [:br] \"\" [:br] \"    tol : non-negative real, default 1e-06.\" [:br] \"        the tolerance used in the iterative algorithm\" [:br] \"\" [:br] \"    copy : boolean\" [:br] \"        Whether the deflation be done on a copy. Let the default value\" [:br] \"        to True unless you don't care about side effects\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    x_weights_ : array, [p, n_components]\" [:br] \"        X block weights vectors.\" [:br] \"\" [:br] \"    y_weights_ : array, [q, n_components]\" [:br] \"        Y block weights vectors.\" [:br] \"\" [:br] \"    x_loadings_ : array, [p, n_components]\" [:br] \"        X block loadings vectors.\" [:br] \"\" [:br] \"    y_loadings_ : array, [q, n_components]\" [:br] \"        Y block loadings vectors.\" [:br] \"\" [:br] \"    x_scores_ : array, [n_samples, n_components]\" [:br] \"        X scores.\" [:br] \"\" [:br] \"    y_scores_ : array, [n_samples, n_components]\" [:br] \"        Y scores.\" [:br] \"\" [:br] \"    x_rotations_ : array, [p, n_components]\" [:br] \"        X block to latents rotations.\" [:br] \"\" [:br] \"    y_rotations_ : array, [q, n_components]\" [:br] \"        Y block to latents rotations.\" [:br] \"\" [:br] \"    coef_ : array of shape (p, q)\" [:br] \"        The coefficients of the linear model: ``Y = X coef_ + Err``\" [:br] \"\" [:br] \"    n_iter_ : array-like\" [:br] \"        Number of iterations of the NIPALS inner loop for each\" [:br] \"        component.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    For each component k, find the weights u, v that maximizes\" [:br] \"    max corr(Xk u, Yk v), such that ``|u| = |v| = 1``\" [:br] \"\" [:br] \"    Note that it maximizes only the correlations between the scores.\" [:br] \"\" [:br] \"    The residual matrix of X (Xk+1) block is obtained by the deflation on the\" [:br] \"    current X score: x_score.\" [:br] \"\" [:br] \"    The residual matrix of Y (Yk+1) block is obtained by deflation on the\" [:br] \"    current Y score.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.cross_decomposition import CCA\" [:br] \"    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]\" [:br] \"    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\" [:br] \"    >>> cca = CCA(n_components=1)\" [:br] \"    >>> cca.fit(X, Y)\" [:br] \"    CCA(n_components=1)\" [:br] \"    >>> X_c, Y_c = cca.transform(X, Y)\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"\" [:br] \"    Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with\" [:br] \"    emphasis on the two-block case. Technical Report 371, Department of\" [:br] \"    Statistics, University of Washington, Seattle, 2000.\" [:br] \"\" [:br] \"    In french but still a reference:\" [:br] \"    Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\" [:br] \"    Editions Technic.\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    PLSCanonical\" [:br] \"    PLSSVD\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/decision-tree-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [13 2]:\\n\\n|                     :name |   :default |\\n|---------------------------|------------|\\n| :min-weight-fraction-leaf |      0.000 |\\n|           :max-leaf-nodes |            |\\n|    :min-impurity-decrease |      0.000 |\\n|        :min-samples-split |      2.000 |\\n|                  :presort | deprecated |\\n|                :ccp-alpha |      0.000 |\\n|                 :splitter |       best |\\n|             :random-state |            |\\n|         :min-samples-leaf |          1 |\\n|             :max-features |            |\\n|       :min-impurity-split |            |\\n|                :max-depth |            |\\n|                :criterion |        mse |\\n\"]]] [:span (\"A decision tree regressor.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <tree>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    criterion : {\\\"mse\\\", \\\"friedman_mse\\\", \\\"mae\\\"}, default=\\\"mse\\\"\" [:br] \"        The function to measure the quality of a split. Supported criteria\" [:br] \"        are \\\"mse\\\" for the mean squared error, which is equal to variance\" [:br] \"        reduction as feature selection criterion and minimizes the L2 loss\" [:br] \"        using the mean of each terminal node, \\\"friedman_mse\\\", which uses mean\" [:br] \"        squared error with Friedman's improvement score for potential splits,\" [:br] \"        and \\\"mae\\\" for the mean absolute error, which minimizes the L1 loss\" [:br] \"        using the median of each terminal node.\" [:br] \"\" [:br] \"        .. versionadded:: 0.18\" [:br] \"           Mean Absolute Error (MAE) criterion.\" [:br] \"\" [:br] \"    splitter : {\\\"best\\\", \\\"random\\\"}, default=\\\"best\\\"\" [:br] \"        The strategy used to choose the split at each node. Supported\" [:br] \"        strategies are \\\"best\\\" to choose the best split and \\\"random\\\" to choose\" [:br] \"        the best random split.\" [:br] \"\" [:br] \"    max_depth : int, default=None\" [:br] \"        The maximum depth of the tree. If None, then nodes are expanded until\" [:br] \"        all leaves are pure or until all leaves contain less than\" [:br] \"        min_samples_split samples.\" [:br] \"\" [:br] \"    min_samples_split : int or float, default=2\" [:br] \"        The minimum number of samples required to split an internal node:\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_split` as the minimum number.\" [:br] \"        - If float, then `min_samples_split` is a fraction and\" [:br] \"          `ceil(min_samples_split * n_samples)` are the minimum\" [:br] \"          number of samples for each split.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_samples_leaf : int or float, default=1\" [:br] \"        The minimum number of samples required to be at a leaf node.\" [:br] \"        A split point at any depth will only be considered if it leaves at\" [:br] \"        least ``min_samples_leaf`` training samples in each of the left and\" [:br] \"        right branches.  This may have the effect of smoothing the model,\" [:br] \"        especially in regression.\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_leaf` as the minimum number.\" [:br] \"        - If float, then `min_samples_leaf` is a fraction and\" [:br] \"          `ceil(min_samples_leaf * n_samples)` are the minimum\" [:br] \"          number of samples for each node.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_weight_fraction_leaf : float, default=0.0\" [:br] \"        The minimum weighted fraction of the sum total of weights (of all\" [:br] \"        the input samples) required to be at a leaf node. Samples have\" [:br] \"        equal weight when sample_weight is not provided.\" [:br] \"\" [:br] \"    max_features : int, float or {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"}, default=None\" [:br] \"        The number of features to consider when looking for the best split:\" [:br] \"\" [:br] \"        - If int, then consider `max_features` features at each split.\" [:br] \"        - If float, then `max_features` is a fraction and\" [:br] \"          `int(max_features * n_features)` features are considered at each\" [:br] \"          split.\" [:br] \"        - If \\\"auto\\\", then `max_features=n_features`.\" [:br] \"        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\" [:br] \"        - If \\\"log2\\\", then `max_features=log2(n_features)`.\" [:br] \"        - If None, then `max_features=n_features`.\" [:br] \"\" [:br] \"        Note: the search for a split does not stop until at least one\" [:br] \"        valid partition of the node samples is found, even if it requires to\" [:br] \"        effectively inspect more than ``max_features`` features.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Controls the randomness of the estimator. The features are always\" [:br] \"        randomly permuted at each split, even if ``splitter`` is set to\" [:br] \"        ``\\\"best\\\"``. When ``max_features < n_features``, the algorithm will\" [:br] \"        select ``max_features`` at random at each split before finding the best\" [:br] \"        split among them. But the best found split may vary across different\" [:br] \"        runs, even if ``max_features=n_features``. That is the case, if the\" [:br] \"        improvement of the criterion is identical for several splits and one\" [:br] \"        split has to be selected at random. To obtain a deterministic behaviour\" [:br] \"        during fitting, ``random_state`` has to be fixed to an integer.\" [:br] \"        See :term:`Glossary <random_state>` for details.\" [:br] \"\" [:br] \"    max_leaf_nodes : int, default=None\" [:br] \"        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\" [:br] \"        Best nodes are defined as relative reduction in impurity.\" [:br] \"        If None then unlimited number of leaf nodes.\" [:br] \"\" [:br] \"    min_impurity_decrease : float, default=0.0\" [:br] \"        A node will be split if this split induces a decrease of the impurity\" [:br] \"        greater than or equal to this value.\" [:br] \"\" [:br] \"        The weighted impurity decrease equation is the following::\" [:br] \"\" [:br] \"            N_t / N * (impurity - N_t_R / N_t * right_impurity\" [:br] \"                                - N_t_L / N_t * left_impurity)\" [:br] \"\" [:br] \"        where ``N`` is the total number of samples, ``N_t`` is the number of\" [:br] \"        samples at the current node, ``N_t_L`` is the number of samples in the\" [:br] \"        left child, and ``N_t_R`` is the number of samples in the right child.\" [:br] \"\" [:br] \"        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\" [:br] \"        if ``sample_weight`` is passed.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    min_impurity_split : float, (default=0)\" [:br] \"        Threshold for early stopping in tree growth. A node will split\" [:br] \"        if its impurity is above the threshold, otherwise it is a leaf.\" [:br] \"\" [:br] \"        .. deprecated:: 0.19\" [:br] \"           ``min_impurity_split`` has been deprecated in favor of\" [:br] \"           ``min_impurity_decrease`` in 0.19. The default value of\" [:br] \"           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\" [:br] \"           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\" [:br] \"\" [:br] \"    presort : deprecated, default='deprecated'\" [:br] \"        This parameter is deprecated and will be removed in v0.24.\" [:br] \"\" [:br] \"        .. deprecated:: 0.22\" [:br] \"\" [:br] \"    ccp_alpha : non-negative float, default=0.0\" [:br] \"        Complexity parameter used for Minimal Cost-Complexity Pruning. The\" [:br] \"        subtree with the largest cost complexity that is smaller than\" [:br] \"        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\" [:br] \"        :ref:`minimal_cost_complexity_pruning` for details.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    feature_importances_ : ndarray of shape (n_features,)\" [:br] \"        The feature importances.\" [:br] \"        The higher, the more important the feature.\" [:br] \"        The importance of a feature is computed as the\" [:br] \"        (normalized) total reduction of the criterion brought\" [:br] \"        by that feature. It is also known as the Gini importance [4]_.\" [:br] \"\" [:br] \"        Warning: impurity-based feature importances can be misleading for\" [:br] \"        high cardinality features (many unique values). See\" [:br] \"        :func:`sklearn.inspection.permutation_importance` as an alternative.\" [:br] \"\" [:br] \"    max_features_ : int\" [:br] \"        The inferred value of max_features.\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        The number of features when ``fit`` is performed.\" [:br] \"\" [:br] \"    n_outputs_ : int\" [:br] \"        The number of outputs when ``fit`` is performed.\" [:br] \"\" [:br] \"    tree_ : Tree\" [:br] \"        The underlying Tree object. Please refer to\" [:br] \"        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\" [:br] \"        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\" [:br] \"        for basic usage of these attributes.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    DecisionTreeClassifier : A decision tree classifier.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The default values for the parameters controlling the size of the trees\" [:br] \"    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\" [:br] \"    unpruned trees which can potentially be very large on some data sets. To\" [:br] \"    reduce memory consumption, the complexity and size of the trees should be\" [:br] \"    controlled by setting those parameter values.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"\" [:br] \"    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\" [:br] \"\" [:br] \"    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \\\"Classification\" [:br] \"           and Regression Trees\\\", Wadsworth, Belmont, CA, 1984.\" [:br] \"\" [:br] \"    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \\\"Elements of Statistical\" [:br] \"           Learning\\\", Springer, 2009.\" [:br] \"\" [:br] \"    .. [4] L. Breiman, and A. Cutler, \\\"Random Forests\\\",\" [:br] \"           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import load_diabetes\" [:br] \"    >>> from sklearn.model_selection import cross_val_score\" [:br] \"    >>> from sklearn.tree import DecisionTreeRegressor\" [:br] \"    >>> X, y = load_diabetes(return_X_y=True)\" [:br] \"    >>> regressor = DecisionTreeRegressor(random_state=0)\" [:br] \"    >>> cross_val_score(regressor, X, y, cv=10)\" [:br] \"    ...                    # doctest: +SKIP\" [:br] \"    ...\" [:br] \"    array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,\" [:br] \"           0.16...,  0.11..., -0.73..., -0.30..., -0.00...])\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/dummy-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"230px\"}} [:p/markdown \"_unnamed [3 2]:\\n\\n|     :name | :default |\\n|-----------|----------|\\n| :constant |          |\\n| :quantile |          |\\n| :strategy |     mean |\\n\"]]] [:span (\"\" [:br] \"    DummyRegressor is a regressor that makes predictions using\" [:br] \"    simple rules.\" [:br] \"\" [:br] \"    This regressor is useful as a simple baseline to compare with other\" [:br] \"    (real) regressors. Do not use it for real problems.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <dummy_estimators>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.13\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    strategy : str\" [:br] \"        Strategy to use to generate predictions.\" [:br] \"\" [:br] \"        * \\\"mean\\\": always predicts the mean of the training set\" [:br] \"        * \\\"median\\\": always predicts the median of the training set\" [:br] \"        * \\\"quantile\\\": always predicts a specified quantile of the training set,\" [:br] \"          provided with the quantile parameter.\" [:br] \"        * \\\"constant\\\": always predicts a constant value that is provided by\" [:br] \"          the user.\" [:br] \"\" [:br] \"    constant : int or float or array-like of shape (n_outputs,)\" [:br] \"        The explicit constant as predicted by the \\\"constant\\\" strategy. This\" [:br] \"        parameter is useful only for the \\\"constant\\\" strategy.\" [:br] \"\" [:br] \"    quantile : float in [0.0, 1.0]\" [:br] \"        The quantile to predict using the \\\"quantile\\\" strategy. A quantile of\" [:br] \"        0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the\" [:br] \"        maximum.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    constant_ : array, shape (1, n_outputs)\" [:br] \"        Mean or median or quantile of the training targets or constant value\" [:br] \"        given by the user.\" [:br] \"\" [:br] \"    n_outputs_ : int,\" [:br] \"        Number of outputs.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> from sklearn.dummy import DummyRegressor\" [:br] \"    >>> X = np.array([1.0, 2.0, 3.0, 4.0])\" [:br] \"    >>> y = np.array([2.0, 3.0, 5.0, 10.0])\" [:br] \"    >>> dummy_regr = DummyRegressor(strategy=\\\"mean\\\")\" [:br] \"    >>> dummy_regr.fit(X, y)\" [:br] \"    DummyRegressor()\" [:br] \"    >>> dummy_regr.predict(X)\" [:br] \"    array([5., 5., 5., 5.])\" [:br] \"    >>> dummy_regr.score(X, y)\" [:br] \"    0.0\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/elastic-net\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [12 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|      :positive |     false |\\n|           :tol | 0.0001000 |\\n|      :max-iter |      1000 |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n|    :precompute |     false |\\n| :fit-intercept |      true |\\n|         :alpha |     1.000 |\\n|    :warm-start |     false |\\n|     :selection |    cyclic |\\n|     :l-1-ratio |    0.5000 |\\n\"]]] [:span (\"Linear regression with combined L1 and L2 priors as regularizer.\" [:br] \"\" [:br] \"    Minimizes the objective function::\" [:br] \"\" [:br] \"            1 / (2 * n_samples) * ||y - Xw||^2_2\" [:br] \"            + alpha * l1_ratio * ||w||_1\" [:br] \"            + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\" [:br] \"\" [:br] \"    If you are interested in controlling the L1 and L2 penalty\" [:br] \"    separately, keep in mind that this is equivalent to::\" [:br] \"\" [:br] \"            a * L1 + b * L2\" [:br] \"\" [:br] \"    where::\" [:br] \"\" [:br] \"            alpha = a + b and l1_ratio = a / (a + b)\" [:br] \"\" [:br] \"    The parameter l1_ratio corresponds to alpha in the glmnet R package while\" [:br] \"    alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\" [:br] \"    = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\" [:br] \"    unless you supply your own sequence of alpha.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <elastic_net>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : float, default=1.0\" [:br] \"        Constant that multiplies the penalty terms. Defaults to 1.0.\" [:br] \"        See the notes for the exact mathematical meaning of this\" [:br] \"        parameter. ``alpha = 0`` is equivalent to an ordinary least square,\" [:br] \"        solved by the :class:`LinearRegression` object. For numerical\" [:br] \"        reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\" [:br] \"        Given this, you should use the :class:`LinearRegression` object.\" [:br] \"\" [:br] \"    l1_ratio : float, default=0.5\" [:br] \"        The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\" [:br] \"        ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\" [:br] \"        is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\" [:br] \"        combination of L1 and L2.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether the intercept should be estimated or not. If ``False``, the\" [:br] \"        data is assumed to be already centered.\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    precompute : bool or array-like of shape (n_features, n_features),                 default=False\" [:br] \"        Whether to use a precomputed Gram matrix to speed up\" [:br] \"        calculations. The Gram matrix can also be passed as argument.\" [:br] \"        For sparse input this option is always ``True`` to preserve sparsity.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        The maximum number of iterations\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If ``True``, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        The tolerance for the optimization: if the updates are\" [:br] \"        smaller than ``tol``, the optimization code checks the\" [:br] \"        dual gap for optimality and continues until it is smaller\" [:br] \"        than ``tol``.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to ``True``, reuse the solution of the previous call to fit as\" [:br] \"        initialization, otherwise, just erase the previous solution.\" [:br] \"        See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    positive : bool, default=False\" [:br] \"        When set to ``True``, forces the coefficients to be positive.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        The seed of the pseudo random number generator that selects a random\" [:br] \"        feature to update. Used when ``selection`` == 'random'.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    selection : {'cyclic', 'random'}, default='cyclic'\" [:br] \"        If set to 'random', a random coefficient is updated every iteration\" [:br] \"        rather than looping over features sequentially by default. This\" [:br] \"        (setting to 'random') often leads to significantly faster convergence\" [:br] \"        especially when tol is higher than 1e-4.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\" [:br] \"        parameter vector (w in the cost function formula)\" [:br] \"\" [:br] \"    sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\" [:br] \"        ``sparse_coef_`` is a readonly property derived from ``coef_``\" [:br] \"\" [:br] \"    intercept_ : float or ndarray of shape (n_targets,)\" [:br] \"        independent term in decision function.\" [:br] \"\" [:br] \"    n_iter_ : list of int\" [:br] \"        number of iterations run by the coordinate descent solver to reach\" [:br] \"        the specified tolerance.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.linear_model import ElasticNet\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"\" [:br] \"    >>> X, y = make_regression(n_features=2, random_state=0)\" [:br] \"    >>> regr = ElasticNet(random_state=0)\" [:br] \"    >>> regr.fit(X, y)\" [:br] \"    ElasticNet(random_state=0)\" [:br] \"    >>> print(regr.coef_)\" [:br] \"    [18.83816048 64.55968825]\" [:br] \"    >>> print(regr.intercept_)\" [:br] \"    1.451...\" [:br] \"    >>> print(regr.predict([[0, 0]]))\" [:br] \"    [1.451...]\" [:br] \"\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    To avoid unnecessary memory duplication the X argument of the fit method\" [:br] \"    should be directly passed as a Fortran-contiguous numpy array.\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    ElasticNetCV : Elastic net model with best model selection by\" [:br] \"        cross-validation.\" [:br] \"    SGDRegressor: implements elastic net regression with incremental training.\" [:br] \"    SGDClassifier: implements logistic regression with elastic net penalty\" [:br] \"        (``SGDClassifier(loss=\\\"log\\\", penalty=\\\"elasticnet\\\")``).\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/elastic-net-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [16 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|      :positive |     false |\\n|           :tol | 0.0001000 |\\n|      :n-alphas |       100 |\\n|           :eps |  0.001000 |\\n|        :alphas |           |\\n|      :max-iter |      1000 |\\n|        :n-jobs |           |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n|    :precompute |      auto |\\n| :fit-intercept |      true |\\n|            :cv |           |\\n|     :selection |    cyclic |\\n|     :l-1-ratio |    0.5000 |\\n|       :verbose |         0 |\\n\"]]] [:span (\"Elastic Net model with iterative fitting along a regularization path.\" [:br] \"\" [:br] \"    See glossary entry for :term:`cross-validation estimator`.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <elastic_net>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    l1_ratio : float or list of float, default=0.5\" [:br] \"        float between 0 and 1 passed to ElasticNet (scaling between\" [:br] \"        l1 and l2 penalties). For ``l1_ratio = 0``\" [:br] \"        the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\" [:br] \"        For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\" [:br] \"        This parameter can be a list, in which case the different\" [:br] \"        values are tested by cross-validation and the one giving the best\" [:br] \"        prediction score is used. Note that a good choice of list of\" [:br] \"        values for l1_ratio is often to put more values close to 1\" [:br] \"        (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\" [:br] \"        .9, .95, .99, 1]``\" [:br] \"\" [:br] \"    eps : float, default=1e-3\" [:br] \"        Length of the path. ``eps=1e-3`` means that\" [:br] \"        ``alpha_min / alpha_max = 1e-3``.\" [:br] \"\" [:br] \"    n_alphas : int, default=100\" [:br] \"        Number of alphas along the regularization path, used for each l1_ratio.\" [:br] \"\" [:br] \"    alphas : ndarray, default=None\" [:br] \"        List of alphas where to compute the models.\" [:br] \"        If None alphas are set automatically\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\" [:br] \"        Whether to use a precomputed Gram matrix to speed up\" [:br] \"        calculations. If set to ``'auto'`` let us decide. The Gram\" [:br] \"        matrix can also be passed as argument.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        The maximum number of iterations\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        The tolerance for the optimization: if the updates are\" [:br] \"        smaller than ``tol``, the optimization code checks the\" [:br] \"        dual gap for optimality and continues until it is smaller\" [:br] \"        than ``tol``.\" [:br] \"\" [:br] \"    cv : int, cross-validation generator or iterable, default=None\" [:br] \"        Determines the cross-validation splitting strategy.\" [:br] \"        Possible inputs for cv are:\" [:br] \"\" [:br] \"        - None, to use the default 5-fold cross-validation,\" [:br] \"        - int, to specify the number of folds.\" [:br] \"        - :term:`CV splitter`,\" [:br] \"        - An iterable yielding (train, test) splits as arrays of indices.\" [:br] \"\" [:br] \"        For int/None inputs, :class:`KFold` is used.\" [:br] \"\" [:br] \"        Refer :ref:`User Guide <cross_validation>` for the various\" [:br] \"        cross-validation strategies that can be used here.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"            ``cv`` default value if None changed from 3-fold to 5-fold.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If ``True``, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    verbose : bool or int, default=0\" [:br] \"        Amount of verbosity.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        Number of CPUs to use during the cross validation.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    positive : bool, default=False\" [:br] \"        When set to ``True``, forces the coefficients to be positive.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        The seed of the pseudo random number generator that selects a random\" [:br] \"        feature to update. Used when ``selection`` == 'random'.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    selection : {'cyclic', 'random'}, default='cyclic'\" [:br] \"        If set to 'random', a random coefficient is updated every iteration\" [:br] \"        rather than looping over features sequentially by default. This\" [:br] \"        (setting to 'random') often leads to significantly faster convergence\" [:br] \"        especially when tol is higher than 1e-4.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    alpha_ : float\" [:br] \"        The amount of penalization chosen by cross validation\" [:br] \"\" [:br] \"    l1_ratio_ : float\" [:br] \"        The compromise between l1 and l2 penalization chosen by\" [:br] \"        cross validation\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\" [:br] \"        Parameter vector (w in the cost function formula),\" [:br] \"\" [:br] \"    intercept_ : float or ndarray of shape (n_targets, n_features)\" [:br] \"        Independent term in the decision function.\" [:br] \"\" [:br] \"    mse_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds)\" [:br] \"        Mean square error for the test set on each fold, varying l1_ratio and\" [:br] \"        alpha.\" [:br] \"\" [:br] \"    alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\" [:br] \"        The grid of alphas used for fitting, for each l1_ratio.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        number of iterations run by the coordinate descent solver to reach\" [:br] \"        the specified tolerance for the optimal alpha.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.linear_model import ElasticNetCV\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"\" [:br] \"    >>> X, y = make_regression(n_features=2, random_state=0)\" [:br] \"    >>> regr = ElasticNetCV(cv=5, random_state=0)\" [:br] \"    >>> regr.fit(X, y)\" [:br] \"    ElasticNetCV(cv=5, random_state=0)\" [:br] \"    >>> print(regr.alpha_)\" [:br] \"    0.199...\" [:br] \"    >>> print(regr.intercept_)\" [:br] \"    0.398...\" [:br] \"    >>> print(regr.predict([[0, 0]]))\" [:br] \"    [0.398...]\" [:br] \"\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    For an example, see\" [:br] \"    :ref:`examples/linear_model/plot_lasso_model_selection.py\" [:br] \"    <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\" [:br] \"\" [:br] \"    To avoid unnecessary memory duplication the X argument of the fit method\" [:br] \"    should be directly passed as a Fortran-contiguous numpy array.\" [:br] \"\" [:br] \"    The parameter l1_ratio corresponds to alpha in the glmnet R package\" [:br] \"    while alpha corresponds to the lambda parameter in glmnet.\" [:br] \"    More specifically, the optimization objective is::\" [:br] \"\" [:br] \"        1 / (2 * n_samples) * ||y - Xw||^2_2\" [:br] \"        + alpha * l1_ratio * ||w||_1\" [:br] \"        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\" [:br] \"\" [:br] \"    If you are interested in controlling the L1 and L2 penalty\" [:br] \"    separately, keep in mind that this is equivalent to::\" [:br] \"\" [:br] \"        a * L1 + b * L2\" [:br] \"\" [:br] \"    for::\" [:br] \"\" [:br] \"        alpha = a + b and l1_ratio = a / (a + b).\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    enet_path\" [:br] \"    ElasticNet\" [:br] \"\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/extra-tree-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [12 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :ccp-alpha |    0.000 |\\n|                 :splitter |   random |\\n|             :random-state |          |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     auto |\\n|       :min-impurity-split |          |\\n|                :max-depth |          |\\n|                :criterion |      mse |\\n\"]]] [:span (\"An extremely randomized tree regressor.\" [:br] \"\" [:br] \"    Extra-trees differ from classic decision trees in the way they are built.\" [:br] \"    When looking for the best split to separate the samples of a node into two\" [:br] \"    groups, random splits are drawn for each of the `max_features` randomly\" [:br] \"    selected features and the best split among those is chosen. When\" [:br] \"    `max_features` is set 1, this amounts to building a totally random\" [:br] \"    decision tree.\" [:br] \"\" [:br] \"    Warning: Extra-trees should only be used within ensemble methods.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <tree>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    criterion : {\\\"mse\\\", \\\"friedman_mse\\\", \\\"mae\\\"}, default=\\\"mse\\\"\" [:br] \"        The function to measure the quality of a split. Supported criteria\" [:br] \"        are \\\"mse\\\" for the mean squared error, which is equal to variance\" [:br] \"        reduction as feature selection criterion, and \\\"mae\\\" for the mean\" [:br] \"        absolute error.\" [:br] \"\" [:br] \"        .. versionadded:: 0.18\" [:br] \"           Mean Absolute Error (MAE) criterion.\" [:br] \"\" [:br] \"    splitter : {\\\"random\\\", \\\"best\\\"}, default=\\\"random\\\"\" [:br] \"        The strategy used to choose the split at each node. Supported\" [:br] \"        strategies are \\\"best\\\" to choose the best split and \\\"random\\\" to choose\" [:br] \"        the best random split.\" [:br] \"\" [:br] \"    max_depth : int, default=None\" [:br] \"        The maximum depth of the tree. If None, then nodes are expanded until\" [:br] \"        all leaves are pure or until all leaves contain less than\" [:br] \"        min_samples_split samples.\" [:br] \"\" [:br] \"    min_samples_split : int or float, default=2\" [:br] \"        The minimum number of samples required to split an internal node:\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_split` as the minimum number.\" [:br] \"        - If float, then `min_samples_split` is a fraction and\" [:br] \"          `ceil(min_samples_split * n_samples)` are the minimum\" [:br] \"          number of samples for each split.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_samples_leaf : int or float, default=1\" [:br] \"        The minimum number of samples required to be at a leaf node.\" [:br] \"        A split point at any depth will only be considered if it leaves at\" [:br] \"        least ``min_samples_leaf`` training samples in each of the left and\" [:br] \"        right branches.  This may have the effect of smoothing the model,\" [:br] \"        especially in regression.\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_leaf` as the minimum number.\" [:br] \"        - If float, then `min_samples_leaf` is a fraction and\" [:br] \"          `ceil(min_samples_leaf * n_samples)` are the minimum\" [:br] \"          number of samples for each node.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_weight_fraction_leaf : float, default=0.0\" [:br] \"        The minimum weighted fraction of the sum total of weights (of all\" [:br] \"        the input samples) required to be at a leaf node. Samples have\" [:br] \"        equal weight when sample_weight is not provided.\" [:br] \"\" [:br] \"    max_features : int, float, {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"} or None, default=\\\"auto\\\"\" [:br] \"        The number of features to consider when looking for the best split:\" [:br] \"\" [:br] \"        - If int, then consider `max_features` features at each split.\" [:br] \"        - If float, then `max_features` is a fraction and\" [:br] \"          `int(max_features * n_features)` features are considered at each\" [:br] \"          split.\" [:br] \"        - If \\\"auto\\\", then `max_features=n_features`.\" [:br] \"        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\" [:br] \"        - If \\\"log2\\\", then `max_features=log2(n_features)`.\" [:br] \"        - If None, then `max_features=n_features`.\" [:br] \"\" [:br] \"        Note: the search for a split does not stop until at least one\" [:br] \"        valid partition of the node samples is found, even if it requires to\" [:br] \"        effectively inspect more than ``max_features`` features.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Used to pick randomly the `max_features` used at each split.\" [:br] \"        See :term:`Glossary <random_state>` for details.\" [:br] \"\" [:br] \"    min_impurity_decrease : float, default=0.0\" [:br] \"        A node will be split if this split induces a decrease of the impurity\" [:br] \"        greater than or equal to this value.\" [:br] \"\" [:br] \"        The weighted impurity decrease equation is the following::\" [:br] \"\" [:br] \"            N_t / N * (impurity - N_t_R / N_t * right_impurity\" [:br] \"                                - N_t_L / N_t * left_impurity)\" [:br] \"\" [:br] \"        where ``N`` is the total number of samples, ``N_t`` is the number of\" [:br] \"        samples at the current node, ``N_t_L`` is the number of samples in the\" [:br] \"        left child, and ``N_t_R`` is the number of samples in the right child.\" [:br] \"\" [:br] \"        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\" [:br] \"        if ``sample_weight`` is passed.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    min_impurity_split : float, (default=0)\" [:br] \"        Threshold for early stopping in tree growth. A node will split\" [:br] \"        if its impurity is above the threshold, otherwise it is a leaf.\" [:br] \"\" [:br] \"        .. deprecated:: 0.19\" [:br] \"           ``min_impurity_split`` has been deprecated in favor of\" [:br] \"           ``min_impurity_decrease`` in 0.19. The default value of\" [:br] \"           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\" [:br] \"           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\" [:br] \"\" [:br] \"    max_leaf_nodes : int, default=None\" [:br] \"        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\" [:br] \"        Best nodes are defined as relative reduction in impurity.\" [:br] \"        If None then unlimited number of leaf nodes.\" [:br] \"\" [:br] \"    ccp_alpha : non-negative float, default=0.0\" [:br] \"        Complexity parameter used for Minimal Cost-Complexity Pruning. The\" [:br] \"        subtree with the largest cost complexity that is smaller than\" [:br] \"        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\" [:br] \"        :ref:`minimal_cost_complexity_pruning` for details.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    max_features_ : int\" [:br] \"        The inferred value of max_features.\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        The number of features when ``fit`` is performed.\" [:br] \"\" [:br] \"    feature_importances_ : ndarray of shape (n_features,)\" [:br] \"        Return impurity-based feature importances (the higher, the more\" [:br] \"        important the feature).\" [:br] \"\" [:br] \"        Warning: impurity-based feature importances can be misleading for\" [:br] \"        high cardinality features (many unique values). See\" [:br] \"        :func:`sklearn.inspection.permutation_importance` as an alternative.\" [:br] \"\" [:br] \"    n_outputs_ : int\" [:br] \"        The number of outputs when ``fit`` is performed.\" [:br] \"\" [:br] \"    tree_ : Tree\" [:br] \"        The underlying Tree object. Please refer to\" [:br] \"        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\" [:br] \"        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\" [:br] \"        for basic usage of these attributes.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    ExtraTreeClassifier : An extremely randomized tree classifier.\" [:br] \"    sklearn.ensemble.ExtraTreesClassifier : An extra-trees classifier.\" [:br] \"    sklearn.ensemble.ExtraTreesRegressor : An extra-trees regressor.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The default values for the parameters controlling the size of the trees\" [:br] \"    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\" [:br] \"    unpruned trees which can potentially be very large on some data sets. To\" [:br] \"    reduce memory consumption, the complexity and size of the trees should be\" [:br] \"    controlled by setting those parameter values.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"\" [:br] \"    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized trees\\\",\" [:br] \"           Machine Learning, 63(1), 3-42, 2006.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import load_diabetes\" [:br] \"    >>> from sklearn.model_selection import train_test_split\" [:br] \"    >>> from sklearn.ensemble import BaggingRegressor\" [:br] \"    >>> from sklearn.tree import ExtraTreeRegressor\" [:br] \"    >>> X, y = load_diabetes(return_X_y=True)\" [:br] \"    >>> X_train, X_test, y_train, y_test = train_test_split(\" [:br] \"    ...     X, y, random_state=0)\" [:br] \"    >>> extra_tree = ExtraTreeRegressor(random_state=0)\" [:br] \"    >>> reg = BaggingRegressor(extra_tree, random_state=0).fit(\" [:br] \"    ...     X_train, y_train)\" [:br] \"    >>> reg.score(X_test, y_test)\" [:br] \"    0.33...\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/extra-trees-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [18 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :bootstrap |    false |\\n|                :ccp-alpha |    0.000 |\\n|                   :n-jobs |          |\\n|             :random-state |          |\\n|                :oob-score |    false |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     auto |\\n|       :min-impurity-split |          |\\n|               :warm-start |    false |\\n|                :max-depth |          |\\n|             :n-estimators |      100 |\\n|              :max-samples |          |\\n|                :criterion |      mse |\\n|                  :verbose |        0 |\\n\"]]] [:span (\"\" [:br] \"    An extra-trees regressor.\" [:br] \"\" [:br] \"    This class implements a meta estimator that fits a number of\" [:br] \"    randomized decision trees (a.k.a. extra-trees) on various sub-samples\" [:br] \"    of the dataset and uses averaging to improve the predictive accuracy\" [:br] \"    and control over-fitting.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <forest>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    n_estimators : int, default=100\" [:br] \"        The number of trees in the forest.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"           The default value of ``n_estimators`` changed from 10 to 100\" [:br] \"           in 0.22.\" [:br] \"\" [:br] \"    criterion : {\\\"mse\\\", \\\"mae\\\"}, default=\\\"mse\\\"\" [:br] \"        The function to measure the quality of a split. Supported criteria\" [:br] \"        are \\\"mse\\\" for the mean squared error, which is equal to variance\" [:br] \"        reduction as feature selection criterion, and \\\"mae\\\" for the mean\" [:br] \"        absolute error.\" [:br] \"\" [:br] \"        .. versionadded:: 0.18\" [:br] \"           Mean Absolute Error (MAE) criterion.\" [:br] \"\" [:br] \"    max_depth : int, default=None\" [:br] \"        The maximum depth of the tree. If None, then nodes are expanded until\" [:br] \"        all leaves are pure or until all leaves contain less than\" [:br] \"        min_samples_split samples.\" [:br] \"\" [:br] \"    min_samples_split : int or float, default=2\" [:br] \"        The minimum number of samples required to split an internal node:\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_split` as the minimum number.\" [:br] \"        - If float, then `min_samples_split` is a fraction and\" [:br] \"          `ceil(min_samples_split * n_samples)` are the minimum\" [:br] \"          number of samples for each split.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_samples_leaf : int or float, default=1\" [:br] \"        The minimum number of samples required to be at a leaf node.\" [:br] \"        A split point at any depth will only be considered if it leaves at\" [:br] \"        least ``min_samples_leaf`` training samples in each of the left and\" [:br] \"        right branches.  This may have the effect of smoothing the model,\" [:br] \"        especially in regression.\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_leaf` as the minimum number.\" [:br] \"        - If float, then `min_samples_leaf` is a fraction and\" [:br] \"          `ceil(min_samples_leaf * n_samples)` are the minimum\" [:br] \"          number of samples for each node.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_weight_fraction_leaf : float, default=0.0\" [:br] \"        The minimum weighted fraction of the sum total of weights (of all\" [:br] \"        the input samples) required to be at a leaf node. Samples have\" [:br] \"        equal weight when sample_weight is not provided.\" [:br] \"\" [:br] \"    max_features : {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"} int or float, default=\\\"auto\\\"\" [:br] \"        The number of features to consider when looking for the best split:\" [:br] \"\" [:br] \"        - If int, then consider `max_features` features at each split.\" [:br] \"        - If float, then `max_features` is a fraction and\" [:br] \"          `int(max_features * n_features)` features are considered at each\" [:br] \"          split.\" [:br] \"        - If \\\"auto\\\", then `max_features=n_features`.\" [:br] \"        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\" [:br] \"        - If \\\"log2\\\", then `max_features=log2(n_features)`.\" [:br] \"        - If None, then `max_features=n_features`.\" [:br] \"\" [:br] \"        Note: the search for a split does not stop until at least one\" [:br] \"        valid partition of the node samples is found, even if it requires to\" [:br] \"        effectively inspect more than ``max_features`` features.\" [:br] \"\" [:br] \"    max_leaf_nodes : int, default=None\" [:br] \"        Grow trees with ``max_leaf_nodes`` in best-first fashion.\" [:br] \"        Best nodes are defined as relative reduction in impurity.\" [:br] \"        If None then unlimited number of leaf nodes.\" [:br] \"\" [:br] \"    min_impurity_decrease : float, default=0.0\" [:br] \"        A node will be split if this split induces a decrease of the impurity\" [:br] \"        greater than or equal to this value.\" [:br] \"\" [:br] \"        The weighted impurity decrease equation is the following::\" [:br] \"\" [:br] \"            N_t / N * (impurity - N_t_R / N_t * right_impurity\" [:br] \"                                - N_t_L / N_t * left_impurity)\" [:br] \"\" [:br] \"        where ``N`` is the total number of samples, ``N_t`` is the number of\" [:br] \"        samples at the current node, ``N_t_L`` is the number of samples in the\" [:br] \"        left child, and ``N_t_R`` is the number of samples in the right child.\" [:br] \"\" [:br] \"        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\" [:br] \"        if ``sample_weight`` is passed.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    min_impurity_split : float, default=None\" [:br] \"        Threshold for early stopping in tree growth. A node will split\" [:br] \"        if its impurity is above the threshold, otherwise it is a leaf.\" [:br] \"\" [:br] \"        .. deprecated:: 0.19\" [:br] \"           ``min_impurity_split`` has been deprecated in favor of\" [:br] \"           ``min_impurity_decrease`` in 0.19. The default value of\" [:br] \"           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\" [:br] \"           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\" [:br] \"\" [:br] \"    bootstrap : bool, default=False\" [:br] \"        Whether bootstrap samples are used when building trees. If False, the\" [:br] \"        whole dataset is used to build each tree.\" [:br] \"\" [:br] \"    oob_score : bool, default=False\" [:br] \"        Whether to use out-of-bag samples to estimate the R^2 on unseen data.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\" [:br] \"        :meth:`decision_path` and :meth:`apply` are all parallelized over the\" [:br] \"        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\" [:br] \"        context. ``-1`` means using all processors. See :term:`Glossary\" [:br] \"        <n_jobs>` for more details.\" [:br] \"\" [:br] \"    random_state : int or RandomState, default=None\" [:br] \"        Controls 3 sources of randomness:\" [:br] \"\" [:br] \"        - the bootstrapping of the samples used when building trees\" [:br] \"          (if ``bootstrap=True``)\" [:br] \"        - the sampling of the features to consider when looking for the best\" [:br] \"          split at each node (if ``max_features < n_features``)\" [:br] \"        - the draw of the splits for each of the `max_features`\" [:br] \"\" [:br] \"        See :term:`Glossary <random_state>` for details.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        Controls the verbosity when fitting and predicting.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to ``True``, reuse the solution of the previous call to fit\" [:br] \"        and add more estimators to the ensemble, otherwise, just fit a whole\" [:br] \"        new forest. See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    ccp_alpha : non-negative float, default=0.0\" [:br] \"        Complexity parameter used for Minimal Cost-Complexity Pruning. The\" [:br] \"        subtree with the largest cost complexity that is smaller than\" [:br] \"        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\" [:br] \"        :ref:`minimal_cost_complexity_pruning` for details.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    max_samples : int or float, default=None\" [:br] \"        If bootstrap is True, the number of samples to draw from X\" [:br] \"        to train each base estimator.\" [:br] \"\" [:br] \"        - If None (default), then draw `X.shape[0]` samples.\" [:br] \"        - If int, then draw `max_samples` samples.\" [:br] \"        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\" [:br] \"          `max_samples` should be in the interval `(0, 1)`.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    base_estimator_ : ExtraTreeRegressor\" [:br] \"        The child estimator template used to create the collection of fitted\" [:br] \"        sub-estimators.\" [:br] \"\" [:br] \"    estimators_ : list of DecisionTreeRegressor\" [:br] \"        The collection of fitted sub-estimators.\" [:br] \"\" [:br] \"    feature_importances_ : ndarray of shape (n_features,)\" [:br] \"        The impurity-based feature importances.\" [:br] \"        The higher, the more important the feature.\" [:br] \"        The importance of a feature is computed as the (normalized)\" [:br] \"        total reduction of the criterion brought by that feature.  It is also\" [:br] \"        known as the Gini importance.\" [:br] \"\" [:br] \"        Warning: impurity-based feature importances can be misleading for\" [:br] \"        high cardinality features (many unique values). See\" [:br] \"        :func:`sklearn.inspection.permutation_importance` as an alternative.\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        The number of features.\" [:br] \"\" [:br] \"    n_outputs_ : int\" [:br] \"        The number of outputs.\" [:br] \"\" [:br] \"    oob_score_ : float\" [:br] \"        Score of the training dataset obtained using an out-of-bag estimate.\" [:br] \"        This attribute exists only when ``oob_score`` is True.\" [:br] \"\" [:br] \"    oob_prediction_ : ndarray of shape (n_samples,)\" [:br] \"        Prediction computed with out-of-bag estimate on the training set.\" [:br] \"        This attribute exists only when ``oob_score`` is True.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.\" [:br] \"    RandomForestRegressor: Ensemble regressor using trees with optimal splits.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The default values for the parameters controlling the size of the trees\" [:br] \"    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\" [:br] \"    unpruned trees which can potentially be very large on some data sets. To\" [:br] \"    reduce memory consumption, the complexity and size of the trees should be\" [:br] \"    controlled by setting those parameter values.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized trees\\\",\" [:br] \"           Machine Learning, 63(1), 3-42, 2006.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import load_diabetes\" [:br] \"    >>> from sklearn.model_selection import train_test_split\" [:br] \"    >>> from sklearn.ensemble import ExtraTreesRegressor\" [:br] \"    >>> X, y = load_diabetes(return_X_y=True)\" [:br] \"    >>> X_train, X_test, y_train, y_test = train_test_split(\" [:br] \"    ...     X, y, random_state=0)\" [:br] \"    >>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\" [:br] \"    ...    X_train, y_train)\" [:br] \"    >>> reg.score(X_test, y_test)\" [:br] \"    0.2708...\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/gamma-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"368px\"}} [:p/markdown \"_unnamed [6 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|         :alpha |     1.000 |\\n| :fit-intercept |      true |\\n|      :max-iter |       100 |\\n|           :tol | 0.0001000 |\\n|       :verbose |         0 |\\n|    :warm-start |     false |\\n\"]]] [:span (\"Generalized Linear Model with a Gamma distribution.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <Generalized_linear_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : float, default=1\" [:br] \"        Constant that multiplies the penalty term and thus determines the\" [:br] \"        regularization strength. ``alpha = 0`` is equivalent to unpenalized\" [:br] \"        GLMs. In this case, the design matrix `X` must have full column rank\" [:br] \"        (no collinearities).\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Specifies if a constant (a.k.a. bias or intercept) should be\" [:br] \"        added to the linear predictor (X @ coef + intercept).\" [:br] \"\" [:br] \"    max_iter : int, default=100\" [:br] \"        The maximal number of iterations for the solver.\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        Stopping criterion. For the lbfgs solver,\" [:br] \"        the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\" [:br] \"        where ``g_j`` is the j-th component of the gradient (derivative) of\" [:br] \"        the objective function.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        If set to ``True``, reuse the solution of the previous call to ``fit``\" [:br] \"        as initialization for ``coef_`` and ``intercept_`` .\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        For the lbfgs solver set verbose to any positive number for verbosity.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array of shape (n_features,)\" [:br] \"        Estimated coefficients for the linear predictor (`X * coef_ +\" [:br] \"        intercept_`) in the GLM.\" [:br] \"\" [:br] \"    intercept_ : float\" [:br] \"        Intercept (a.k.a. bias) added to linear predictor.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        Actual number of iterations used in the solver.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/gaussian-process-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [7 2]:\\n\\n|                 :name |      :default |\\n|-----------------------|---------------|\\n|                :alpha |     1.000E-10 |\\n|         :copy-x-train |          true |\\n|               :kernel |               |\\n| :n-restarts-optimizer |             0 |\\n|          :normalize-y |         false |\\n|            :optimizer | fmin_l_bfgs_b |\\n|         :random-state |               |\\n\"]]] [:span (\"Gaussian process regression (GPR).\" [:br] \"\" [:br] \"    The implementation is based on Algorithm 2.1 of Gaussian Processes\" [:br] \"    for Machine Learning (GPML) by Rasmussen and Williams.\" [:br] \"\" [:br] \"    In addition to standard scikit-learn estimator API,\" [:br] \"    GaussianProcessRegressor:\" [:br] \"\" [:br] \"       * allows prediction without prior fitting (based on the GP prior)\" [:br] \"       * provides an additional method sample_y(X), which evaluates samples\" [:br] \"         drawn from the GPR (prior or posterior) at given inputs\" [:br] \"       * exposes a method log_marginal_likelihood(theta), which can be used\" [:br] \"         externally for other ways of selecting hyperparameters, e.g., via\" [:br] \"         Markov chain Monte Carlo.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <gaussian_process>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.18\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    kernel : kernel instance, default=None\" [:br] \"        The kernel specifying the covariance function of the GP. If None is\" [:br] \"        passed, the kernel \\\"1.0 * RBF(1.0)\\\" is used as default. Note that\" [:br] \"        the kernel's hyperparameters are optimized during fitting.\" [:br] \"\" [:br] \"    alpha : float or array-like of shape (n_samples), default=1e-10\" [:br] \"        Value added to the diagonal of the kernel matrix during fitting.\" [:br] \"        Larger values correspond to increased noise level in the observations.\" [:br] \"        This can also prevent a potential numerical issue during fitting, by\" [:br] \"        ensuring that the calculated values form a positive definite matrix.\" [:br] \"        If an array is passed, it must have the same number of entries as the\" [:br] \"        data used for fitting and is used as datapoint-dependent noise level.\" [:br] \"        Note that this is equivalent to adding a WhiteKernel with c=alpha.\" [:br] \"        Allowing to specify the noise level directly as a parameter is mainly\" [:br] \"        for convenience and for consistency with Ridge.\" [:br] \"\" [:br] \"    optimizer : \\\"fmin_l_bfgs_b\\\" or callable, default=\\\"fmin_l_bfgs_b\\\"\" [:br] \"        Can either be one of the internally supported optimizers for optimizing\" [:br] \"        the kernel's parameters, specified by a string, or an externally\" [:br] \"        defined optimizer passed as a callable. If a callable is passed, it\" [:br] \"        must have the signature::\" [:br] \"\" [:br] \"            def optimizer(obj_func, initial_theta, bounds):\" [:br] \"                # * 'obj_func' is the objective function to be minimized, which\" [:br] \"                #   takes the hyperparameters theta as parameter and an\" [:br] \"                #   optional flag eval_gradient, which determines if the\" [:br] \"                #   gradient is returned additionally to the function value\" [:br] \"                # * 'initial_theta': the initial value for theta, which can be\" [:br] \"                #   used by local optimizers\" [:br] \"                # * 'bounds': the bounds on the values of theta\" [:br] \"                ....\" [:br] \"                # Returned are the best found hyperparameters theta and\" [:br] \"                # the corresponding value of the target function.\" [:br] \"                return theta_opt, func_min\" [:br] \"\" [:br] \"        Per default, the 'L-BGFS-B' algorithm from scipy.optimize.minimize\" [:br] \"        is used. If None is passed, the kernel's parameters are kept fixed.\" [:br] \"        Available internal optimizers are::\" [:br] \"\" [:br] \"            'fmin_l_bfgs_b'\" [:br] \"\" [:br] \"    n_restarts_optimizer : int, default=0\" [:br] \"        The number of restarts of the optimizer for finding the kernel's\" [:br] \"        parameters which maximize the log-marginal likelihood. The first run\" [:br] \"        of the optimizer is performed from the kernel's initial parameters,\" [:br] \"        the remaining ones (if any) from thetas sampled log-uniform randomly\" [:br] \"        from the space of allowed theta-values. If greater than 0, all bounds\" [:br] \"        must be finite. Note that n_restarts_optimizer == 0 implies that one\" [:br] \"        run is performed.\" [:br] \"\" [:br] \"    normalize_y : boolean, optional (default: False)\" [:br] \"        Whether the target values y are normalized, the mean and variance of\" [:br] \"        the target values are set equal to 0 and 1 respectively. This is\" [:br] \"        recommended for cases where zero-mean, unit-variance priors are used.\" [:br] \"        Note that, in this implementation, the normalisation is reversed\" [:br] \"        before the GP predictions are reported.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.23\" [:br] \"\" [:br] \"    copy_X_train : bool, default=True\" [:br] \"        If True, a persistent copy of the training data is stored in the\" [:br] \"        object. Otherwise, just a reference to the training data is stored,\" [:br] \"        which might cause predictions to change if the data is modified\" [:br] \"        externally.\" [:br] \"\" [:br] \"    random_state : int or RandomState, default=None\" [:br] \"        Determines random number generation used to initialize the centers.\" [:br] \"        Pass an int for reproducible results across multiple function calls.\" [:br] \"        See :term: `Glossary <random_state>`.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    X_train_ : array-like of shape (n_samples, n_features) or list of object\" [:br] \"        Feature vectors or other representations of training data (also\" [:br] \"        required for prediction).\" [:br] \"\" [:br] \"    y_train_ : array-like of shape (n_samples,) or (n_samples, n_targets)\" [:br] \"        Target values in training data (also required for prediction)\" [:br] \"\" [:br] \"    kernel_ : kernel instance\" [:br] \"        The kernel used for prediction. The structure of the kernel is the\" [:br] \"        same as the one passed as parameter but with optimized hyperparameters\" [:br] \"\" [:br] \"    L_ : array-like of shape (n_samples, n_samples)\" [:br] \"        Lower-triangular Cholesky decomposition of the kernel in ``X_train_``\" [:br] \"\" [:br] \"    alpha_ : array-like of shape (n_samples,)\" [:br] \"        Dual coefficients of training data points in kernel space\" [:br] \"\" [:br] \"    log_marginal_likelihood_value_ : float\" [:br] \"        The log-marginal-likelihood of ``self.kernel_.theta``\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import make_friedman2\" [:br] \"    >>> from sklearn.gaussian_process import GaussianProcessRegressor\" [:br] \"    >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\" [:br] \"    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\" [:br] \"    >>> kernel = DotProduct() + WhiteKernel()\" [:br] \"    >>> gpr = GaussianProcessRegressor(kernel=kernel,\" [:br] \"    ...         random_state=0).fit(X, y)\" [:br] \"    >>> gpr.score(X, y)\" [:br] \"    0.3680...\" [:br] \"    >>> gpr.predict(X[:2,:], return_std=True)\" [:br] \"    (array([653.0..., 592.1...]), array([316.6..., 316.6...]))\" [:br] \"\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/gradient-boosting-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [23 2]:\\n\\n|                     :name |     :default |\\n|---------------------------|--------------|\\n|         :n-iter-no-change |              |\\n|            :learning-rate |       0.1000 |\\n| :min-weight-fraction-leaf |        0.000 |\\n|           :max-leaf-nodes |              |\\n|    :min-impurity-decrease |        0.000 |\\n|        :min-samples-split |        2.000 |\\n|                      :tol |    0.0001000 |\\n|                  :presort |   deprecated |\\n|                :subsample |        1.000 |\\n|                :ccp-alpha |        0.000 |\\n|             :random-state |              |\\n|         :min-samples-leaf |            1 |\\n|             :max-features |              |\\n|                     :init |              |\\n|       :min-impurity-split |              |\\n|                    :alpha |       0.9000 |\\n|               :warm-start |        false |\\n|                :max-depth |            3 |\\n|      :validation-fraction |       0.1000 |\\n|             :n-estimators |          100 |\\n|                :criterion | friedman_mse |\\n|                     :loss |           ls |\\n|                  :verbose |            0 |\\n\"]]] [:span (\"Gradient Boosting for regression.\" [:br] \"\" [:br] \"    GB builds an additive model in a forward stage-wise fashion;\" [:br] \"    it allows for the optimization of arbitrary differentiable loss functions.\" [:br] \"    In each stage a regression tree is fit on the negative gradient of the\" [:br] \"    given loss function.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <gradient_boosting>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    loss : {'ls', 'lad', 'huber', 'quantile'}, default='ls'\" [:br] \"        loss function to be optimized. 'ls' refers to least squares\" [:br] \"        regression. 'lad' (least absolute deviation) is a highly robust\" [:br] \"        loss function solely based on order information of the input\" [:br] \"        variables. 'huber' is a combination of the two. 'quantile'\" [:br] \"        allows quantile regression (use `alpha` to specify the quantile).\" [:br] \"\" [:br] \"    learning_rate : float, default=0.1\" [:br] \"        learning rate shrinks the contribution of each tree by `learning_rate`.\" [:br] \"        There is a trade-off between learning_rate and n_estimators.\" [:br] \"\" [:br] \"    n_estimators : int, default=100\" [:br] \"        The number of boosting stages to perform. Gradient boosting\" [:br] \"        is fairly robust to over-fitting so a large number usually\" [:br] \"        results in better performance.\" [:br] \"\" [:br] \"    subsample : float, default=1.0\" [:br] \"        The fraction of samples to be used for fitting the individual base\" [:br] \"        learners. If smaller than 1.0 this results in Stochastic Gradient\" [:br] \"        Boosting. `subsample` interacts with the parameter `n_estimators`.\" [:br] \"        Choosing `subsample < 1.0` leads to a reduction of variance\" [:br] \"        and an increase in bias.\" [:br] \"\" [:br] \"    criterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\" [:br] \"        The function to measure the quality of a split. Supported criteria\" [:br] \"        are \\\"friedman_mse\\\" for the mean squared error with improvement\" [:br] \"        score by Friedman, \\\"mse\\\" for mean squared error, and \\\"mae\\\" for\" [:br] \"        the mean absolute error. The default value of \\\"friedman_mse\\\" is\" [:br] \"        generally the best as it can provide a better approximation in\" [:br] \"        some cases.\" [:br] \"\" [:br] \"        .. versionadded:: 0.18\" [:br] \"\" [:br] \"    min_samples_split : int or float, default=2\" [:br] \"        The minimum number of samples required to split an internal node:\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_split` as the minimum number.\" [:br] \"        - If float, then `min_samples_split` is a fraction and\" [:br] \"          `ceil(min_samples_split * n_samples)` are the minimum\" [:br] \"          number of samples for each split.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_samples_leaf : int or float, default=1\" [:br] \"        The minimum number of samples required to be at a leaf node.\" [:br] \"        A split point at any depth will only be considered if it leaves at\" [:br] \"        least ``min_samples_leaf`` training samples in each of the left and\" [:br] \"        right branches.  This may have the effect of smoothing the model,\" [:br] \"        especially in regression.\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_leaf` as the minimum number.\" [:br] \"        - If float, then `min_samples_leaf` is a fraction and\" [:br] \"          `ceil(min_samples_leaf * n_samples)` are the minimum\" [:br] \"          number of samples for each node.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_weight_fraction_leaf : float, default=0.0\" [:br] \"        The minimum weighted fraction of the sum total of weights (of all\" [:br] \"        the input samples) required to be at a leaf node. Samples have\" [:br] \"        equal weight when sample_weight is not provided.\" [:br] \"\" [:br] \"    max_depth : int, default=3\" [:br] \"        maximum depth of the individual regression estimators. The maximum\" [:br] \"        depth limits the number of nodes in the tree. Tune this parameter\" [:br] \"        for best performance; the best value depends on the interaction\" [:br] \"        of the input variables.\" [:br] \"\" [:br] \"    min_impurity_decrease : float, default=0.0\" [:br] \"        A node will be split if this split induces a decrease of the impurity\" [:br] \"        greater than or equal to this value.\" [:br] \"\" [:br] \"        The weighted impurity decrease equation is the following::\" [:br] \"\" [:br] \"            N_t / N * (impurity - N_t_R / N_t * right_impurity\" [:br] \"                                - N_t_L / N_t * left_impurity)\" [:br] \"\" [:br] \"        where ``N`` is the total number of samples, ``N_t`` is the number of\" [:br] \"        samples at the current node, ``N_t_L`` is the number of samples in the\" [:br] \"        left child, and ``N_t_R`` is the number of samples in the right child.\" [:br] \"\" [:br] \"        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\" [:br] \"        if ``sample_weight`` is passed.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    min_impurity_split : float, default=None\" [:br] \"        Threshold for early stopping in tree growth. A node will split\" [:br] \"        if its impurity is above the threshold, otherwise it is a leaf.\" [:br] \"\" [:br] \"        .. deprecated:: 0.19\" [:br] \"           ``min_impurity_split`` has been deprecated in favor of\" [:br] \"           ``min_impurity_decrease`` in 0.19. The default value of\" [:br] \"           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\" [:br] \"           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\" [:br] \"\" [:br] \"    init : estimator or 'zero', default=None\" [:br] \"        An estimator object that is used to compute the initial predictions.\" [:br] \"        ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\" [:br] \"        initial raw predictions are set to zero. By default a\" [:br] \"        ``DummyEstimator`` is used, predicting either the average target value\" [:br] \"        (for loss='ls'), or a quantile for the other losses.\" [:br] \"\" [:br] \"    random_state : int or RandomState, default=None\" [:br] \"        Controls the random seed given to each Tree estimator at each\" [:br] \"        boosting iteration.\" [:br] \"        In addition, it controls the random permutation of the features at\" [:br] \"        each split (see Notes for more details).\" [:br] \"        It also controls the random spliting of the training data to obtain a\" [:br] \"        validation set if `n_iter_no_change` is not None.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\" [:br] \"        The number of features to consider when looking for the best split:\" [:br] \"\" [:br] \"        - If int, then consider `max_features` features at each split.\" [:br] \"        - If float, then `max_features` is a fraction and\" [:br] \"          `int(max_features * n_features)` features are considered at each\" [:br] \"          split.\" [:br] \"        - If \\\"auto\\\", then `max_features=n_features`.\" [:br] \"        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\" [:br] \"        - If \\\"log2\\\", then `max_features=log2(n_features)`.\" [:br] \"        - If None, then `max_features=n_features`.\" [:br] \"\" [:br] \"        Choosing `max_features < n_features` leads to a reduction of variance\" [:br] \"        and an increase in bias.\" [:br] \"\" [:br] \"        Note: the search for a split does not stop until at least one\" [:br] \"        valid partition of the node samples is found, even if it requires to\" [:br] \"        effectively inspect more than ``max_features`` features.\" [:br] \"\" [:br] \"    alpha : float, default=0.9\" [:br] \"        The alpha-quantile of the huber loss function and the quantile\" [:br] \"        loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        Enable verbose output. If 1 then it prints progress and performance\" [:br] \"        once in a while (the more trees the lower the frequency). If greater\" [:br] \"        than 1 then it prints progress and performance for every tree.\" [:br] \"\" [:br] \"    max_leaf_nodes : int, default=None\" [:br] \"        Grow trees with ``max_leaf_nodes`` in best-first fashion.\" [:br] \"        Best nodes are defined as relative reduction in impurity.\" [:br] \"        If None then unlimited number of leaf nodes.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to ``True``, reuse the solution of the previous call to fit\" [:br] \"        and add more estimators to the ensemble, otherwise, just erase the\" [:br] \"        previous solution. See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    presort : deprecated, default='deprecated'\" [:br] \"        This parameter is deprecated and will be removed in v0.24.\" [:br] \"\" [:br] \"        .. deprecated :: 0.22\" [:br] \"\" [:br] \"    validation_fraction : float, default=0.1\" [:br] \"        The proportion of training data to set aside as validation set for\" [:br] \"        early stopping. Must be between 0 and 1.\" [:br] \"        Only used if ``n_iter_no_change`` is set to an integer.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    n_iter_no_change : int, default=None\" [:br] \"        ``n_iter_no_change`` is used to decide if early stopping will be used\" [:br] \"        to terminate training when validation score is not improving. By\" [:br] \"        default it is set to None to disable early stopping. If set to a\" [:br] \"        number, it will set aside ``validation_fraction`` size of the training\" [:br] \"        data as validation and terminate training when validation score is not\" [:br] \"        improving in all of the previous ``n_iter_no_change`` numbers of\" [:br] \"        iterations.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        Tolerance for the early stopping. When the loss is not improving\" [:br] \"        by at least tol for ``n_iter_no_change`` iterations (if set to a\" [:br] \"        number), the training stops.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    ccp_alpha : non-negative float, default=0.0\" [:br] \"        Complexity parameter used for Minimal Cost-Complexity Pruning. The\" [:br] \"        subtree with the largest cost complexity that is smaller than\" [:br] \"        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\" [:br] \"        :ref:`minimal_cost_complexity_pruning` for details.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    feature_importances_ : ndarray of shape (n_features,)\" [:br] \"        The impurity-based feature importances.\" [:br] \"        The higher, the more important the feature.\" [:br] \"        The importance of a feature is computed as the (normalized)\" [:br] \"        total reduction of the criterion brought by that feature.  It is also\" [:br] \"        known as the Gini importance.\" [:br] \"\" [:br] \"        Warning: impurity-based feature importances can be misleading for\" [:br] \"        high cardinality features (many unique values). See\" [:br] \"        :func:`sklearn.inspection.permutation_importance` as an alternative.\" [:br] \"\" [:br] \"    oob_improvement_ : ndarray of shape (n_estimators,)\" [:br] \"        The improvement in loss (= deviance) on the out-of-bag samples\" [:br] \"        relative to the previous iteration.\" [:br] \"        ``oob_improvement_[0]`` is the improvement in\" [:br] \"        loss of the first stage over the ``init`` estimator.\" [:br] \"        Only available if ``subsample < 1.0``\" [:br] \"\" [:br] \"    train_score_ : ndarray of shape (n_estimators,)\" [:br] \"        The i-th score ``train_score_[i]`` is the deviance (= loss) of the\" [:br] \"        model at iteration ``i`` on the in-bag sample.\" [:br] \"        If ``subsample == 1`` this is the deviance on the training data.\" [:br] \"\" [:br] \"    loss_ : LossFunction\" [:br] \"        The concrete ``LossFunction`` object.\" [:br] \"\" [:br] \"    init_ : estimator\" [:br] \"        The estimator that provides the initial predictions.\" [:br] \"        Set via the ``init`` argument or ``loss.init_estimator``.\" [:br] \"\" [:br] \"    estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\" [:br] \"        The collection of fitted sub-estimators.\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        The number of data features.\" [:br] \"\" [:br] \"    max_features_ : int\" [:br] \"        The inferred value of max_features.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The features are always randomly permuted at each split. Therefore,\" [:br] \"    the best found split may vary, even with the same training data and\" [:br] \"    ``max_features=n_features``, if the improvement of the criterion is\" [:br] \"    identical for several splits enumerated during the search of the best\" [:br] \"    split. To obtain a deterministic behaviour during fitting,\" [:br] \"    ``random_state`` has to be fixed.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> from sklearn.ensemble import GradientBoostingRegressor\" [:br] \"    >>> from sklearn.model_selection import train_test_split\" [:br] \"    >>> X, y = make_regression(random_state=0)\" [:br] \"    >>> X_train, X_test, y_train, y_test = train_test_split(\" [:br] \"    ...     X, y, random_state=0)\" [:br] \"    >>> reg = GradientBoostingRegressor(random_state=0)\" [:br] \"    >>> reg.fit(X_train, y_train)\" [:br] \"    GradientBoostingRegressor(random_state=0)\" [:br] \"    >>> reg.predict(X_test[1:2])\" [:br] \"    array([-61...])\" [:br] \"    >>> reg.score(X_test, y_test)\" [:br] \"    0.4...\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    sklearn.ensemble.HistGradientBoostingRegressor,\" [:br] \"    sklearn.tree.DecisionTreeRegressor, RandomForestRegressor\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    J. Friedman, Greedy Function Approximation: A Gradient Boosting\" [:br] \"    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\" [:br] \"\" [:br] \"    J. Friedman, Stochastic Gradient Boosting, 1999\" [:br] \"\" [:br] \"    T. Hastie, R. Tibshirani and J. Friedman.\" [:br] \"    Elements of Statistical Learning Ed. 2, Springer, 2009.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/hist-gradient-boosting-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [17 2]:\\n\\n|                :name |      :default |\\n|----------------------|---------------|\\n|    :n-iter-no-change |         10.00 |\\n|       :learning-rate |        0.1000 |\\n|      :max-leaf-nodes |         31.00 |\\n|             :scoring |          loss |\\n|                 :tol |     1.000E-07 |\\n|      :early-stopping |          auto |\\n|            :max-iter |           100 |\\n|        :random-state |               |\\n|            :max-bins |           255 |\\n|    :min-samples-leaf |            20 |\\n|       :monotonic-cst |               |\\n|          :warm-start |         false |\\n|           :max-depth |               |\\n| :validation-fraction |        0.1000 |\\n|                :loss | least_squares |\\n|             :verbose |             0 |\\n|  :l-2-regularization |         0.000 |\\n\"]]] [:span (\"Histogram-based Gradient Boosting Regression Tree.\" [:br] \"\" [:br] \"    This estimator is much faster than\" [:br] \"    :class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`\" [:br] \"    for big datasets (n_samples >= 10 000).\" [:br] \"\" [:br] \"    This estimator has native support for missing values (NaNs). During\" [:br] \"    training, the tree grower learns at each split point whether samples\" [:br] \"    with missing values should go to the left or right child, based on the\" [:br] \"    potential gain. When predicting, samples with missing values are\" [:br] \"    assigned to the left or right child consequently. If no missing values\" [:br] \"    were encountered for a given feature during training, then samples with\" [:br] \"    missing values are mapped to whichever child has the most samples.\" [:br] \"\" [:br] \"    This implementation is inspired by\" [:br] \"    `LightGBM <https://github.com/Microsoft/LightGBM>`_.\" [:br] \"\" [:br] \"    .. note::\" [:br] \"\" [:br] \"      This estimator is still **experimental** for now: the predictions\" [:br] \"      and the API might change without any deprecation cycle. To use it,\" [:br] \"      you need to explicitly import ``enable_hist_gradient_boosting``::\" [:br] \"\" [:br] \"        >>> # explicitly require this experimental feature\" [:br] \"        >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\" [:br] \"        >>> # now you can import normally from ensemble\" [:br] \"        >>> from sklearn.ensemble import HistGradientBoostingClassifier\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.21\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    loss : {'least_squares', 'least_absolute_deviation', 'poisson'},             optional (default='least_squares')\" [:br] \"        The loss function to use in the boosting process. Note that the\" [:br] \"        \\\"least squares\\\" and \\\"poisson\\\" losses actually implement\" [:br] \"        \\\"half least squares loss\\\" and \\\"half poisson deviance\\\" to simplify the\" [:br] \"        computation of the gradient. Furthermore, \\\"poisson\\\" loss internally\" [:br] \"        uses a log-link and requires ``y >= 0``\" [:br] \"    learning_rate : float, optional (default=0.1)\" [:br] \"        The learning rate, also known as *shrinkage*. This is used as a\" [:br] \"        multiplicative factor for the leaves values. Use ``1`` for no\" [:br] \"        shrinkage.\" [:br] \"    max_iter : int, optional (default=100)\" [:br] \"        The maximum number of iterations of the boosting process, i.e. the\" [:br] \"        maximum number of trees.\" [:br] \"    max_leaf_nodes : int or None, optional (default=31)\" [:br] \"        The maximum number of leaves for each tree. Must be strictly greater\" [:br] \"        than 1. If None, there is no maximum limit.\" [:br] \"    max_depth : int or None, optional (default=None)\" [:br] \"        The maximum depth of each tree. The depth of a tree is the number of\" [:br] \"        edges to go from the root to the deepest leaf.\" [:br] \"        Depth isn't constrained by default.\" [:br] \"    min_samples_leaf : int, optional (default=20)\" [:br] \"        The minimum number of samples per leaf. For small datasets with less\" [:br] \"        than a few hundred samples, it is recommended to lower this value\" [:br] \"        since only very shallow trees would be built.\" [:br] \"    l2_regularization : float, optional (default=0)\" [:br] \"        The L2 regularization parameter. Use ``0`` for no regularization\" [:br] \"        (default).\" [:br] \"    max_bins : int, optional (default=255)\" [:br] \"        The maximum number of bins to use for non-missing values. Before\" [:br] \"        training, each feature of the input array `X` is binned into\" [:br] \"        integer-valued bins, which allows for a much faster training stage.\" [:br] \"        Features with a small number of unique values may use less than\" [:br] \"        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\" [:br] \"        is always reserved for missing values. Must be no larger than 255.\" [:br] \"    monotonic_cst : array-like of int of shape (n_features), default=None\" [:br] \"        Indicates the monotonic constraint to enforce on each feature. -1, 1\" [:br] \"        and 0 respectively correspond to a positive constraint, negative\" [:br] \"        constraint and no constraint. Read more in the :ref:`User Guide\" [:br] \"        <monotonic_cst_gbdt>`.\" [:br] \"    warm_start : bool, optional (default=False)\" [:br] \"        When set to ``True``, reuse the solution of the previous call to fit\" [:br] \"        and add more estimators to the ensemble. For results to be valid, the\" [:br] \"        estimator should be re-trained on the same data only.\" [:br] \"        See :term:`the Glossary <warm_start>`.\" [:br] \"    early_stopping : 'auto' or bool (default='auto')\" [:br] \"        If 'auto', early stopping is enabled if the sample size is larger than\" [:br] \"        10000. If True, early stopping is enabled, otherwise early stopping is\" [:br] \"        disabled.\" [:br] \"    scoring : str or callable or None, optional (default='loss')\" [:br] \"        Scoring parameter to use for early stopping. It can be a single\" [:br] \"        string (see :ref:`scoring_parameter`) or a callable (see\" [:br] \"        :ref:`scoring`). If None, the estimator's default scorer is used. If\" [:br] \"        ``scoring='loss'``, early stopping is checked w.r.t the loss value.\" [:br] \"        Only used if early stopping is performed.\" [:br] \"    validation_fraction : int or float or None, optional (default=0.1)\" [:br] \"        Proportion (or absolute size) of training data to set aside as\" [:br] \"        validation data for early stopping. If None, early stopping is done on\" [:br] \"        the training data. Only used if early stopping is performed.\" [:br] \"    n_iter_no_change : int, optional (default=10)\" [:br] \"        Used to determine when to \\\"early stop\\\". The fitting process is\" [:br] \"        stopped when none of the last ``n_iter_no_change`` scores are better\" [:br] \"        than the ``n_iter_no_change - 1`` -th-to-last one, up to some\" [:br] \"        tolerance. Only used if early stopping is performed.\" [:br] \"    tol : float or None, optional (default=1e-7)\" [:br] \"        The absolute tolerance to use when comparing scores during early\" [:br] \"        stopping. The higher the tolerance, the more likely we are to early\" [:br] \"        stop: higher tolerance means that it will be harder for subsequent\" [:br] \"        iterations to be considered an improvement upon the reference score.\" [:br] \"    verbose: int, optional (default=0)\" [:br] \"        The verbosity level. If not zero, print some information about the\" [:br] \"        fitting process.\" [:br] \"    random_state : int, np.random.RandomStateInstance or None,         optional (default=None)\" [:br] \"        Pseudo-random number generator to control the subsampling in the\" [:br] \"        binning process, and the train/validation data split if early stopping\" [:br] \"        is enabled.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    n_iter_ : int\" [:br] \"        The number of iterations as selected by early stopping, depending on\" [:br] \"        the `early_stopping` parameter. Otherwise it corresponds to max_iter.\" [:br] \"    n_trees_per_iteration_ : int\" [:br] \"        The number of tree that are built at each iteration. For regressors,\" [:br] \"        this is always 1.\" [:br] \"    train_score_ : ndarray, shape (n_iter_+1,)\" [:br] \"        The scores at each iteration on the training data. The first entry\" [:br] \"        is the score of the ensemble before the first iteration. Scores are\" [:br] \"        computed according to the ``scoring`` parameter. If ``scoring`` is\" [:br] \"        not 'loss', scores are computed on a subset of at most 10 000\" [:br] \"        samples. Empty if no early stopping.\" [:br] \"    validation_score_ : ndarray, shape (n_iter_+1,)\" [:br] \"        The scores at each iteration on the held-out validation data. The\" [:br] \"        first entry is the score of the ensemble before the first iteration.\" [:br] \"        Scores are computed according to the ``scoring`` parameter. Empty if\" [:br] \"        no early stopping or if ``validation_fraction`` is None.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> # To use this experimental feature, we need to explicitly ask for it:\" [:br] \"    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\" [:br] \"    >>> from sklearn.ensemble import HistGradientBoostingRegressor\" [:br] \"    >>> from sklearn.datasets import load_diabetes\" [:br] \"    >>> X, y = load_diabetes(return_X_y=True)\" [:br] \"    >>> est = HistGradientBoostingRegressor().fit(X, y)\" [:br] \"    >>> est.score(X, y)\" [:br] \"    0.92...\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/huber-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"368px\"}} [:p/markdown \"_unnamed [6 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|         :alpha | 0.0001000 |\\n|       :epsilon |     1.350 |\\n| :fit-intercept |      true |\\n|      :max-iter |       100 |\\n|           :tol | 1.000E-05 |\\n|    :warm-start |     false |\\n\"]]] [:span (\"Linear regression model that is robust to outliers.\" [:br] \"\" [:br] \"    The Huber Regressor optimizes the squared loss for the samples where\" [:br] \"    ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples\" [:br] \"    where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters\" [:br] \"    to be optimized. The parameter sigma makes sure that if y is scaled up\" [:br] \"    or down by a certain factor, one does not need to rescale epsilon to\" [:br] \"    achieve the same robustness. Note that this does not take into account\" [:br] \"    the fact that the different features of X may be of different scales.\" [:br] \"\" [:br] \"    This makes sure that the loss function is not heavily influenced by the\" [:br] \"    outliers while not completely ignoring their effect.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <huber_regression>`\" [:br] \"\" [:br] \"    .. versionadded:: 0.18\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    epsilon : float, greater than 1.0, default 1.35\" [:br] \"        The parameter epsilon controls the number of samples that should be\" [:br] \"        classified as outliers. The smaller the epsilon, the more robust it is\" [:br] \"        to outliers.\" [:br] \"\" [:br] \"    max_iter : int, default 100\" [:br] \"        Maximum number of iterations that\" [:br] \"        ``scipy.optimize.minimize(method=\\\"L-BFGS-B\\\")`` should run for.\" [:br] \"\" [:br] \"    alpha : float, default 0.0001\" [:br] \"        Regularization parameter.\" [:br] \"\" [:br] \"    warm_start : bool, default False\" [:br] \"        This is useful if the stored attributes of a previously used model\" [:br] \"        has to be reused. If set to False, then the coefficients will\" [:br] \"        be rewritten for every call to fit.\" [:br] \"        See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    fit_intercept : bool, default True\" [:br] \"        Whether or not to fit the intercept. This can be set to False\" [:br] \"        if the data is already centered around the origin.\" [:br] \"\" [:br] \"    tol : float, default 1e-5\" [:br] \"        The iteration will stop when\" [:br] \"        ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\" [:br] \"        where pg_i is the i-th component of the projected gradient.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array, shape (n_features,)\" [:br] \"        Features got by optimizing the Huber loss.\" [:br] \"\" [:br] \"    intercept_ : float\" [:br] \"        Bias.\" [:br] \"\" [:br] \"    scale_ : float\" [:br] \"        The value by which ``|y - X'w - c|`` is scaled down.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        Number of iterations that\" [:br] \"        ``scipy.optimize.minimize(method=\\\"L-BFGS-B\\\")`` has run for.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.20\" [:br] \"\" [:br] \"            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\" [:br] \"            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\" [:br] \"\" [:br] \"    outliers_ : array, shape (n_samples,)\" [:br] \"        A boolean mask which is set to True where the samples are identified\" [:br] \"        as outliers.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> from sklearn.linear_model import HuberRegressor, LinearRegression\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> rng = np.random.RandomState(0)\" [:br] \"    >>> X, y, coef = make_regression(\" [:br] \"    ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\" [:br] \"    >>> X[:4] = rng.uniform(10, 20, (4, 2))\" [:br] \"    >>> y[:4] = rng.uniform(10, 20, 4)\" [:br] \"    >>> huber = HuberRegressor().fit(X, y)\" [:br] \"    >>> huber.score(X, y)\" [:br] \"    -7.284...\" [:br] \"    >>> huber.predict(X[:1,])\" [:br] \"    array([806.7200...])\" [:br] \"    >>> linear = LinearRegression().fit(X, y)\" [:br] \"    >>> print(\\\"True coefficients:\\\", coef)\" [:br] \"    True coefficients: [20.4923...  34.1698...]\" [:br] \"    >>> print(\\\"Huber coefficients:\\\", huber.coef_)\" [:br] \"    Huber coefficients: [17.7906... 31.0106...]\" [:br] \"    >>> print(\\\"Linear Regression coefficients:\\\", linear.coef_)\" [:br] \"    Linear Regression coefficients: [-1.9221...  7.0226...]\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\" [:br] \"           Concomitant scale estimates, pg 172\" [:br] \"    .. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.\" [:br] \"           https://statweb.stanford.edu/~owen/reports/hhu.pdf\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/isotonic-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"276px\"}} [:p/markdown \"_unnamed [4 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|    :increasing |     true |\\n| :out-of-bounds |      nan |\\n|         :y-max |          |\\n|         :y-min |          |\\n\"]]] [:span (\"Isotonic regression model.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <isotonic>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.13\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    y_min : float, default=None\" [:br] \"        Lower bound on the lowest predicted value (the minimum value may\" [:br] \"        still be higher). If not set, defaults to -inf.\" [:br] \"\" [:br] \"    y_max : float, default=None\" [:br] \"        Upper bound on the highest predicted value (the maximum may still be\" [:br] \"        lower). If not set, defaults to +inf.\" [:br] \"\" [:br] \"    increasing : bool or 'auto', default=True\" [:br] \"        Determines whether the predictions should be constrained to increase\" [:br] \"        or decrease with `X`. 'auto' will decide based on the Spearman\" [:br] \"        correlation estimate's sign.\" [:br] \"\" [:br] \"    out_of_bounds : str, default=\\\"nan\\\"\" [:br] \"        The ``out_of_bounds`` parameter handles how `X` values outside of the\" [:br] \"        training domain are handled.  When set to \\\"nan\\\", predictions\" [:br] \"        will be NaN.  When set to \\\"clip\\\", predictions will be\" [:br] \"        set to the value corresponding to the nearest train interval endpoint.\" [:br] \"        When set to \\\"raise\\\" a `ValueError` is raised.\" [:br] \"\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    X_min_ : float\" [:br] \"        Minimum value of input array `X_` for left bound.\" [:br] \"\" [:br] \"    X_max_ : float\" [:br] \"        Maximum value of input array `X_` for right bound.\" [:br] \"\" [:br] \"    f_ : function\" [:br] \"        The stepwise interpolating function that covers the input domain ``X``.\" [:br] \"\" [:br] \"    increasing_ : bool\" [:br] \"        Inferred value for ``increasing``.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    Ties are broken using the secondary method from Leeuw, 1977.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    Isotonic Median Regression: A Linear Programming Approach\" [:br] \"    Nilotpal Chakravarti\" [:br] \"    Mathematics of Operations Research\" [:br] \"    Vol. 14, No. 2 (May, 1989), pp. 303-308\" [:br] \"\" [:br] \"    Isotone Optimization in R : Pool-Adjacent-Violators\" [:br] \"    Algorithm (PAVA) and Active Set Methods\" [:br] \"    Leeuw, Hornik, Mair\" [:br] \"    Journal of Statistical Software 2009\" [:br] \"\" [:br] \"    Correctness of Kruskal's algorithms for monotone regression with ties\" [:br] \"    Leeuw, Psychometrica, 1977\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> from sklearn.isotonic import IsotonicRegression\" [:br] \"    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\" [:br] \"    >>> iso_reg = IsotonicRegression().fit(X.flatten(), y)\" [:br] \"    >>> iso_reg.predict([.1, .2])\" [:br] \"    array([1.8628..., 3.7256...])\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/k-neighbors-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :algorithm |      auto |\\n|     :leaf-size |        30 |\\n|        :metric | minkowski |\\n| :metric-params |           |\\n|        :n-jobs |           |\\n|   :n-neighbors |         5 |\\n|             :p |         2 |\\n|       :weights |   uniform |\\n\"]]] [:span (\"Regression based on k-nearest neighbors.\" [:br] \"\" [:br] \"    The target is predicted by local interpolation of the targets\" [:br] \"    associated of the nearest neighbors in the training set.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <regression>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.9\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    n_neighbors : int, default=5\" [:br] \"        Number of neighbors to use by default for :meth:`kneighbors` queries.\" [:br] \"\" [:br] \"    weights : {'uniform', 'distance'} or callable, default='uniform'\" [:br] \"        weight function used in prediction.  Possible values:\" [:br] \"\" [:br] \"        - 'uniform' : uniform weights.  All points in each neighborhood\" [:br] \"          are weighted equally.\" [:br] \"        - 'distance' : weight points by the inverse of their distance.\" [:br] \"          in this case, closer neighbors of a query point will have a\" [:br] \"          greater influence than neighbors which are further away.\" [:br] \"        - [callable] : a user-defined function which accepts an\" [:br] \"          array of distances, and returns an array of the same shape\" [:br] \"          containing the weights.\" [:br] \"\" [:br] \"        Uniform weights are used by default.\" [:br] \"\" [:br] \"    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\" [:br] \"        Algorithm used to compute the nearest neighbors:\" [:br] \"\" [:br] \"        - 'ball_tree' will use :class:`BallTree`\" [:br] \"        - 'kd_tree' will use :class:`KDTree`\" [:br] \"        - 'brute' will use a brute-force search.\" [:br] \"        - 'auto' will attempt to decide the most appropriate algorithm\" [:br] \"          based on the values passed to :meth:`fit` method.\" [:br] \"\" [:br] \"        Note: fitting on sparse input will override the setting of\" [:br] \"        this parameter, using brute force.\" [:br] \"\" [:br] \"    leaf_size : int, default=30\" [:br] \"        Leaf size passed to BallTree or KDTree.  This can affect the\" [:br] \"        speed of the construction and query, as well as the memory\" [:br] \"        required to store the tree.  The optimal value depends on the\" [:br] \"        nature of the problem.\" [:br] \"\" [:br] \"    p : int, default=2\" [:br] \"        Power parameter for the Minkowski metric. When p = 1, this is\" [:br] \"        equivalent to using manhattan_distance (l1), and euclidean_distance\" [:br] \"        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\" [:br] \"\" [:br] \"    metric : str or callable, default='minkowski'\" [:br] \"        the distance metric to use for the tree.  The default metric is\" [:br] \"        minkowski, and with p=2 is equivalent to the standard Euclidean\" [:br] \"        metric. See the documentation of :class:`DistanceMetric` for a\" [:br] \"        list of available metrics.\" [:br] \"        If metric is \\\"precomputed\\\", X is assumed to be a distance matrix and\" [:br] \"        must be square during fit. X may be a :term:`sparse graph`,\" [:br] \"        in which case only \\\"nonzero\\\" elements may be considered neighbors.\" [:br] \"\" [:br] \"    metric_params : dict, default=None\" [:br] \"        Additional keyword arguments for the metric function.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of parallel jobs to run for neighbors search.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"        Doesn't affect :meth:`fit` method.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    effective_metric_ : str or callable\" [:br] \"        The distance metric to use. It will be same as the `metric` parameter\" [:br] \"        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\" [:br] \"        'minkowski' and `p` parameter set to 2.\" [:br] \"\" [:br] \"    effective_metric_params_ : dict\" [:br] \"        Additional keyword arguments for the metric function. For most metrics\" [:br] \"        will be same with `metric_params` parameter, but may also contain the\" [:br] \"        `p` parameter value if the `effective_metric_` attribute is set to\" [:br] \"        'minkowski'.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> X = [[0], [1], [2], [3]]\" [:br] \"    >>> y = [0, 0, 1, 1]\" [:br] \"    >>> from sklearn.neighbors import KNeighborsRegressor\" [:br] \"    >>> neigh = KNeighborsRegressor(n_neighbors=2)\" [:br] \"    >>> neigh.fit(X, y)\" [:br] \"    KNeighborsRegressor(...)\" [:br] \"    >>> print(neigh.predict([[1.5]]))\" [:br] \"    [0.5]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    NearestNeighbors\" [:br] \"    RadiusNeighborsRegressor\" [:br] \"    KNeighborsClassifier\" [:br] \"    RadiusNeighborsClassifier\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\" [:br] \"    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\" [:br] \"\" [:br] \"    .. warning::\" [:br] \"\" [:br] \"       Regarding the Nearest Neighbors algorithms, if it is found that two\" [:br] \"       neighbors, neighbor `k+1` and `k`, have identical distances but\" [:br] \"       different labels, the results will depend on the ordering of the\" [:br] \"       training data.\" [:br] \"\" [:br] \"    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/kernel-ridge\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"368px\"}} [:p/markdown \"_unnamed [6 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|         :alpha |        1 |\\n|        :coef-0 |        1 |\\n|        :degree |        3 |\\n|         :gamma |          |\\n|        :kernel |   linear |\\n| :kernel-params |          |\\n\"]]] [:span (\"Kernel ridge regression.\" [:br] \"\" [:br] \"    Kernel ridge regression (KRR) combines ridge regression (linear least\" [:br] \"    squares with l2-norm regularization) with the kernel trick. It thus\" [:br] \"    learns a linear function in the space induced by the respective kernel and\" [:br] \"    the data. For non-linear kernels, this corresponds to a non-linear\" [:br] \"    function in the original space.\" [:br] \"\" [:br] \"    The form of the model learned by KRR is identical to support vector\" [:br] \"    regression (SVR). However, different loss functions are used: KRR uses\" [:br] \"    squared error loss while support vector regression uses epsilon-insensitive\" [:br] \"    loss, both combined with l2 regularization. In contrast to SVR, fitting a\" [:br] \"    KRR model can be done in closed-form and is typically faster for\" [:br] \"    medium-sized datasets. On the other hand, the learned model is non-sparse\" [:br] \"    and thus slower than SVR, which learns a sparse model for epsilon > 0, at\" [:br] \"    prediction-time.\" [:br] \"\" [:br] \"    This estimator has built-in support for multi-variate regression\" [:br] \"    (i.e., when y is a 2d-array of shape [n_samples, n_targets]).\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <kernel_ridge>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : float or array-like of shape (n_targets,)\" [:br] \"        Regularization strength; must be a positive float. Regularization\" [:br] \"        improves the conditioning of the problem and reduces the variance of\" [:br] \"        the estimates. Larger values specify stronger regularization.\" [:br] \"        Alpha corresponds to ``1 / (2C)`` in other linear models such as\" [:br] \"        :class:`~sklearn.linear_model.LogisticRegression` or\" [:br] \"        :class:`sklearn.svm.LinearSVC`. If an array is passed, penalties are\" [:br] \"        assumed to be specific to the targets. Hence they must correspond in\" [:br] \"        number. See :ref:`ridge_regression` for formula.\" [:br] \"\" [:br] \"    kernel : string or callable, default=\\\"linear\\\"\" [:br] \"        Kernel mapping used internally. This parameter is directly passed to\" [:br] \"        :class:`sklearn.metrics.pairwise.pairwise_kernel`.\" [:br] \"        If `kernel` is a string, it must be one of the metrics\" [:br] \"        in `pairwise.PAIRWISE_KERNEL_FUNCTIONS`.\" [:br] \"        If `kernel` is \\\"precomputed\\\", X is assumed to be a kernel matrix.\" [:br] \"        Alternatively, if `kernel` is a callable function, it is called on\" [:br] \"        each pair of instances (rows) and the resulting value recorded. The\" [:br] \"        callable should take two rows from X as input and return the\" [:br] \"        corresponding kernel value as a single number. This means that\" [:br] \"        callables from :mod:`sklearn.metrics.pairwise` are not allowed, as\" [:br] \"        they operate on matrices, not single samples. Use the string\" [:br] \"        identifying the kernel instead.\" [:br] \"\" [:br] \"    gamma : float, default=None\" [:br] \"        Gamma parameter for the RBF, laplacian, polynomial, exponential chi2\" [:br] \"        and sigmoid kernels. Interpretation of the default value is left to\" [:br] \"        the kernel; see the documentation for sklearn.metrics.pairwise.\" [:br] \"        Ignored by other kernels.\" [:br] \"\" [:br] \"    degree : float, default=3\" [:br] \"        Degree of the polynomial kernel. Ignored by other kernels.\" [:br] \"\" [:br] \"    coef0 : float, default=1\" [:br] \"        Zero coefficient for polynomial and sigmoid kernels.\" [:br] \"        Ignored by other kernels.\" [:br] \"\" [:br] \"    kernel_params : mapping of string to any, optional\" [:br] \"        Additional parameters (keyword arguments) for kernel function passed\" [:br] \"        as callable object.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    dual_coef_ : ndarray of shape (n_samples,) or (n_samples, n_targets)\" [:br] \"        Representation of weight vector(s) in kernel space\" [:br] \"\" [:br] \"    X_fit_ : {ndarray, sparse matrix} of shape (n_samples, n_features)\" [:br] \"        Training data, which is also required for prediction. If\" [:br] \"        kernel == \\\"precomputed\\\" this is instead the precomputed\" [:br] \"        training matrix, of shape (n_samples, n_samples).\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    * Kevin P. Murphy\" [:br] \"      \\\"Machine Learning: A Probabilistic Perspective\\\", The MIT Press\" [:br] \"      chapter 14.4.3, pp. 492-493\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    sklearn.linear_model.Ridge:\" [:br] \"        Linear ridge regression.\" [:br] \"    sklearn.svm.SVR:\" [:br] \"        Support Vector Regression implemented using libsvm.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.kernel_ridge import KernelRidge\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> n_samples, n_features = 10, 5\" [:br] \"    >>> rng = np.random.RandomState(0)\" [:br] \"    >>> y = rng.randn(n_samples)\" [:br] \"    >>> X = rng.randn(n_samples, n_features)\" [:br] \"    >>> clf = KernelRidge(alpha=1.0)\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    KernelRidge(alpha=1.0)\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/lars\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [10 2]:\\n\\n|            :name |  :default |\\n|------------------|-----------|\\n|       :normalize |      true |\\n|        :fit-path |      true |\\n|             :eps | 2.220E-16 |\\n|    :random-state |           |\\n|          :jitter |           |\\n|          :copy-x |      true |\\n|      :precompute |      auto |\\n|   :fit-intercept |      true |\\n| :n-nonzero-coefs |       500 |\\n|         :verbose |     false |\\n\"]]] [:span (\"Least Angle Regression model a.k.a. LAR\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <least_angle_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    verbose : bool or int, default=False\" [:br] \"        Sets the verbosity amount\" [:br] \"\" [:br] \"    normalize : bool, default=True\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    precompute : bool, 'auto' or array-like , default='auto'\" [:br] \"        Whether to use a precomputed Gram matrix to speed up\" [:br] \"        calculations. If set to ``'auto'`` let us decide. The Gram\" [:br] \"        matrix can also be passed as argument.\" [:br] \"\" [:br] \"    n_nonzero_coefs : int, default=500\" [:br] \"        Target number of non-zero coefficients. Use ``np.inf`` for no limit.\" [:br] \"\" [:br] \"    eps : float, optional\" [:br] \"        The machine-precision regularization in the computation of the\" [:br] \"        Cholesky diagonal factors. Increase this for very ill-conditioned\" [:br] \"        systems. Unlike the ``tol`` parameter in some iterative\" [:br] \"        optimization-based algorithms, this parameter does not control\" [:br] \"        the tolerance of the optimization.\" [:br] \"        By default, ``np.finfo(np.float).eps`` is used.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If ``True``, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    fit_path : bool, default=True\" [:br] \"        If True the full path is stored in the ``coef_path_`` attribute.\" [:br] \"        If you compute the solution for a large problem or many targets,\" [:br] \"        setting ``fit_path`` to ``False`` will lead to a speedup, especially\" [:br] \"        with a small alpha.\" [:br] \"\" [:br] \"    jitter : float, default=None\" [:br] \"        Upper bound on a uniform noise parameter to be added to the\" [:br] \"        `y` values, to satisfy the model's assumption of\" [:br] \"        one-at-a-time computations. Might help with stability.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance or None (default)\" [:br] \"        Determines random number generation for jittering. Pass an int\" [:br] \"        for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    alphas_ : array-like of shape (n_alphas + 1,) | list of n_targets such             arrays\" [:br] \"        Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``n_nonzero_coefs`` or ``n_features``,         whichever is smaller.\" [:br] \"\" [:br] \"    active_ : list, length = n_alphas | list of n_targets such lists\" [:br] \"        Indices of active variables at the end of the path.\" [:br] \"\" [:br] \"    coef_path_ : array-like of shape (n_features, n_alphas + 1)         | list of n_targets such arrays\" [:br] \"        The varying values of the coefficients along the path. It is not\" [:br] \"        present if the ``fit_path`` parameter is ``False``.\" [:br] \"\" [:br] \"    coef_ : array-like of shape (n_features,) or (n_targets, n_features)\" [:br] \"        Parameter vector (w in the formulation formula).\" [:br] \"\" [:br] \"    intercept_ : float or array-like of shape (n_targets,)\" [:br] \"        Independent term in decision function.\" [:br] \"\" [:br] \"    n_iter_ : array-like or int\" [:br] \"        The number of iterations taken by lars_path to find the\" [:br] \"        grid of alphas for each target.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn import linear_model\" [:br] \"    >>> reg = linear_model.Lars(n_nonzero_coefs=1)\" [:br] \"    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\" [:br] \"    Lars(n_nonzero_coefs=1)\" [:br] \"    >>> print(reg.coef_)\" [:br] \"    [ 0. -1.11...]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    lars_path, LarsCV\" [:br] \"    sklearn.decomposition.sparse_encode\" [:br] \"\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/lars-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [10 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |      true |\\n|           :eps | 2.220E-16 |\\n|  :max-n-alphas |      1000 |\\n|      :max-iter |       500 |\\n|        :n-jobs |           |\\n|        :copy-x |      true |\\n|    :precompute |      auto |\\n| :fit-intercept |      true |\\n|            :cv |           |\\n|       :verbose |     false |\\n\"]]] [:span (\"Cross-validated Least Angle Regression model.\" [:br] \"\" [:br] \"    See glossary entry for :term:`cross-validation estimator`.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <least_angle_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    verbose : bool or int, default=False\" [:br] \"        Sets the verbosity amount\" [:br] \"\" [:br] \"    max_iter : int, default=500\" [:br] \"        Maximum number of iterations to perform.\" [:br] \"\" [:br] \"    normalize : bool, default=True\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    precompute : bool, 'auto' or array-like , default='auto'\" [:br] \"        Whether to use a precomputed Gram matrix to speed up\" [:br] \"        calculations. If set to ``'auto'`` let us decide. The Gram matrix\" [:br] \"        cannot be passed as argument since we will use only subsets of X.\" [:br] \"\" [:br] \"    cv : int, cross-validation generator or an iterable, default=None\" [:br] \"        Determines the cross-validation splitting strategy.\" [:br] \"        Possible inputs for cv are:\" [:br] \"\" [:br] \"        - None, to use the default 5-fold cross-validation,\" [:br] \"        - integer, to specify the number of folds.\" [:br] \"        - :term:`CV splitter`,\" [:br] \"        - An iterable yielding (train, test) splits as arrays of indices.\" [:br] \"\" [:br] \"        For integer/None inputs, :class:`KFold` is used.\" [:br] \"\" [:br] \"        Refer :ref:`User Guide <cross_validation>` for the various\" [:br] \"        cross-validation strategies that can be used here.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"            ``cv`` default value if None changed from 3-fold to 5-fold.\" [:br] \"\" [:br] \"    max_n_alphas : int, default=1000\" [:br] \"        The maximum number of points on the path used to compute the\" [:br] \"        residuals in the cross-validation\" [:br] \"\" [:br] \"    n_jobs : int or None, default=None\" [:br] \"        Number of CPUs to use during the cross validation.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    eps : float, optional\" [:br] \"        The machine-precision regularization in the computation of the\" [:br] \"        Cholesky diagonal factors. Increase this for very ill-conditioned\" [:br] \"        systems. By default, ``np.finfo(np.float).eps`` is used.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If ``True``, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array-like of shape (n_features,)\" [:br] \"        parameter vector (w in the formulation formula)\" [:br] \"\" [:br] \"    intercept_ : float\" [:br] \"        independent term in decision function\" [:br] \"\" [:br] \"    coef_path_ : array-like of shape (n_features, n_alphas)\" [:br] \"        the varying values of the coefficients along the path\" [:br] \"\" [:br] \"    alpha_ : float\" [:br] \"        the estimated regularization parameter alpha\" [:br] \"\" [:br] \"    alphas_ : array-like of shape (n_alphas,)\" [:br] \"        the different values of alpha along the path\" [:br] \"\" [:br] \"    cv_alphas_ : array-like of shape (n_cv_alphas,)\" [:br] \"        all the values of alpha along the path for the different folds\" [:br] \"\" [:br] \"    mse_path_ : array-like of shape (n_folds, n_cv_alphas)\" [:br] \"        the mean square error on left-out for each fold along the path\" [:br] \"        (alpha values given by ``cv_alphas``)\" [:br] \"\" [:br] \"    n_iter_ : array-like or int\" [:br] \"        the number of iterations run by Lars with the optimal alpha.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.linear_model import LarsCV\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\" [:br] \"    >>> reg = LarsCV(cv=5).fit(X, y)\" [:br] \"    >>> reg.score(X, y)\" [:br] \"    0.9996...\" [:br] \"    >>> reg.alpha_\" [:br] \"    0.0254...\" [:br] \"    >>> reg.predict(X[:1,])\" [:br] \"    array([154.0842...])\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    lars_path, LassoLars, LassoLarsCV\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/lasso\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [11 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|      :positive |     false |\\n|           :tol | 0.0001000 |\\n|      :max-iter |      1000 |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n|    :precompute |     false |\\n| :fit-intercept |      true |\\n|         :alpha |     1.000 |\\n|    :warm-start |     false |\\n|     :selection |    cyclic |\\n\"]]] [:span (\"Linear Model trained with L1 prior as regularizer (aka the Lasso)\" [:br] \"\" [:br] \"    The optimization objective for Lasso is::\" [:br] \"\" [:br] \"        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\" [:br] \"\" [:br] \"    Technically the Lasso model is optimizing the same objective function as\" [:br] \"    the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <lasso>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : float, default=1.0\" [:br] \"        Constant that multiplies the L1 term. Defaults to 1.0.\" [:br] \"        ``alpha = 0`` is equivalent to an ordinary least square, solved\" [:br] \"        by the :class:`LinearRegression` object. For numerical\" [:br] \"        reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\" [:br] \"        Given this, you should use the :class:`LinearRegression` object.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether to calculate the intercept for this model. If set\" [:br] \"        to False, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default=False\" [:br] \"        Whether to use a precomputed Gram matrix to speed up\" [:br] \"        calculations. If set to ``'auto'`` let us decide. The Gram\" [:br] \"        matrix can also be passed as argument. For sparse input\" [:br] \"        this option is always ``True`` to preserve sparsity.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If ``True``, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        The maximum number of iterations\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        The tolerance for the optimization: if the updates are\" [:br] \"        smaller than ``tol``, the optimization code checks the\" [:br] \"        dual gap for optimality and continues until it is smaller\" [:br] \"        than ``tol``.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to True, reuse the solution of the previous call to fit as\" [:br] \"        initialization, otherwise, just erase the previous solution.\" [:br] \"        See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    positive : bool, default=False\" [:br] \"        When set to ``True``, forces the coefficients to be positive.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        The seed of the pseudo random number generator that selects a random\" [:br] \"        feature to update. Used when ``selection`` == 'random'.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    selection : {'cyclic', 'random'}, default='cyclic'\" [:br] \"        If set to 'random', a random coefficient is updated every iteration\" [:br] \"        rather than looping over features sequentially by default. This\" [:br] \"        (setting to 'random') often leads to significantly faster convergence\" [:br] \"        especially when tol is higher than 1e-4.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\" [:br] \"        parameter vector (w in the cost function formula)\" [:br] \"\" [:br] \"    sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\" [:br] \"        ``sparse_coef_`` is a readonly property derived from ``coef_``\" [:br] \"\" [:br] \"    intercept_ : float or ndarray of shape (n_targets,)\" [:br] \"        independent term in decision function.\" [:br] \"\" [:br] \"    n_iter_ : int or list of int\" [:br] \"        number of iterations run by the coordinate descent solver to reach\" [:br] \"        the specified tolerance.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn import linear_model\" [:br] \"    >>> clf = linear_model.Lasso(alpha=0.1)\" [:br] \"    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\" [:br] \"    Lasso(alpha=0.1)\" [:br] \"    >>> print(clf.coef_)\" [:br] \"    [0.85 0.  ]\" [:br] \"    >>> print(clf.intercept_)\" [:br] \"    0.15...\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    lars_path\" [:br] \"    lasso_path\" [:br] \"    LassoLars\" [:br] \"    LassoCV\" [:br] \"    LassoLarsCV\" [:br] \"    sklearn.decomposition.sparse_encode\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The algorithm used to fit the model is coordinate descent.\" [:br] \"\" [:br] \"    To avoid unnecessary memory duplication the X argument of the fit method\" [:br] \"    should be directly passed as a Fortran-contiguous numpy array.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/lasso-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [15 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|      :positive |     false |\\n|           :tol | 0.0001000 |\\n|      :n-alphas |       100 |\\n|           :eps |  0.001000 |\\n|        :alphas |           |\\n|      :max-iter |      1000 |\\n|        :n-jobs |           |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n|    :precompute |      auto |\\n| :fit-intercept |      true |\\n|            :cv |           |\\n|     :selection |    cyclic |\\n|       :verbose |     false |\\n\"]]] [:span (\"Lasso linear model with iterative fitting along a regularization path.\" [:br] \"\" [:br] \"    See glossary entry for :term:`cross-validation estimator`.\" [:br] \"\" [:br] \"    The best model is selected by cross-validation.\" [:br] \"\" [:br] \"    The optimization objective for Lasso is::\" [:br] \"\" [:br] \"        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <lasso>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    eps : float, default=1e-3\" [:br] \"        Length of the path. ``eps=1e-3`` means that\" [:br] \"        ``alpha_min / alpha_max = 1e-3``.\" [:br] \"\" [:br] \"    n_alphas : int, default=100\" [:br] \"        Number of alphas along the regularization path\" [:br] \"\" [:br] \"    alphas : ndarray, default=None\" [:br] \"        List of alphas where to compute the models.\" [:br] \"        If ``None`` alphas are set automatically\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\" [:br] \"        Whether to use a precomputed Gram matrix to speed up\" [:br] \"        calculations. If set to ``'auto'`` let us decide. The Gram\" [:br] \"        matrix can also be passed as argument.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        The maximum number of iterations\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        The tolerance for the optimization: if the updates are\" [:br] \"        smaller than ``tol``, the optimization code checks the\" [:br] \"        dual gap for optimality and continues until it is smaller\" [:br] \"        than ``tol``.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If ``True``, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    cv : int, cross-validation generator or iterable, default=None\" [:br] \"        Determines the cross-validation splitting strategy.\" [:br] \"        Possible inputs for cv are:\" [:br] \"\" [:br] \"        - None, to use the default 5-fold cross-validation,\" [:br] \"        - int, to specify the number of folds.\" [:br] \"        - :term:`CV splitter`,\" [:br] \"        - An iterable yielding (train, test) splits as arrays of indices.\" [:br] \"\" [:br] \"        For int/None inputs, :class:`KFold` is used.\" [:br] \"\" [:br] \"        Refer :ref:`User Guide <cross_validation>` for the various\" [:br] \"        cross-validation strategies that can be used here.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"            ``cv`` default value if None changed from 3-fold to 5-fold.\" [:br] \"\" [:br] \"    verbose : bool or int, default=False\" [:br] \"        Amount of verbosity.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        Number of CPUs to use during the cross validation.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    positive : bool, default=False\" [:br] \"        If positive, restrict regression coefficients to be positive\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        The seed of the pseudo random number generator that selects a random\" [:br] \"        feature to update. Used when ``selection`` == 'random'.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    selection : {'cyclic', 'random'}, default='cyclic'\" [:br] \"        If set to 'random', a random coefficient is updated every iteration\" [:br] \"        rather than looping over features sequentially by default. This\" [:br] \"        (setting to 'random') often leads to significantly faster convergence\" [:br] \"        especially when tol is higher than 1e-4.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    alpha_ : float\" [:br] \"        The amount of penalization chosen by cross validation\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\" [:br] \"        parameter vector (w in the cost function formula)\" [:br] \"\" [:br] \"    intercept_ : float or ndarray of shape (n_targets,)\" [:br] \"        independent term in decision function.\" [:br] \"\" [:br] \"    mse_path_ : ndarray of shape (n_alphas, n_folds)\" [:br] \"        mean square error for the test set on each fold, varying alpha\" [:br] \"\" [:br] \"    alphas_ : ndarray of shape (n_alphas,)\" [:br] \"        The grid of alphas used for fitting\" [:br] \"\" [:br] \"    dual_gap_ : float or ndarray of shape (n_targets,)\" [:br] \"        The dual gap at the end of the optimization for the optimal alpha\" [:br] \"        (``alpha_``).\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        number of iterations run by the coordinate descent solver to reach\" [:br] \"        the specified tolerance for the optimal alpha.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.linear_model import LassoCV\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> X, y = make_regression(noise=4, random_state=0)\" [:br] \"    >>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\" [:br] \"    >>> reg.score(X, y)\" [:br] \"    0.9993...\" [:br] \"    >>> reg.predict(X[:1,])\" [:br] \"    array([-78.4951...])\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    For an example, see\" [:br] \"    :ref:`examples/linear_model/plot_lasso_model_selection.py\" [:br] \"    <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\" [:br] \"\" [:br] \"    To avoid unnecessary memory duplication the X argument of the fit method\" [:br] \"    should be directly passed as a Fortran-contiguous numpy array.\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    lars_path\" [:br] \"    lasso_path\" [:br] \"    LassoLars\" [:br] \"    Lasso\" [:br] \"    LassoLarsCV\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/lasso-lars\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [12 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |      true |\\n|      :positive |     false |\\n|      :fit-path |      true |\\n|           :eps | 2.220E-16 |\\n|      :max-iter |       500 |\\n|  :random-state |           |\\n|        :jitter |           |\\n|        :copy-x |      true |\\n|    :precompute |      auto |\\n| :fit-intercept |      true |\\n|         :alpha |     1.000 |\\n|       :verbose |     false |\\n\"]]] [:span (\"Lasso model fit with Least Angle Regression a.k.a. Lars\" [:br] \"\" [:br] \"    It is a Linear Model trained with an L1 prior as regularizer.\" [:br] \"\" [:br] \"    The optimization objective for Lasso is::\" [:br] \"\" [:br] \"    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <least_angle_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : float, default=1.0\" [:br] \"        Constant that multiplies the penalty term. Defaults to 1.0.\" [:br] \"        ``alpha = 0`` is equivalent to an ordinary least square, solved\" [:br] \"        by :class:`LinearRegression`. For numerical reasons, using\" [:br] \"        ``alpha = 0`` with the LassoLars object is not advised and you\" [:br] \"        should prefer the LinearRegression object.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    verbose : bool or int, default=False\" [:br] \"        Sets the verbosity amount\" [:br] \"\" [:br] \"    normalize : bool, default=True\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    precompute : bool, 'auto' or array-like, default='auto'\" [:br] \"        Whether to use a precomputed Gram matrix to speed up\" [:br] \"        calculations. If set to ``'auto'`` let us decide. The Gram\" [:br] \"        matrix can also be passed as argument.\" [:br] \"\" [:br] \"    max_iter : int, default=500\" [:br] \"        Maximum number of iterations to perform.\" [:br] \"\" [:br] \"    eps : float, optional\" [:br] \"        The machine-precision regularization in the computation of the\" [:br] \"        Cholesky diagonal factors. Increase this for very ill-conditioned\" [:br] \"        systems. Unlike the ``tol`` parameter in some iterative\" [:br] \"        optimization-based algorithms, this parameter does not control\" [:br] \"        the tolerance of the optimization.\" [:br] \"        By default, ``np.finfo(np.float).eps`` is used.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If True, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    fit_path : bool, default=True\" [:br] \"        If ``True`` the full path is stored in the ``coef_path_`` attribute.\" [:br] \"        If you compute the solution for a large problem or many targets,\" [:br] \"        setting ``fit_path`` to ``False`` will lead to a speedup, especially\" [:br] \"        with a small alpha.\" [:br] \"\" [:br] \"    positive : bool, default=False\" [:br] \"        Restrict coefficients to be >= 0. Be aware that you might want to\" [:br] \"        remove fit_intercept which is set True by default.\" [:br] \"        Under the positive restriction the model coefficients will not converge\" [:br] \"        to the ordinary-least-squares solution for small values of alpha.\" [:br] \"        Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\" [:br] \"        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\" [:br] \"        algorithm are typically in congruence with the solution of the\" [:br] \"        coordinate descent Lasso estimator.\" [:br] \"\" [:br] \"    jitter : float, default=None\" [:br] \"        Upper bound on a uniform noise parameter to be added to the\" [:br] \"        `y` values, to satisfy the model's assumption of\" [:br] \"        one-at-a-time computations. Might help with stability.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance or None (default)\" [:br] \"        Determines random number generation for jittering. Pass an int\" [:br] \"        for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    alphas_ : array-like of shape (n_alphas + 1,) | list of n_targets such             arrays\" [:br] \"        Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``max_iter``, ``n_features``, or the number of         nodes in the path with correlation greater than ``alpha``, whichever         is smaller.\" [:br] \"\" [:br] \"    active_ : list, length = n_alphas | list of n_targets such lists\" [:br] \"        Indices of active variables at the end of the path.\" [:br] \"\" [:br] \"    coef_path_ : array-like of shape (n_features, n_alphas + 1) or list\" [:br] \"        If a list is passed it's expected to be one of n_targets such arrays.\" [:br] \"        The varying values of the coefficients along the path. It is not\" [:br] \"        present if the ``fit_path`` parameter is ``False``.\" [:br] \"\" [:br] \"    coef_ : array-like of shape (n_features,) or (n_targets, n_features)\" [:br] \"        Parameter vector (w in the formulation formula).\" [:br] \"\" [:br] \"    intercept_ : float or array-like of shape (n_targets,)\" [:br] \"        Independent term in decision function.\" [:br] \"\" [:br] \"    n_iter_ : array-like or int.\" [:br] \"        The number of iterations taken by lars_path to find the\" [:br] \"        grid of alphas for each target.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn import linear_model\" [:br] \"    >>> reg = linear_model.LassoLars(alpha=0.01)\" [:br] \"    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\" [:br] \"    LassoLars(alpha=0.01)\" [:br] \"    >>> print(reg.coef_)\" [:br] \"    [ 0.         -0.963257...]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    lars_path\" [:br] \"    lasso_path\" [:br] \"    Lasso\" [:br] \"    LassoCV\" [:br] \"    LassoLarsCV\" [:br] \"    LassoLarsIC\" [:br] \"    sklearn.decomposition.sparse_encode\" [:br] \"\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/lasso-lars-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [11 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |      true |\\n|      :positive |     false |\\n|           :eps | 2.220E-16 |\\n|  :max-n-alphas |      1000 |\\n|      :max-iter |       500 |\\n|        :n-jobs |           |\\n|        :copy-x |      true |\\n|    :precompute |      auto |\\n| :fit-intercept |      true |\\n|            :cv |           |\\n|       :verbose |     false |\\n\"]]] [:span (\"Cross-validated Lasso, using the LARS algorithm.\" [:br] \"\" [:br] \"    See glossary entry for :term:`cross-validation estimator`.\" [:br] \"\" [:br] \"    The optimization objective for Lasso is::\" [:br] \"\" [:br] \"    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <least_angle_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    verbose : bool or int, default=False\" [:br] \"        Sets the verbosity amount\" [:br] \"\" [:br] \"    max_iter : int, default=500\" [:br] \"        Maximum number of iterations to perform.\" [:br] \"\" [:br] \"    normalize : bool, default=True\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    precompute : bool or 'auto' , default='auto'\" [:br] \"        Whether to use a precomputed Gram matrix to speed up\" [:br] \"        calculations. If set to ``'auto'`` let us decide. The Gram matrix\" [:br] \"        cannot be passed as argument since we will use only subsets of X.\" [:br] \"\" [:br] \"    cv : int, cross-validation generator or an iterable, default=None\" [:br] \"        Determines the cross-validation splitting strategy.\" [:br] \"        Possible inputs for cv are:\" [:br] \"\" [:br] \"        - None, to use the default 5-fold cross-validation,\" [:br] \"        - integer, to specify the number of folds.\" [:br] \"        - :term:`CV splitter`,\" [:br] \"        - An iterable yielding (train, test) splits as arrays of indices.\" [:br] \"\" [:br] \"        For integer/None inputs, :class:`KFold` is used.\" [:br] \"\" [:br] \"        Refer :ref:`User Guide <cross_validation>` for the various\" [:br] \"        cross-validation strategies that can be used here.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"            ``cv`` default value if None changed from 3-fold to 5-fold.\" [:br] \"\" [:br] \"    max_n_alphas : int, default=1000\" [:br] \"        The maximum number of points on the path used to compute the\" [:br] \"        residuals in the cross-validation\" [:br] \"\" [:br] \"    n_jobs : int or None, default=None\" [:br] \"        Number of CPUs to use during the cross validation.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    eps : float, optional\" [:br] \"        The machine-precision regularization in the computation of the\" [:br] \"        Cholesky diagonal factors. Increase this for very ill-conditioned\" [:br] \"        systems. By default, ``np.finfo(np.float).eps`` is used.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If True, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    positive : bool, default=False\" [:br] \"        Restrict coefficients to be >= 0. Be aware that you might want to\" [:br] \"        remove fit_intercept which is set True by default.\" [:br] \"        Under the positive restriction the model coefficients do not converge\" [:br] \"        to the ordinary-least-squares solution for small values of alpha.\" [:br] \"        Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\" [:br] \"        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\" [:br] \"        algorithm are typically in congruence with the solution of the\" [:br] \"        coordinate descent Lasso estimator.\" [:br] \"        As a consequence using LassoLarsCV only makes sense for problems where\" [:br] \"        a sparse solution is expected and/or reached.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array-like of shape (n_features,)\" [:br] \"        parameter vector (w in the formulation formula)\" [:br] \"\" [:br] \"    intercept_ : float\" [:br] \"        independent term in decision function.\" [:br] \"\" [:br] \"    coef_path_ : array-like of shape (n_features, n_alphas)\" [:br] \"        the varying values of the coefficients along the path\" [:br] \"\" [:br] \"    alpha_ : float\" [:br] \"        the estimated regularization parameter alpha\" [:br] \"\" [:br] \"    alphas_ : array-like of shape (n_alphas,)\" [:br] \"        the different values of alpha along the path\" [:br] \"\" [:br] \"    cv_alphas_ : array-like of shape (n_cv_alphas,)\" [:br] \"        all the values of alpha along the path for the different folds\" [:br] \"\" [:br] \"    mse_path_ : array-like of shape (n_folds, n_cv_alphas)\" [:br] \"        the mean square error on left-out for each fold along the path\" [:br] \"        (alpha values given by ``cv_alphas``)\" [:br] \"\" [:br] \"    n_iter_ : array-like or int\" [:br] \"        the number of iterations run by Lars with the optimal alpha.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.linear_model import LassoLarsCV\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> X, y = make_regression(noise=4.0, random_state=0)\" [:br] \"    >>> reg = LassoLarsCV(cv=5).fit(X, y)\" [:br] \"    >>> reg.score(X, y)\" [:br] \"    0.9992...\" [:br] \"    >>> reg.alpha_\" [:br] \"    0.0484...\" [:br] \"    >>> reg.predict(X[:1,])\" [:br] \"    array([-77.8723...])\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"\" [:br] \"    The object solves the same problem as the LassoCV object. However,\" [:br] \"    unlike the LassoCV, it find the relevant alphas values by itself.\" [:br] \"    In general, because of this property, it will be more stable.\" [:br] \"    However, it is more fragile to heavily multicollinear datasets.\" [:br] \"\" [:br] \"    It is more efficient than the LassoCV if only a small number of\" [:br] \"    features are selected compared to the total number, for instance if\" [:br] \"    there are very few samples compared to the number of features.\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    lars_path, LassoLars, LarsCV, LassoCV\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/lasso-lars-ic\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [9 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |      true |\\n|      :positive |     false |\\n|           :eps | 2.220E-16 |\\n|      :max-iter |       500 |\\n|        :copy-x |      true |\\n|    :precompute |      auto |\\n| :fit-intercept |      true |\\n|     :criterion |       aic |\\n|       :verbose |     false |\\n\"]]] [:span (\"Lasso model fit with Lars using BIC or AIC for model selection\" [:br] \"\" [:br] \"    The optimization objective for Lasso is::\" [:br] \"\" [:br] \"    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\" [:br] \"\" [:br] \"    AIC is the Akaike information criterion and BIC is the Bayes\" [:br] \"    Information criterion. Such criteria are useful to select the value\" [:br] \"    of the regularization parameter by making a trade-off between the\" [:br] \"    goodness of fit and the complexity of the model. A good model should\" [:br] \"    explain well the data while being simple.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <least_angle_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    criterion : {'bic' , 'aic'}, default='aic'\" [:br] \"        The type of criterion to use.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    verbose : bool or int, default=False\" [:br] \"        Sets the verbosity amount\" [:br] \"\" [:br] \"    normalize : bool, default=True\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    precompute : bool, 'auto' or array-like, default='auto'\" [:br] \"        Whether to use a precomputed Gram matrix to speed up\" [:br] \"        calculations. If set to ``'auto'`` let us decide. The Gram\" [:br] \"        matrix can also be passed as argument.\" [:br] \"\" [:br] \"    max_iter : int, default=500\" [:br] \"        Maximum number of iterations to perform. Can be used for\" [:br] \"        early stopping.\" [:br] \"\" [:br] \"    eps : float, optional\" [:br] \"        The machine-precision regularization in the computation of the\" [:br] \"        Cholesky diagonal factors. Increase this for very ill-conditioned\" [:br] \"        systems. Unlike the ``tol`` parameter in some iterative\" [:br] \"        optimization-based algorithms, this parameter does not control\" [:br] \"        the tolerance of the optimization.\" [:br] \"        By default, ``np.finfo(np.float).eps`` is used\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If True, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    positive : bool, default=False\" [:br] \"        Restrict coefficients to be >= 0. Be aware that you might want to\" [:br] \"        remove fit_intercept which is set True by default.\" [:br] \"        Under the positive restriction the model coefficients do not converge\" [:br] \"        to the ordinary-least-squares solution for small values of alpha.\" [:br] \"        Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\" [:br] \"        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\" [:br] \"        algorithm are typically in congruence with the solution of the\" [:br] \"        coordinate descent Lasso estimator.\" [:br] \"        As a consequence using LassoLarsIC only makes sense for problems where\" [:br] \"        a sparse solution is expected and/or reached.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array-like of shape (n_features,)\" [:br] \"        parameter vector (w in the formulation formula)\" [:br] \"\" [:br] \"    intercept_ : float\" [:br] \"        independent term in decision function.\" [:br] \"\" [:br] \"    alpha_ : float\" [:br] \"        the alpha parameter chosen by the information criterion\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        number of iterations run by lars_path to find the grid of\" [:br] \"        alphas.\" [:br] \"\" [:br] \"    criterion_ : array-like of shape (n_alphas,)\" [:br] \"        The value of the information criteria ('aic', 'bic') across all\" [:br] \"        alphas. The alpha which has the smallest information criterion is\" [:br] \"        chosen. This value is larger by a factor of ``n_samples`` compared to\" [:br] \"        Eqns. 2.15 and 2.16 in (Zou et al, 2007).\" [:br] \"\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn import linear_model\" [:br] \"    >>> reg = linear_model.LassoLarsIC(criterion='bic')\" [:br] \"    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\" [:br] \"    LassoLarsIC(criterion='bic')\" [:br] \"    >>> print(reg.coef_)\" [:br] \"    [ 0.  -1.11...]\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The estimation of the number of degrees of freedom is given by:\" [:br] \"\" [:br] \"    \\\"On the degrees of freedom of the lasso\\\"\" [:br] \"    Hui Zou, Trevor Hastie, and Robert Tibshirani\" [:br] \"    Ann. Statist. Volume 35, Number 5 (2007), 2173-2192.\" [:br] \"\" [:br] \"    https://en.wikipedia.org/wiki/Akaike_information_criterion\" [:br] \"    https://en.wikipedia.org/wiki/Bayesian_information_criterion\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    lars_path, LassoLars, LassoLarsCV\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/linear-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"276px\"}} [:p/markdown \"_unnamed [4 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|        :copy-x |     true |\\n| :fit-intercept |     true |\\n|        :n-jobs |          |\\n|     :normalize |    false |\\n\"]]] [:span (\"\" [:br] \"    Ordinary least squares Linear Regression.\" [:br] \"\" [:br] \"    LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\" [:br] \"    to minimize the residual sum of squares between the observed targets in\" [:br] \"    the dataset, and the targets predicted by the linear approximation.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether to calculate the intercept for this model. If set\" [:br] \"        to False, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on\" [:br] \"        an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If True, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of jobs to use for the computation. This will only provide\" [:br] \"        speedup for n_targets > 1 and sufficient large problems.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array of shape (n_features, ) or (n_targets, n_features)\" [:br] \"        Estimated coefficients for the linear regression problem.\" [:br] \"        If multiple targets are passed during the fit (y 2D), this\" [:br] \"        is a 2D array of shape (n_targets, n_features), while if only\" [:br] \"        one target is passed, this is a 1D array of length n_features.\" [:br] \"\" [:br] \"    rank_ : int\" [:br] \"        Rank of matrix `X`. Only available when `X` is dense.\" [:br] \"\" [:br] \"    singular_ : array of shape (min(X, y),)\" [:br] \"        Singular values of `X`. Only available when `X` is dense.\" [:br] \"\" [:br] \"    intercept_ : float or array of shape (n_targets,)\" [:br] \"        Independent term in the linear model. Set to 0.0 if\" [:br] \"        `fit_intercept = False`.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    sklearn.linear_model.Ridge : Ridge regression addresses some of the\" [:br] \"        problems of Ordinary Least Squares by imposing a penalty on the\" [:br] \"        size of the coefficients with l2 regularization.\" [:br] \"    sklearn.linear_model.Lasso : The Lasso is a linear model that estimates\" [:br] \"        sparse coefficients with l1 regularization.\" [:br] \"    sklearn.linear_model.ElasticNet : Elastic-Net is a linear regression\" [:br] \"        model trained with both l1 and l2 -norm regularization of the\" [:br] \"        coefficients.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    From the implementation point of view, this is just plain Ordinary\" [:br] \"    Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> from sklearn.linear_model import LinearRegression\" [:br] \"    >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\" [:br] \"    >>> # y = 1 * x_0 + 2 * x_1 + 3\" [:br] \"    >>> y = np.dot(X, np.array([1, 2])) + 3\" [:br] \"    >>> reg = LinearRegression().fit(X, y)\" [:br] \"    >>> reg.score(X, y)\" [:br] \"    1.0\" [:br] \"    >>> reg.coef_\" [:br] \"    array([1., 2.])\" [:br] \"    >>> reg.intercept_\" [:br] \"    3.0000...\" [:br] \"    >>> reg.predict(np.array([[3, 5]]))\" [:br] \"    array([16.])\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/linear-svr\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [10 2]:\\n\\n|              :name |            :default |\\n|--------------------|---------------------|\\n|               :tol |           0.0001000 |\\n| :intercept-scaling |               1.000 |\\n|                 :c |               1.000 |\\n|          :max-iter |                1000 |\\n|      :random-state |                     |\\n|              :dual |                true |\\n|     :fit-intercept |                true |\\n|              :loss | epsilon_insensitive |\\n|           :verbose |                   0 |\\n|           :epsilon |               0.000 |\\n\"]]] [:span (\"Linear Support Vector Regression.\" [:br] \"\" [:br] \"    Similar to SVR with parameter kernel='linear', but implemented in terms of\" [:br] \"    liblinear rather than libsvm, so it has more flexibility in the choice of\" [:br] \"    penalties and loss functions and should scale better to large numbers of\" [:br] \"    samples.\" [:br] \"\" [:br] \"    This class supports both dense and sparse input.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <svm_regression>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.16\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    epsilon : float, default=0.0\" [:br] \"        Epsilon parameter in the epsilon-insensitive loss function. Note\" [:br] \"        that the value of this parameter depends on the scale of the target\" [:br] \"        variable y. If unsure, set ``epsilon=0``.\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        Tolerance for stopping criteria.\" [:br] \"\" [:br] \"    C : float, default=1.0\" [:br] \"        Regularization parameter. The strength of the regularization is\" [:br] \"        inversely proportional to C. Must be strictly positive.\" [:br] \"\" [:br] \"    loss : {'epsilon_insensitive', 'squared_epsilon_insensitive'},             default='epsilon_insensitive'\" [:br] \"        Specifies the loss function. The epsilon-insensitive loss\" [:br] \"        (standard SVR) is the L1 loss, while the squared epsilon-insensitive\" [:br] \"        loss ('squared_epsilon_insensitive') is the L2 loss.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be already centered).\" [:br] \"\" [:br] \"    intercept_scaling : float, default=1.\" [:br] \"        When self.fit_intercept is True, instance vector x becomes\" [:br] \"        [x, self.intercept_scaling],\" [:br] \"        i.e. a \\\"synthetic\\\" feature with constant value equals to\" [:br] \"        intercept_scaling is appended to the instance vector.\" [:br] \"        The intercept becomes intercept_scaling * synthetic feature weight\" [:br] \"        Note! the synthetic feature weight is subject to l1/l2 regularization\" [:br] \"        as all other features.\" [:br] \"        To lessen the effect of regularization on synthetic feature weight\" [:br] \"        (and therefore on the intercept) intercept_scaling has to be increased.\" [:br] \"\" [:br] \"    dual : bool, default=True\" [:br] \"        Select the algorithm to either solve the dual or primal\" [:br] \"        optimization problem. Prefer dual=False when n_samples > n_features.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        Enable verbose output. Note that this setting takes advantage of a\" [:br] \"        per-process runtime setting in liblinear that, if enabled, may not work\" [:br] \"        properly in a multithreaded context.\" [:br] \"\" [:br] \"    random_state : int or RandomState instance, default=None\" [:br] \"        Controls the pseudo random number generation for shuffling the data.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        The maximum number of iterations to be run.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : ndarray of shape (n_features) if n_classes == 2             else (n_classes, n_features)\" [:br] \"        Weights assigned to the features (coefficients in the primal\" [:br] \"        problem). This is only available in the case of a linear kernel.\" [:br] \"\" [:br] \"        `coef_` is a readonly property derived from `raw_coef_` that\" [:br] \"        follows the internal memory layout of liblinear.\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape (1) if n_classes == 2 else (n_classes)\" [:br] \"        Constants in decision function.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        Maximum number of iterations run across all classes.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.svm import LinearSVR\" [:br] \"    >>> from sklearn.pipeline import make_pipeline\" [:br] \"    >>> from sklearn.preprocessing import StandardScaler\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> X, y = make_regression(n_features=4, random_state=0)\" [:br] \"    >>> regr = make_pipeline(StandardScaler(),\" [:br] \"    ...                      LinearSVR(random_state=0, tol=1e-5))\" [:br] \"    >>> regr.fit(X, y)\" [:br] \"    Pipeline(steps=[('standardscaler', StandardScaler()),\" [:br] \"                    ('linearsvr', LinearSVR(random_state=0, tol=1e-05))])\" [:br] \"\" [:br] \"    >>> print(regr.named_steps['linearsvr'].coef_)\" [:br] \"    [18.582... 27.023... 44.357... 64.522...]\" [:br] \"    >>> print(regr.named_steps['linearsvr'].intercept_)\" [:br] \"    [-4...]\" [:br] \"    >>> print(regr.predict([[0, 0, 0, 0]]))\" [:br] \"    [-2.384...]\" [:br] \"\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    LinearSVC\" [:br] \"        Implementation of Support Vector Machine classifier using the\" [:br] \"        same library as this class (liblinear).\" [:br] \"\" [:br] \"    SVR\" [:br] \"        Implementation of Support Vector Machine regression using libsvm:\" [:br] \"        the kernel can be non-linear but its SMO algorithm does not\" [:br] \"        scale to large number of samples as LinearSVC does.\" [:br] \"\" [:br] \"    sklearn.linear_model.SGDRegressor\" [:br] \"        SGDRegressor can optimize the same cost function as LinearSVR\" [:br] \"        by adjusting the penalty and loss parameters. In addition it requires\" [:br] \"        less memory, allows incremental (online) learning, and implements\" [:br] \"        various loss functions and regularization regimes.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/mlp-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [23 2]:\\n\\n|                :name |  :default |\\n|----------------------|-----------|\\n|    :n-iter-no-change |        10 |\\n|       :learning-rate |  constant |\\n|          :activation |      relu |\\n|  :hidden-layer-sizes |     [100] |\\n|                 :tol | 0.0001000 |\\n|              :beta-2 |    0.9990 |\\n|      :early-stopping |     false |\\n|  :nesterovs-momentum |      true |\\n|          :batch-size |      auto |\\n|              :solver |      adam |\\n|             :shuffle |      true |\\n|             :power-t |    0.5000 |\\n|             :max-fun |     15000 |\\n|              :beta-1 |    0.9000 |\\n|            :max-iter |       200 |\\n|        :random-state |           |\\n|            :momentum |    0.9000 |\\n|  :learning-rate-init |  0.001000 |\\n|               :alpha | 0.0001000 |\\n|          :warm-start |     false |\\n| :validation-fraction |    0.1000 |\\n|             :verbose |     false |\\n|             :epsilon | 1.000E-08 |\\n\"]]] [:span (\"Multi-layer Perceptron regressor.\" [:br] \"\" [:br] \"    This model optimizes the squared-loss using LBFGS or stochastic gradient\" [:br] \"    descent.\" [:br] \"\" [:br] \"    .. versionadded:: 0.18\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    hidden_layer_sizes : tuple, length = n_layers - 2, default=(100,)\" [:br] \"        The ith element represents the number of neurons in the ith\" [:br] \"        hidden layer.\" [:br] \"\" [:br] \"    activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\" [:br] \"        Activation function for the hidden layer.\" [:br] \"\" [:br] \"        - 'identity', no-op activation, useful to implement linear bottleneck,\" [:br] \"          returns f(x) = x\" [:br] \"\" [:br] \"        - 'logistic', the logistic sigmoid function,\" [:br] \"          returns f(x) = 1 / (1 + exp(-x)).\" [:br] \"\" [:br] \"        - 'tanh', the hyperbolic tan function,\" [:br] \"          returns f(x) = tanh(x).\" [:br] \"\" [:br] \"        - 'relu', the rectified linear unit function,\" [:br] \"          returns f(x) = max(0, x)\" [:br] \"\" [:br] \"    solver : {'lbfgs', 'sgd', 'adam'}, default='adam'\" [:br] \"        The solver for weight optimization.\" [:br] \"\" [:br] \"        - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\" [:br] \"\" [:br] \"        - 'sgd' refers to stochastic gradient descent.\" [:br] \"\" [:br] \"        - 'adam' refers to a stochastic gradient-based optimizer proposed by\" [:br] \"          Kingma, Diederik, and Jimmy Ba\" [:br] \"\" [:br] \"        Note: The default solver 'adam' works pretty well on relatively\" [:br] \"        large datasets (with thousands of training samples or more) in terms of\" [:br] \"        both training time and validation score.\" [:br] \"        For small datasets, however, 'lbfgs' can converge faster and perform\" [:br] \"        better.\" [:br] \"\" [:br] \"    alpha : float, default=0.0001\" [:br] \"        L2 penalty (regularization term) parameter.\" [:br] \"\" [:br] \"    batch_size : int, default='auto'\" [:br] \"        Size of minibatches for stochastic optimizers.\" [:br] \"        If the solver is 'lbfgs', the classifier will not use minibatch.\" [:br] \"        When set to \\\"auto\\\", `batch_size=min(200, n_samples)`\" [:br] \"\" [:br] \"    learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\" [:br] \"        Learning rate schedule for weight updates.\" [:br] \"\" [:br] \"        - 'constant' is a constant learning rate given by\" [:br] \"          'learning_rate_init'.\" [:br] \"\" [:br] \"        - 'invscaling' gradually decreases the learning rate ``learning_rate_``\" [:br] \"          at each time step 't' using an inverse scaling exponent of 'power_t'.\" [:br] \"          effective_learning_rate = learning_rate_init / pow(t, power_t)\" [:br] \"\" [:br] \"        - 'adaptive' keeps the learning rate constant to\" [:br] \"          'learning_rate_init' as long as training loss keeps decreasing.\" [:br] \"          Each time two consecutive epochs fail to decrease training loss by at\" [:br] \"          least tol, or fail to increase validation score by at least tol if\" [:br] \"          'early_stopping' is on, the current learning rate is divided by 5.\" [:br] \"\" [:br] \"        Only used when solver='sgd'.\" [:br] \"\" [:br] \"    learning_rate_init : double, default=0.001\" [:br] \"        The initial learning rate used. It controls the step-size\" [:br] \"        in updating the weights. Only used when solver='sgd' or 'adam'.\" [:br] \"\" [:br] \"    power_t : double, default=0.5\" [:br] \"        The exponent for inverse scaling learning rate.\" [:br] \"        It is used in updating effective learning rate when the learning_rate\" [:br] \"        is set to 'invscaling'. Only used when solver='sgd'.\" [:br] \"\" [:br] \"    max_iter : int, default=200\" [:br] \"        Maximum number of iterations. The solver iterates until convergence\" [:br] \"        (determined by 'tol') or this number of iterations. For stochastic\" [:br] \"        solvers ('sgd', 'adam'), note that this determines the number of epochs\" [:br] \"        (how many times each data point will be used), not the number of\" [:br] \"        gradient steps.\" [:br] \"\" [:br] \"    shuffle : bool, default=True\" [:br] \"        Whether to shuffle samples in each iteration. Only used when\" [:br] \"        solver='sgd' or 'adam'.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Determines random number generation for weights and bias\" [:br] \"        initialization, train-test split if early stopping is used, and batch\" [:br] \"        sampling when solver='sgd' or 'adam'.\" [:br] \"        Pass an int for reproducible results across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        Tolerance for the optimization. When the loss or score is not improving\" [:br] \"        by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\" [:br] \"        unless ``learning_rate`` is set to 'adaptive', convergence is\" [:br] \"        considered to be reached and training stops.\" [:br] \"\" [:br] \"    verbose : bool, default=False\" [:br] \"        Whether to print progress messages to stdout.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to True, reuse the solution of the previous\" [:br] \"        call to fit as initialization, otherwise, just erase the\" [:br] \"        previous solution. See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    momentum : float, default=0.9\" [:br] \"        Momentum for gradient descent update.  Should be between 0 and 1. Only\" [:br] \"        used when solver='sgd'.\" [:br] \"\" [:br] \"    nesterovs_momentum : boolean, default=True\" [:br] \"        Whether to use Nesterov's momentum. Only used when solver='sgd' and\" [:br] \"        momentum > 0.\" [:br] \"\" [:br] \"    early_stopping : bool, default=False\" [:br] \"        Whether to use early stopping to terminate training when validation\" [:br] \"        score is not improving. If set to true, it will automatically set\" [:br] \"        aside 10% of training data as validation and terminate training when\" [:br] \"        validation score is not improving by at least ``tol`` for\" [:br] \"        ``n_iter_no_change`` consecutive epochs.\" [:br] \"        Only effective when solver='sgd' or 'adam'\" [:br] \"\" [:br] \"    validation_fraction : float, default=0.1\" [:br] \"        The proportion of training data to set aside as validation set for\" [:br] \"        early stopping. Must be between 0 and 1.\" [:br] \"        Only used if early_stopping is True\" [:br] \"\" [:br] \"    beta_1 : float, default=0.9\" [:br] \"        Exponential decay rate for estimates of first moment vector in adam,\" [:br] \"        should be in [0, 1). Only used when solver='adam'\" [:br] \"\" [:br] \"    beta_2 : float, default=0.999\" [:br] \"        Exponential decay rate for estimates of second moment vector in adam,\" [:br] \"        should be in [0, 1). Only used when solver='adam'\" [:br] \"\" [:br] \"    epsilon : float, default=1e-8\" [:br] \"        Value for numerical stability in adam. Only used when solver='adam'\" [:br] \"\" [:br] \"    n_iter_no_change : int, default=10\" [:br] \"        Maximum number of epochs to not meet ``tol`` improvement.\" [:br] \"        Only effective when solver='sgd' or 'adam'\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    max_fun : int, default=15000\" [:br] \"        Only used when solver='lbfgs'. Maximum number of function calls.\" [:br] \"        The solver iterates until convergence (determined by 'tol'), number\" [:br] \"        of iterations reaches max_iter, or this number of function calls.\" [:br] \"        Note that number of function calls will be greater than or equal to\" [:br] \"        the number of iterations for the MLPRegressor.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    loss_ : float\" [:br] \"        The current loss computed with the loss function.\" [:br] \"\" [:br] \"    coefs_ : list, length n_layers - 1\" [:br] \"        The ith element in the list represents the weight matrix corresponding\" [:br] \"        to layer i.\" [:br] \"\" [:br] \"    intercepts_ : list, length n_layers - 1\" [:br] \"        The ith element in the list represents the bias vector corresponding to\" [:br] \"        layer i + 1.\" [:br] \"\" [:br] \"    n_iter_ : int,\" [:br] \"        The number of iterations the solver has ran.\" [:br] \"\" [:br] \"    n_layers_ : int\" [:br] \"        Number of layers.\" [:br] \"\" [:br] \"    n_outputs_ : int\" [:br] \"        Number of outputs.\" [:br] \"\" [:br] \"    out_activation_ : string\" [:br] \"        Name of the output activation function.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.neural_network import MLPRegressor\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> from sklearn.model_selection import train_test_split\" [:br] \"    >>> X, y = make_regression(n_samples=200, random_state=1)\" [:br] \"    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\" [:br] \"    ...                                                     random_state=1)\" [:br] \"    >>> regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\" [:br] \"    >>> regr.predict(X_test[:2])\" [:br] \"    array([-0.9..., -7.1...])\" [:br] \"    >>> regr.score(X_test, y_test)\" [:br] \"    0.4...\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    MLPRegressor trains iteratively since at each time step\" [:br] \"    the partial derivatives of the loss function with respect to the model\" [:br] \"    parameters are computed to update the parameters.\" [:br] \"\" [:br] \"    It can also have a regularization term added to the loss function\" [:br] \"    that shrinks model parameters to prevent overfitting.\" [:br] \"\" [:br] \"    This implementation works with data represented as dense and sparse numpy\" [:br] \"    arrays of floating point values.\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    Hinton, Geoffrey E.\" [:br] \"        \\\"Connectionist learning procedures.\\\" Artificial intelligence 40.1\" [:br] \"        (1989): 185-234.\" [:br] \"\" [:br] \"    Glorot, Xavier, and Yoshua Bengio. \\\"Understanding the difficulty of\" [:br] \"        training deep feedforward neural networks.\\\" International Conference\" [:br] \"        on Artificial Intelligence and Statistics. 2010.\" [:br] \"\" [:br] \"    He, Kaiming, et al. \\\"Delving deep into rectifiers: Surpassing human-level\" [:br] \"        performance on imagenet classification.\\\" arXiv preprint\" [:br] \"        arXiv:1502.01852 (2015).\" [:br] \"\" [:br] \"    Kingma, Diederik, and Jimmy Ba. \\\"Adam: A method for stochastic\" [:br] \"        optimization.\\\" arXiv preprint arXiv:1412.6980 (2014).\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/multi-task-elastic-net\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [10 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|           :tol | 0.0001000 |\\n|      :max-iter |      1000 |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n| :fit-intercept |      true |\\n|         :alpha |     1.000 |\\n|    :warm-start |     false |\\n|     :selection |    cyclic |\\n|     :l-1-ratio |    0.5000 |\\n\"]]] [:span (\"Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer\" [:br] \"\" [:br] \"    The optimization objective for MultiTaskElasticNet is::\" [:br] \"\" [:br] \"        (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\" [:br] \"        + alpha * l1_ratio * ||W||_21\" [:br] \"        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\" [:br] \"\" [:br] \"    Where::\" [:br] \"\" [:br] \"        ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)\" [:br] \"\" [:br] \"    i.e. the sum of norms of each row.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <multi_task_elastic_net>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : float, default=1.0\" [:br] \"        Constant that multiplies the L1/L2 term. Defaults to 1.0\" [:br] \"\" [:br] \"    l1_ratio : float, default=0.5\" [:br] \"        The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\" [:br] \"        For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\" [:br] \"        is an L2 penalty.\" [:br] \"        For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If ``True``, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        The maximum number of iterations\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        The tolerance for the optimization: if the updates are\" [:br] \"        smaller than ``tol``, the optimization code checks the\" [:br] \"        dual gap for optimality and continues until it is smaller\" [:br] \"        than ``tol``.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to ``True``, reuse the solution of the previous call to fit as\" [:br] \"        initialization, otherwise, just erase the previous solution.\" [:br] \"        See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        The seed of the pseudo random number generator that selects a random\" [:br] \"        feature to update. Used when ``selection`` == 'random'.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    selection : {'cyclic', 'random'}, default='cyclic'\" [:br] \"        If set to 'random', a random coefficient is updated every iteration\" [:br] \"        rather than looping over features sequentially by default. This\" [:br] \"        (setting to 'random') often leads to significantly faster convergence\" [:br] \"        especially when tol is higher than 1e-4.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    intercept_ : ndarray of shape (n_tasks,)\" [:br] \"        Independent term in decision function.\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (n_tasks, n_features)\" [:br] \"        Parameter vector (W in the cost function formula). If a 1D y is\" [:br] \"        passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.\" [:br] \"        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        number of iterations run by the coordinate descent solver to reach\" [:br] \"        the specified tolerance.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn import linear_model\" [:br] \"    >>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)\" [:br] \"    >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\" [:br] \"    MultiTaskElasticNet(alpha=0.1)\" [:br] \"    >>> print(clf.coef_)\" [:br] \"    [[0.45663524 0.45612256]\" [:br] \"     [0.45663524 0.45612256]]\" [:br] \"    >>> print(clf.intercept_)\" [:br] \"    [0.0872422 0.0872422]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    MultiTaskElasticNet : Multi-task L1/L2 ElasticNet with built-in\" [:br] \"        cross-validation.\" [:br] \"    ElasticNet\" [:br] \"    MultiTaskLasso\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The algorithm used to fit the model is coordinate descent.\" [:br] \"\" [:br] \"    To avoid unnecessary memory duplication the X and y arguments of the fit\" [:br] \"    method should be directly passed as Fortran-contiguous numpy arrays.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/multi-task-elastic-net-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [14 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|           :tol | 0.0001000 |\\n|      :n-alphas |       100 |\\n|           :eps |  0.001000 |\\n|        :alphas |           |\\n|      :max-iter |      1000 |\\n|        :n-jobs |           |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n| :fit-intercept |      true |\\n|            :cv |           |\\n|     :selection |    cyclic |\\n|     :l-1-ratio |    0.5000 |\\n|       :verbose |         0 |\\n\"]]] [:span (\"Multi-task L1/L2 ElasticNet with built-in cross-validation.\" [:br] \"\" [:br] \"    See glossary entry for :term:`cross-validation estimator`.\" [:br] \"\" [:br] \"    The optimization objective for MultiTaskElasticNet is::\" [:br] \"\" [:br] \"        (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\" [:br] \"        + alpha * l1_ratio * ||W||_21\" [:br] \"        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\" [:br] \"\" [:br] \"    Where::\" [:br] \"\" [:br] \"        ||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\" [:br] \"\" [:br] \"    i.e. the sum of norm of each row.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <multi_task_elastic_net>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.15\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    l1_ratio : float or list of float, default=0.5\" [:br] \"        The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\" [:br] \"        For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\" [:br] \"        is an L2 penalty.\" [:br] \"        For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\" [:br] \"        This parameter can be a list, in which case the different\" [:br] \"        values are tested by cross-validation and the one giving the best\" [:br] \"        prediction score is used. Note that a good choice of list of\" [:br] \"        values for l1_ratio is often to put more values close to 1\" [:br] \"        (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\" [:br] \"        .9, .95, .99, 1]``\" [:br] \"\" [:br] \"    eps : float, default=1e-3\" [:br] \"        Length of the path. ``eps=1e-3`` means that\" [:br] \"        ``alpha_min / alpha_max = 1e-3``.\" [:br] \"\" [:br] \"    n_alphas : int, default=100\" [:br] \"        Number of alphas along the regularization path\" [:br] \"\" [:br] \"    alphas : array-like, default=None\" [:br] \"        List of alphas where to compute the models.\" [:br] \"        If not provided, set automatically.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        The maximum number of iterations\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        The tolerance for the optimization: if the updates are\" [:br] \"        smaller than ``tol``, the optimization code checks the\" [:br] \"        dual gap for optimality and continues until it is smaller\" [:br] \"        than ``tol``.\" [:br] \"\" [:br] \"    cv : int, cross-validation generator or iterable, default=None\" [:br] \"        Determines the cross-validation splitting strategy.\" [:br] \"        Possible inputs for cv are:\" [:br] \"\" [:br] \"        - None, to use the default 5-fold cross-validation,\" [:br] \"        - int, to specify the number of folds.\" [:br] \"        - :term:`CV splitter`,\" [:br] \"        - An iterable yielding (train, test) splits as arrays of indices.\" [:br] \"\" [:br] \"        For int/None inputs, :class:`KFold` is used.\" [:br] \"\" [:br] \"        Refer :ref:`User Guide <cross_validation>` for the various\" [:br] \"        cross-validation strategies that can be used here.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"            ``cv`` default value if None changed from 3-fold to 5-fold.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If ``True``, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    verbose : bool or int, default=0\" [:br] \"        Amount of verbosity.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        Number of CPUs to use during the cross validation. Note that this is\" [:br] \"        used only if multiple values for l1_ratio are given.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        The seed of the pseudo random number generator that selects a random\" [:br] \"        feature to update. Used when ``selection`` == 'random'.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    selection : {'cyclic', 'random'}, default='cyclic'\" [:br] \"        If set to 'random', a random coefficient is updated every iteration\" [:br] \"        rather than looping over features sequentially by default. This\" [:br] \"        (setting to 'random') often leads to significantly faster convergence\" [:br] \"        especially when tol is higher than 1e-4.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    intercept_ : ndarray of shape (n_tasks,)\" [:br] \"        Independent term in decision function.\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (n_tasks, n_features)\" [:br] \"        Parameter vector (W in the cost function formula).\" [:br] \"        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\" [:br] \"\" [:br] \"    alpha_ : float\" [:br] \"        The amount of penalization chosen by cross validation\" [:br] \"\" [:br] \"    mse_path_ : ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)\" [:br] \"        mean square error for the test set on each fold, varying alpha\" [:br] \"\" [:br] \"    alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\" [:br] \"        The grid of alphas used for fitting, for each l1_ratio\" [:br] \"\" [:br] \"    l1_ratio_ : float\" [:br] \"        best l1_ratio obtained by cross-validation.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        number of iterations run by the coordinate descent solver to reach\" [:br] \"        the specified tolerance for the optimal alpha.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn import linear_model\" [:br] \"    >>> clf = linear_model.MultiTaskElasticNetCV(cv=3)\" [:br] \"    >>> clf.fit([[0,0], [1, 1], [2, 2]],\" [:br] \"    ...         [[0, 0], [1, 1], [2, 2]])\" [:br] \"    MultiTaskElasticNetCV(cv=3)\" [:br] \"    >>> print(clf.coef_)\" [:br] \"    [[0.52875032 0.46958558]\" [:br] \"     [0.52875032 0.46958558]]\" [:br] \"    >>> print(clf.intercept_)\" [:br] \"    [0.00166409 0.00166409]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    MultiTaskElasticNet\" [:br] \"    ElasticNetCV\" [:br] \"    MultiTaskLassoCV\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The algorithm used to fit the model is coordinate descent.\" [:br] \"\" [:br] \"    To avoid unnecessary memory duplication the X and y arguments of the fit\" [:br] \"    method should be directly passed as Fortran-contiguous numpy arrays.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/multi-task-lasso\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [9 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|           :tol | 0.0001000 |\\n|      :max-iter |      1000 |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n| :fit-intercept |      true |\\n|         :alpha |     1.000 |\\n|    :warm-start |     false |\\n|     :selection |    cyclic |\\n\"]]] [:span (\"Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\" [:br] \"\" [:br] \"    The optimization objective for Lasso is::\" [:br] \"\" [:br] \"        (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\" [:br] \"\" [:br] \"    Where::\" [:br] \"\" [:br] \"        ||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\" [:br] \"\" [:br] \"    i.e. the sum of norm of each row.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <multi_task_lasso>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : float, default=1.0\" [:br] \"        Constant that multiplies the L1/L2 term. Defaults to 1.0\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If ``True``, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        The maximum number of iterations\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        The tolerance for the optimization: if the updates are\" [:br] \"        smaller than ``tol``, the optimization code checks the\" [:br] \"        dual gap for optimality and continues until it is smaller\" [:br] \"        than ``tol``.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to ``True``, reuse the solution of the previous call to fit as\" [:br] \"        initialization, otherwise, just erase the previous solution.\" [:br] \"        See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        The seed of the pseudo random number generator that selects a random\" [:br] \"        feature to update. Used when ``selection`` == 'random'.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    selection : {'cyclic', 'random'}, default='cyclic'\" [:br] \"        If set to 'random', a random coefficient is updated every iteration\" [:br] \"        rather than looping over features sequentially by default. This\" [:br] \"        (setting to 'random') often leads to significantly faster convergence\" [:br] \"        especially when tol is higher than 1e-4\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : ndarray of shape (n_tasks, n_features)\" [:br] \"        Parameter vector (W in the cost function formula).\" [:br] \"        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape (n_tasks,)\" [:br] \"        independent term in decision function.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        number of iterations run by the coordinate descent solver to reach\" [:br] \"        the specified tolerance.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn import linear_model\" [:br] \"    >>> clf = linear_model.MultiTaskLasso(alpha=0.1)\" [:br] \"    >>> clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])\" [:br] \"    MultiTaskLasso(alpha=0.1)\" [:br] \"    >>> print(clf.coef_)\" [:br] \"    [[0.         0.60809415]\" [:br] \"    [0.         0.94592424]]\" [:br] \"    >>> print(clf.intercept_)\" [:br] \"    [-0.41888636 -0.87382323]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    MultiTaskLasso : Multi-task L1/L2 Lasso with built-in cross-validation\" [:br] \"    Lasso\" [:br] \"    MultiTaskElasticNet\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The algorithm used to fit the model is coordinate descent.\" [:br] \"\" [:br] \"    To avoid unnecessary memory duplication the X and y arguments of the fit\" [:br] \"    method should be directly passed as Fortran-contiguous numpy arrays.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/multi-task-lasso-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [13 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :normalize |     false |\\n|           :tol | 0.0001000 |\\n|      :n-alphas |       100 |\\n|           :eps |  0.001000 |\\n|        :alphas |           |\\n|      :max-iter |      1000 |\\n|        :n-jobs |           |\\n|  :random-state |           |\\n|        :copy-x |      true |\\n| :fit-intercept |      true |\\n|            :cv |           |\\n|     :selection |    cyclic |\\n|       :verbose |     false |\\n\"]]] [:span (\"Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\" [:br] \"\" [:br] \"    See glossary entry for :term:`cross-validation estimator`.\" [:br] \"\" [:br] \"    The optimization objective for MultiTaskLasso is::\" [:br] \"\" [:br] \"        (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21\" [:br] \"\" [:br] \"    Where::\" [:br] \"\" [:br] \"        ||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\" [:br] \"\" [:br] \"    i.e. the sum of norm of each row.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <multi_task_lasso>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.15\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    eps : float, default=1e-3\" [:br] \"        Length of the path. ``eps=1e-3`` means that\" [:br] \"        ``alpha_min / alpha_max = 1e-3``.\" [:br] \"\" [:br] \"    n_alphas : int, default=100\" [:br] \"        Number of alphas along the regularization path\" [:br] \"\" [:br] \"    alphas : array-like, default=None\" [:br] \"        List of alphas where to compute the models.\" [:br] \"        If not provided, set automatically.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        The maximum number of iterations.\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        The tolerance for the optimization: if the updates are\" [:br] \"        smaller than ``tol``, the optimization code checks the\" [:br] \"        dual gap for optimality and continues until it is smaller\" [:br] \"        than ``tol``.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If ``True``, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    cv : int, cross-validation generator or iterable, default=None\" [:br] \"        Determines the cross-validation splitting strategy.\" [:br] \"        Possible inputs for cv are:\" [:br] \"\" [:br] \"        - None, to use the default 5-fold cross-validation,\" [:br] \"        - int, to specify the number of folds.\" [:br] \"        - :term:`CV splitter`,\" [:br] \"        - An iterable yielding (train, test) splits as arrays of indices.\" [:br] \"\" [:br] \"        For int/None inputs, :class:`KFold` is used.\" [:br] \"\" [:br] \"        Refer :ref:`User Guide <cross_validation>` for the various\" [:br] \"        cross-validation strategies that can be used here.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"            ``cv`` default value if None changed from 3-fold to 5-fold.\" [:br] \"\" [:br] \"    verbose : bool or int, default=False\" [:br] \"        Amount of verbosity.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        Number of CPUs to use during the cross validation. Note that this is\" [:br] \"        used only if multiple values for l1_ratio are given.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        The seed of the pseudo random number generator that selects a random\" [:br] \"        feature to update. Used when ``selection`` == 'random'.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    selection : {'cyclic', 'random'}, default='cyclic'\" [:br] \"        If set to 'random', a random coefficient is updated every iteration\" [:br] \"        rather than looping over features sequentially by default. This\" [:br] \"        (setting to 'random') often leads to significantly faster convergence\" [:br] \"        especially when tol is higher than 1e-4.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    intercept_ : ndarray of shape (n_tasks,)\" [:br] \"        Independent term in decision function.\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (n_tasks, n_features)\" [:br] \"        Parameter vector (W in the cost function formula).\" [:br] \"        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\" [:br] \"\" [:br] \"    alpha_ : float\" [:br] \"        The amount of penalization chosen by cross validation\" [:br] \"\" [:br] \"    mse_path_ : ndarray of shape (n_alphas, n_folds)\" [:br] \"        mean square error for the test set on each fold, varying alpha\" [:br] \"\" [:br] \"    alphas_ : ndarray of shape (n_alphas,)\" [:br] \"        The grid of alphas used for fitting.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        number of iterations run by the coordinate descent solver to reach\" [:br] \"        the specified tolerance for the optimal alpha.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.linear_model import MultiTaskLassoCV\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> from sklearn.metrics import r2_score\" [:br] \"    >>> X, y = make_regression(n_targets=2, noise=4, random_state=0)\" [:br] \"    >>> reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\" [:br] \"    >>> r2_score(y, reg.predict(X))\" [:br] \"    0.9994...\" [:br] \"    >>> reg.alpha_\" [:br] \"    0.5713...\" [:br] \"    >>> reg.predict(X[:1,])\" [:br] \"    array([[153.7971...,  94.9015...]])\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    MultiTaskElasticNet\" [:br] \"    ElasticNetCV\" [:br] \"    MultiTaskElasticNetCV\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The algorithm used to fit the model is coordinate descent.\" [:br] \"\" [:br] \"    To avoid unnecessary memory duplication the X and y arguments of the fit\" [:br] \"    method should be directly passed as Fortran-contiguous numpy arrays.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/nu-svr\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [11 2]:\\n\\n|       :name | :default |\\n|-------------|----------|\\n|     :kernel |      rbf |\\n|      :gamma |    scale |\\n|     :degree |        3 |\\n|        :tol | 0.001000 |\\n|         :nu |   0.5000 |\\n|  :shrinking |     true |\\n|          :c |    1.000 |\\n|   :max-iter |       -1 |\\n|     :coef-0 |    0.000 |\\n| :cache-size |      200 |\\n|    :verbose |    false |\\n\"]]] [:span (\"Nu Support Vector Regression.\" [:br] \"\" [:br] \"    Similar to NuSVC, for regression, uses a parameter nu to control\" [:br] \"    the number of support vectors. However, unlike NuSVC, where nu\" [:br] \"    replaces C, here nu replaces the parameter epsilon of epsilon-SVR.\" [:br] \"\" [:br] \"    The implementation is based on libsvm.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <svm_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    nu : float, default=0.5\" [:br] \"        An upper bound on the fraction of training errors and a lower bound of\" [:br] \"        the fraction of support vectors. Should be in the interval (0, 1].  By\" [:br] \"        default 0.5 will be taken.\" [:br] \"\" [:br] \"    C : float, default=1.0\" [:br] \"        Penalty parameter C of the error term.\" [:br] \"\" [:br] \"    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\" [:br] \"         Specifies the kernel type to be used in the algorithm.\" [:br] \"         It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\" [:br] \"         a callable.\" [:br] \"         If none is given, 'rbf' will be used. If a callable is given it is\" [:br] \"         used to precompute the kernel matrix.\" [:br] \"\" [:br] \"    degree : int, default=3\" [:br] \"        Degree of the polynomial kernel function ('poly').\" [:br] \"        Ignored by all other kernels.\" [:br] \"\" [:br] \"    gamma : {'scale', 'auto'} or float, default='scale'\" [:br] \"        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\" [:br] \"\" [:br] \"        - if ``gamma='scale'`` (default) is passed then it uses\" [:br] \"          1 / (n_features * X.var()) as value of gamma,\" [:br] \"        - if 'auto', uses 1 / n_features.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"           The default value of ``gamma`` changed from 'auto' to 'scale'.\" [:br] \"\" [:br] \"    coef0 : float, default=0.0\" [:br] \"        Independent term in kernel function.\" [:br] \"        It is only significant in 'poly' and 'sigmoid'.\" [:br] \"\" [:br] \"    shrinking : bool, default=True\" [:br] \"        Whether to use the shrinking heuristic.\" [:br] \"        See the :ref:`User Guide <shrinking_svm>`.\" [:br] \"\" [:br] \"    tol : float, default=1e-3\" [:br] \"        Tolerance for stopping criterion.\" [:br] \"\" [:br] \"    cache_size : float, default=200\" [:br] \"        Specify the size of the kernel cache (in MB).\" [:br] \"\" [:br] \"    verbose : bool, default=False\" [:br] \"        Enable verbose output. Note that this setting takes advantage of a\" [:br] \"        per-process runtime setting in libsvm that, if enabled, may not work\" [:br] \"        properly in a multithreaded context.\" [:br] \"\" [:br] \"    max_iter : int, default=-1\" [:br] \"        Hard limit on iterations within solver, or -1 for no limit.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    support_ : ndarray of shape (n_SV,)\" [:br] \"        Indices of support vectors.\" [:br] \"\" [:br] \"    support_vectors_ : ndarray of shape (n_SV, n_features)\" [:br] \"        Support vectors.\" [:br] \"\" [:br] \"    dual_coef_ : ndarray of shape (1, n_SV)\" [:br] \"        Coefficients of the support vector in the decision function.\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (1, n_features)\" [:br] \"        Weights assigned to the features (coefficients in the primal\" [:br] \"        problem). This is only available in the case of a linear kernel.\" [:br] \"\" [:br] \"        `coef_` is readonly property derived from `dual_coef_` and\" [:br] \"        `support_vectors_`.\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape (1,)\" [:br] \"        Constants in decision function.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.svm import NuSVR\" [:br] \"    >>> from sklearn.pipeline import make_pipeline\" [:br] \"    >>> from sklearn.preprocessing import StandardScaler\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> n_samples, n_features = 10, 5\" [:br] \"    >>> np.random.seed(0)\" [:br] \"    >>> y = np.random.randn(n_samples)\" [:br] \"    >>> X = np.random.randn(n_samples, n_features)\" [:br] \"    >>> regr = make_pipeline(StandardScaler(), NuSVR(C=1.0, nu=0.1))\" [:br] \"    >>> regr.fit(X, y)\" [:br] \"    Pipeline(steps=[('standardscaler', StandardScaler()),\" [:br] \"                    ('nusvr', NuSVR(nu=0.1))])\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    NuSVC\" [:br] \"        Support Vector Machine for classification implemented with libsvm\" [:br] \"        with a parameter to control the number of support vectors.\" [:br] \"\" [:br] \"    SVR\" [:br] \"        epsilon Support Vector Machine for regression implemented with libsvm.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    **References:**\" [:br] \"    `LIBSVM: A Library for Support Vector Machines\" [:br] \"    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`__\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/orthogonal-matching-pursuit\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"322px\"}} [:p/markdown \"_unnamed [5 2]:\\n\\n|            :name | :default |\\n|------------------|----------|\\n|   :fit-intercept |     true |\\n| :n-nonzero-coefs |          |\\n|       :normalize |     true |\\n|      :precompute |     auto |\\n|             :tol |          |\\n\"]]] [:span (\"Orthogonal Matching Pursuit model (OMP)\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <omp>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    n_nonzero_coefs : int, optional\" [:br] \"        Desired number of non-zero entries in the solution. If None (by\" [:br] \"        default) this value is set to 10% of n_features.\" [:br] \"\" [:br] \"    tol : float, optional\" [:br] \"        Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\" [:br] \"\" [:br] \"    fit_intercept : boolean, optional\" [:br] \"        whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : boolean, optional, default True\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    precompute : {True, False, 'auto'}, default 'auto'\" [:br] \"        Whether to use a precomputed Gram and Xy matrix to speed up\" [:br] \"        calculations. Improves performance when :term:`n_targets` or\" [:br] \"        :term:`n_samples` is very large. Note that if you already have such\" [:br] \"        matrices, you can pass them directly to the fit method.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array, shape (n_features,) or (n_targets, n_features)\" [:br] \"        parameter vector (w in the formula)\" [:br] \"\" [:br] \"    intercept_ : float or array, shape (n_targets,)\" [:br] \"        independent term in decision function.\" [:br] \"\" [:br] \"    n_iter_ : int or array-like\" [:br] \"        Number of active features across every target.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.linear_model import OrthogonalMatchingPursuit\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> X, y = make_regression(noise=4, random_state=0)\" [:br] \"    >>> reg = OrthogonalMatchingPursuit().fit(X, y)\" [:br] \"    >>> reg.score(X, y)\" [:br] \"    0.9991...\" [:br] \"    >>> reg.predict(X[:1,])\" [:br] \"    array([-78.3854...])\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\" [:br] \"    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\" [:br] \"    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\" [:br] \"    (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\" [:br] \"\" [:br] \"    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\" [:br] \"    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\" [:br] \"    Matching Pursuit Technical Report - CS Technion, April 2008.\" [:br] \"    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    orthogonal_mp\" [:br] \"    orthogonal_mp_gram\" [:br] \"    lars_path\" [:br] \"    Lars\" [:br] \"    LassoLars\" [:br] \"    decomposition.sparse_encode\" [:br] \"    OrthogonalMatchingPursuitCV\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/orthogonal-matching-pursuit-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [7 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|          :copy |     true |\\n|            :cv |          |\\n| :fit-intercept |     true |\\n|      :max-iter |          |\\n|        :n-jobs |          |\\n|     :normalize |     true |\\n|       :verbose |    false |\\n\"]]] [:span (\"Cross-validated Orthogonal Matching Pursuit model (OMP).\" [:br] \"\" [:br] \"    See glossary entry for :term:`cross-validation estimator`.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <omp>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    copy : bool, optional\" [:br] \"        Whether the design matrix X must be copied by the algorithm. A false\" [:br] \"        value is only helpful if X is already Fortran-ordered, otherwise a\" [:br] \"        copy is made anyway.\" [:br] \"\" [:br] \"    fit_intercept : boolean, optional\" [:br] \"        whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : boolean, optional, default True\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    max_iter : integer, optional\" [:br] \"        Maximum numbers of iterations to perform, therefore maximum features\" [:br] \"        to include. 10% of ``n_features`` but at least 5 if available.\" [:br] \"\" [:br] \"    cv : int, cross-validation generator or an iterable, optional\" [:br] \"        Determines the cross-validation splitting strategy.\" [:br] \"        Possible inputs for cv are:\" [:br] \"\" [:br] \"        - None, to use the default 5-fold cross-validation,\" [:br] \"        - integer, to specify the number of folds.\" [:br] \"        - :term:`CV splitter`,\" [:br] \"        - An iterable yielding (train, test) splits as arrays of indices.\" [:br] \"\" [:br] \"        For integer/None inputs, :class:`KFold` is used.\" [:br] \"\" [:br] \"        Refer :ref:`User Guide <cross_validation>` for the various\" [:br] \"        cross-validation strategies that can be used here.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"            ``cv`` default value if None changed from 3-fold to 5-fold.\" [:br] \"\" [:br] \"    n_jobs : int or None, optional (default=None)\" [:br] \"        Number of CPUs to use during the cross validation.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    verbose : boolean or integer, optional\" [:br] \"        Sets the verbosity amount\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    intercept_ : float or array, shape (n_targets,)\" [:br] \"        Independent term in decision function.\" [:br] \"\" [:br] \"    coef_ : array, shape (n_features,) or (n_targets, n_features)\" [:br] \"        Parameter vector (w in the problem formulation).\" [:br] \"\" [:br] \"    n_nonzero_coefs_ : int\" [:br] \"        Estimated number of non-zero coefficients giving the best mean squared\" [:br] \"        error over the cross-validation folds.\" [:br] \"\" [:br] \"    n_iter_ : int or array-like\" [:br] \"        Number of active features across every target for the model refit with\" [:br] \"        the best hyperparameters got by cross-validating across all folds.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.linear_model import OrthogonalMatchingPursuitCV\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> X, y = make_regression(n_features=100, n_informative=10,\" [:br] \"    ...                        noise=4, random_state=0)\" [:br] \"    >>> reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)\" [:br] \"    >>> reg.score(X, y)\" [:br] \"    0.9991...\" [:br] \"    >>> reg.n_nonzero_coefs_\" [:br] \"    10\" [:br] \"    >>> reg.predict(X[:1,])\" [:br] \"    array([-78.3854...])\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    orthogonal_mp\" [:br] \"    orthogonal_mp_gram\" [:br] \"    lars_path\" [:br] \"    Lars\" [:br] \"    LassoLars\" [:br] \"    OrthogonalMatchingPursuit\" [:br] \"    LarsCV\" [:br] \"    LassoLarsCV\" [:br] \"    decomposition.sparse_encode\" [:br] \"\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/passive-aggressive-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [14 2]:\\n\\n|                :name |            :default |\\n|----------------------|---------------------|\\n|    :n-iter-no-change |                   5 |\\n|             :average |               false |\\n|                 :tol |            0.001000 |\\n|      :early-stopping |               false |\\n|             :shuffle |                true |\\n|                   :c |               1.000 |\\n|            :max-iter |                1000 |\\n|        :random-state |                     |\\n|       :fit-intercept |                true |\\n|          :warm-start |               false |\\n| :validation-fraction |              0.1000 |\\n|                :loss | epsilon_insensitive |\\n|             :verbose |                   0 |\\n|             :epsilon |              0.1000 |\\n\"]]] [:span (\"Passive Aggressive Regressor\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <passive_aggressive>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"\" [:br] \"    C : float\" [:br] \"        Maximum step size (regularization). Defaults to 1.0.\" [:br] \"\" [:br] \"    fit_intercept : bool\" [:br] \"        Whether the intercept should be estimated or not. If False, the\" [:br] \"        data is assumed to be already centered. Defaults to True.\" [:br] \"\" [:br] \"    max_iter : int, optional (default=1000)\" [:br] \"        The maximum number of passes over the training data (aka epochs).\" [:br] \"        It only impacts the behavior in the ``fit`` method, and not the\" [:br] \"        :meth:`partial_fit` method.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    tol : float or None, optional (default=1e-3)\" [:br] \"        The stopping criterion. If it is not None, the iterations will stop\" [:br] \"        when (loss > previous_loss - tol).\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    early_stopping : bool, default=False\" [:br] \"        Whether to use early stopping to terminate training when validation.\" [:br] \"        score is not improving. If set to True, it will automatically set aside\" [:br] \"        a fraction of training data as validation and terminate\" [:br] \"        training when validation score is not improving by at least tol for\" [:br] \"        n_iter_no_change consecutive epochs.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    validation_fraction : float, default=0.1\" [:br] \"        The proportion of training data to set aside as validation set for\" [:br] \"        early stopping. Must be between 0 and 1.\" [:br] \"        Only used if early_stopping is True.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    n_iter_no_change : int, default=5\" [:br] \"        Number of iterations with no improvement to wait before early stopping.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"\" [:br] \"    shuffle : bool, default=True\" [:br] \"        Whether or not the training data should be shuffled after each epoch.\" [:br] \"\" [:br] \"    verbose : integer, optional\" [:br] \"        The verbosity level\" [:br] \"\" [:br] \"    loss : string, optional\" [:br] \"        The loss function to be used:\" [:br] \"        epsilon_insensitive: equivalent to PA-I in the reference paper.\" [:br] \"        squared_epsilon_insensitive: equivalent to PA-II in the reference\" [:br] \"        paper.\" [:br] \"\" [:br] \"    epsilon : float\" [:br] \"        If the difference between the current prediction and the correct label\" [:br] \"        is below this threshold, the model is not updated.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Used to shuffle the training data, when ``shuffle`` is set to\" [:br] \"        ``True``. Pass an int for reproducible output across multiple\" [:br] \"        function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    warm_start : bool, optional\" [:br] \"        When set to True, reuse the solution of the previous call to fit as\" [:br] \"        initialization, otherwise, just erase the previous solution.\" [:br] \"        See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"        Repeatedly calling fit or partial_fit when warm_start is True can\" [:br] \"        result in a different solution than when calling fit a single time\" [:br] \"        because of the way the data is shuffled.\" [:br] \"\" [:br] \"    average : bool or int, optional\" [:br] \"        When set to True, computes the averaged SGD weights and stores the\" [:br] \"        result in the ``coef_`` attribute. If set to an int greater than 1,\" [:br] \"        averaging will begin once the total number of samples seen reaches\" [:br] \"        average. So average=10 will begin averaging after seeing 10 samples.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"           parameter *average* to use weights averaging in SGD\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\" [:br] \"        Weights assigned to the features.\" [:br] \"\" [:br] \"    intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\" [:br] \"        Constants in decision function.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        The actual number of iterations to reach the stopping criterion.\" [:br] \"\" [:br] \"    t_ : int\" [:br] \"        Number of weight updates performed during training.\" [:br] \"        Same as ``(n_iter_ * n_samples)``.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.linear_model import PassiveAggressiveRegressor\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"\" [:br] \"    >>> X, y = make_regression(n_features=4, random_state=0)\" [:br] \"    >>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\" [:br] \"    ... tol=1e-3)\" [:br] \"    >>> regr.fit(X, y)\" [:br] \"    PassiveAggressiveRegressor(max_iter=100, random_state=0)\" [:br] \"    >>> print(regr.coef_)\" [:br] \"    [20.48736655 34.18818427 67.59122734 87.94731329]\" [:br] \"    >>> print(regr.intercept_)\" [:br] \"    [-0.02306214]\" [:br] \"    >>> print(regr.predict([[0, 0, 0, 0]]))\" [:br] \"    [-0.02306214]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"\" [:br] \"    SGDRegressor\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    Online Passive-Aggressive Algorithms\" [:br] \"    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\" [:br] \"    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\" [:br] \"\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/pls-canonical\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"368px\"}} [:p/markdown \"_unnamed [6 2]:\\n\\n|         :name |  :default |\\n|---------------|-----------|\\n|    :algorithm |    nipals |\\n|         :copy |      true |\\n|     :max-iter |       500 |\\n| :n-components |         2 |\\n|        :scale |      true |\\n|          :tol | 1.000E-06 |\\n\"]]] [:span (\" PLSCanonical implements the 2 blocks canonical PLS of the original Wold\" [:br] \"    algorithm [Tenenhaus 1998] p.204, referred as PLS-C2A in [Wegelin 2000].\" [:br] \"\" [:br] \"    This class inherits from PLS with mode=\\\"A\\\" and deflation_mode=\\\"canonical\\\",\" [:br] \"    norm_y_weights=True and algorithm=\\\"nipals\\\", but svd should provide similar\" [:br] \"    results up to numerical errors.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <cross_decomposition>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.8\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    n_components : int, (default 2).\" [:br] \"        Number of components to keep\" [:br] \"\" [:br] \"    scale : boolean, (default True)\" [:br] \"        Option to scale data\" [:br] \"\" [:br] \"    algorithm : string, \\\"nipals\\\" or \\\"svd\\\"\" [:br] \"        The algorithm used to estimate the weights. It will be called\" [:br] \"        n_components times, i.e. once for each iteration of the outer loop.\" [:br] \"\" [:br] \"    max_iter : an integer, (default 500)\" [:br] \"        the maximum number of iterations of the NIPALS inner loop (used\" [:br] \"        only if algorithm=\\\"nipals\\\")\" [:br] \"\" [:br] \"    tol : non-negative real, default 1e-06\" [:br] \"        the tolerance used in the iterative algorithm\" [:br] \"\" [:br] \"    copy : boolean, default True\" [:br] \"        Whether the deflation should be done on a copy. Let the default\" [:br] \"        value to True unless you don't care about side effect\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    x_weights_ : array, shape = [p, n_components]\" [:br] \"        X block weights vectors.\" [:br] \"\" [:br] \"    y_weights_ : array, shape = [q, n_components]\" [:br] \"        Y block weights vectors.\" [:br] \"\" [:br] \"    x_loadings_ : array, shape = [p, n_components]\" [:br] \"        X block loadings vectors.\" [:br] \"\" [:br] \"    y_loadings_ : array, shape = [q, n_components]\" [:br] \"        Y block loadings vectors.\" [:br] \"\" [:br] \"    x_scores_ : array, shape = [n_samples, n_components]\" [:br] \"        X scores.\" [:br] \"\" [:br] \"    y_scores_ : array, shape = [n_samples, n_components]\" [:br] \"        Y scores.\" [:br] \"\" [:br] \"    x_rotations_ : array, shape = [p, n_components]\" [:br] \"        X block to latents rotations.\" [:br] \"\" [:br] \"    y_rotations_ : array, shape = [q, n_components]\" [:br] \"        Y block to latents rotations.\" [:br] \"\" [:br] \"    coef_ : array of shape (p, q)\" [:br] \"        The coefficients of the linear model: ``Y = X coef_ + Err``\" [:br] \"\" [:br] \"    n_iter_ : array-like\" [:br] \"        Number of iterations of the NIPALS inner loop for each\" [:br] \"        component. Not useful if the algorithm provided is \\\"svd\\\".\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    Matrices::\" [:br] \"\" [:br] \"        T: x_scores_\" [:br] \"        U: y_scores_\" [:br] \"        W: x_weights_\" [:br] \"        C: y_weights_\" [:br] \"        P: x_loadings_\" [:br] \"        Q: y_loadings__\" [:br] \"\" [:br] \"    Are computed such that::\" [:br] \"\" [:br] \"        X = T P.T + Err and Y = U Q.T + Err\" [:br] \"        T[:, k] = Xk W[:, k] for k in range(n_components)\" [:br] \"        U[:, k] = Yk C[:, k] for k in range(n_components)\" [:br] \"        x_rotations_ = W (P.T W)^(-1)\" [:br] \"        y_rotations_ = C (Q.T C)^(-1)\" [:br] \"\" [:br] \"    where Xk and Yk are residual matrices at iteration k.\" [:br] \"\" [:br] \"    `Slides explaining PLS\" [:br] \"    <http://www.eigenvector.com/Docs/Wise_pls_properties.pdf>`_\" [:br] \"\" [:br] \"    For each component k, find weights u, v that optimize::\" [:br] \"\" [:br] \"        max corr(Xk u, Yk v) * std(Xk u) std(Yk u), such that ``|u| = |v| = 1``\" [:br] \"\" [:br] \"    Note that it maximizes both the correlations between the scores and the\" [:br] \"    intra-block variances.\" [:br] \"\" [:br] \"    The residual matrix of X (Xk+1) block is obtained by the deflation on the\" [:br] \"    current X score: x_score.\" [:br] \"\" [:br] \"    The residual matrix of Y (Yk+1) block is obtained by deflation on the\" [:br] \"    current Y score. This performs a canonical symmetric version of the PLS\" [:br] \"    regression. But slightly different than the CCA. This is mostly used\" [:br] \"    for modeling.\" [:br] \"\" [:br] \"    This implementation provides the same results that the \\\"plspm\\\" package\" [:br] \"    provided in the R language (R-project), using the function plsca(X, Y).\" [:br] \"    Results are equal or collinear with the function\" [:br] \"    ``pls(..., mode = \\\"canonical\\\")`` of the \\\"mixOmics\\\" package. The difference\" [:br] \"    relies in the fact that mixOmics implementation does not exactly implement\" [:br] \"    the Wold algorithm since it does not normalize y_weights to one.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.cross_decomposition import PLSCanonical\" [:br] \"    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\" [:br] \"    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\" [:br] \"    >>> plsca = PLSCanonical(n_components=2)\" [:br] \"    >>> plsca.fit(X, Y)\" [:br] \"    PLSCanonical()\" [:br] \"    >>> X_c, Y_c = plsca.transform(X, Y)\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"\" [:br] \"    Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with\" [:br] \"    emphasis on the two-block case. Technical Report 371, Department of\" [:br] \"    Statistics, University of Washington, Seattle, 2000.\" [:br] \"\" [:br] \"    Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\" [:br] \"    Editions Technic.\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    CCA\" [:br] \"    PLSSVD\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/pls-regression\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"322px\"}} [:p/markdown \"_unnamed [5 2]:\\n\\n|         :name |  :default |\\n|---------------|-----------|\\n|         :copy |      true |\\n|     :max-iter |       500 |\\n| :n-components |         2 |\\n|        :scale |      true |\\n|          :tol | 1.000E-06 |\\n\"]]] [:span (\"PLS regression\" [:br] \"\" [:br] \"    PLSRegression implements the PLS 2 blocks regression known as PLS2 or PLS1\" [:br] \"    in case of one dimensional response.\" [:br] \"    This class inherits from _PLS with mode=\\\"A\\\", deflation_mode=\\\"regression\\\",\" [:br] \"    norm_y_weights=False and algorithm=\\\"nipals\\\".\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <cross_decomposition>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.8\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    n_components : int, (default 2)\" [:br] \"        Number of components to keep.\" [:br] \"\" [:br] \"    scale : boolean, (default True)\" [:br] \"        whether to scale the data\" [:br] \"\" [:br] \"    max_iter : an integer, (default 500)\" [:br] \"        the maximum number of iterations of the NIPALS inner loop (used\" [:br] \"        only if algorithm=\\\"nipals\\\")\" [:br] \"\" [:br] \"    tol : non-negative real\" [:br] \"        Tolerance used in the iterative algorithm default 1e-06.\" [:br] \"\" [:br] \"    copy : boolean, default True\" [:br] \"        Whether the deflation should be done on a copy. Let the default\" [:br] \"        value to True unless you don't care about side effect\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    x_weights_ : array, [p, n_components]\" [:br] \"        X block weights vectors.\" [:br] \"\" [:br] \"    y_weights_ : array, [q, n_components]\" [:br] \"        Y block weights vectors.\" [:br] \"\" [:br] \"    x_loadings_ : array, [p, n_components]\" [:br] \"        X block loadings vectors.\" [:br] \"\" [:br] \"    y_loadings_ : array, [q, n_components]\" [:br] \"        Y block loadings vectors.\" [:br] \"\" [:br] \"    x_scores_ : array, [n_samples, n_components]\" [:br] \"        X scores.\" [:br] \"\" [:br] \"    y_scores_ : array, [n_samples, n_components]\" [:br] \"        Y scores.\" [:br] \"\" [:br] \"    x_rotations_ : array, [p, n_components]\" [:br] \"        X block to latents rotations.\" [:br] \"\" [:br] \"    y_rotations_ : array, [q, n_components]\" [:br] \"        Y block to latents rotations.\" [:br] \"\" [:br] \"    coef_ : array, [p, q]\" [:br] \"        The coefficients of the linear model: ``Y = X coef_ + Err``\" [:br] \"\" [:br] \"    n_iter_ : array-like\" [:br] \"        Number of iterations of the NIPALS inner loop for each\" [:br] \"        component.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    Matrices::\" [:br] \"\" [:br] \"        T: x_scores_\" [:br] \"        U: y_scores_\" [:br] \"        W: x_weights_\" [:br] \"        C: y_weights_\" [:br] \"        P: x_loadings_\" [:br] \"        Q: y_loadings_\" [:br] \"\" [:br] \"    Are computed such that::\" [:br] \"\" [:br] \"        X = T P.T + Err and Y = U Q.T + Err\" [:br] \"        T[:, k] = Xk W[:, k] for k in range(n_components)\" [:br] \"        U[:, k] = Yk C[:, k] for k in range(n_components)\" [:br] \"        x_rotations_ = W (P.T W)^(-1)\" [:br] \"        y_rotations_ = C (Q.T C)^(-1)\" [:br] \"\" [:br] \"    where Xk and Yk are residual matrices at iteration k.\" [:br] \"\" [:br] \"    `Slides explaining\" [:br] \"    PLS <http://www.eigenvector.com/Docs/Wise_pls_properties.pdf>`_\" [:br] \"\" [:br] \"\" [:br] \"    For each component k, find weights u, v that optimizes:\" [:br] \"    ``max corr(Xk u, Yk v) * std(Xk u) std(Yk u)``, such that ``|u| = 1``\" [:br] \"\" [:br] \"    Note that it maximizes both the correlations between the scores and the\" [:br] \"    intra-block variances.\" [:br] \"\" [:br] \"    The residual matrix of X (Xk+1) block is obtained by the deflation on\" [:br] \"    the current X score: x_score.\" [:br] \"\" [:br] \"    The residual matrix of Y (Yk+1) block is obtained by deflation on the\" [:br] \"    current X score. This performs the PLS regression known as PLS2. This\" [:br] \"    mode is prediction oriented.\" [:br] \"\" [:br] \"    This implementation provides the same results that 3 PLS packages\" [:br] \"    provided in the R language (R-project):\" [:br] \"\" [:br] \"        - \\\"mixOmics\\\" with function pls(X, Y, mode = \\\"regression\\\")\" [:br] \"        - \\\"plspm \\\" with function plsreg2(X, Y)\" [:br] \"        - \\\"pls\\\" with function oscorespls.fit(X, Y)\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.cross_decomposition import PLSRegression\" [:br] \"    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\" [:br] \"    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\" [:br] \"    >>> pls2 = PLSRegression(n_components=2)\" [:br] \"    >>> pls2.fit(X, Y)\" [:br] \"    PLSRegression()\" [:br] \"    >>> Y_pred = pls2.predict(X)\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"\" [:br] \"    Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with\" [:br] \"    emphasis on the two-block case. Technical Report 371, Department of\" [:br] \"    Statistics, University of Washington, Seattle, 2000.\" [:br] \"\" [:br] \"    In french but still a reference:\" [:br] \"    Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\" [:br] \"    Editions Technic.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/poisson-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"368px\"}} [:p/markdown \"_unnamed [6 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|         :alpha |     1.000 |\\n| :fit-intercept |      true |\\n|      :max-iter |       100 |\\n|           :tol | 0.0001000 |\\n|       :verbose |         0 |\\n|    :warm-start |     false |\\n\"]]] [:span (\"Generalized Linear Model with a Poisson distribution.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <Generalized_linear_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : float, default=1\" [:br] \"        Constant that multiplies the penalty term and thus determines the\" [:br] \"        regularization strength. ``alpha = 0`` is equivalent to unpenalized\" [:br] \"        GLMs. In this case, the design matrix `X` must have full column rank\" [:br] \"        (no collinearities).\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Specifies if a constant (a.k.a. bias or intercept) should be\" [:br] \"        added to the linear predictor (X @ coef + intercept).\" [:br] \"\" [:br] \"    max_iter : int, default=100\" [:br] \"        The maximal number of iterations for the solver.\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        Stopping criterion. For the lbfgs solver,\" [:br] \"        the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\" [:br] \"        where ``g_j`` is the j-th component of the gradient (derivative) of\" [:br] \"        the objective function.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        If set to ``True``, reuse the solution of the previous call to ``fit``\" [:br] \"        as initialization for ``coef_`` and ``intercept_`` .\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        For the lbfgs solver set verbose to any positive number for verbosity.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array of shape (n_features,)\" [:br] \"        Estimated coefficients for the linear predictor (`X @ coef_ +\" [:br] \"        intercept_`) in the GLM.\" [:br] \"\" [:br] \"    intercept_ : float\" [:br] \"        Intercept (a.k.a. bias) added to linear predictor.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        Actual number of iterations used in the solver.\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/radius-neighbors-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|     :algorithm |      auto |\\n|     :leaf-size |        30 |\\n|        :metric | minkowski |\\n| :metric-params |           |\\n|        :n-jobs |           |\\n|             :p |         2 |\\n|        :radius |     1.000 |\\n|       :weights |   uniform |\\n\"]]] [:span (\"Regression based on neighbors within a fixed radius.\" [:br] \"\" [:br] \"    The target is predicted by local interpolation of the targets\" [:br] \"    associated of the nearest neighbors in the training set.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <regression>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.9\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    radius : float, default=1.0\" [:br] \"        Range of parameter space to use by default for :meth:`radius_neighbors`\" [:br] \"        queries.\" [:br] \"\" [:br] \"    weights : {'uniform', 'distance'} or callable, default='uniform'\" [:br] \"        weight function used in prediction.  Possible values:\" [:br] \"\" [:br] \"        - 'uniform' : uniform weights.  All points in each neighborhood\" [:br] \"          are weighted equally.\" [:br] \"        - 'distance' : weight points by the inverse of their distance.\" [:br] \"          in this case, closer neighbors of a query point will have a\" [:br] \"          greater influence than neighbors which are further away.\" [:br] \"        - [callable] : a user-defined function which accepts an\" [:br] \"          array of distances, and returns an array of the same shape\" [:br] \"          containing the weights.\" [:br] \"\" [:br] \"        Uniform weights are used by default.\" [:br] \"\" [:br] \"    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\" [:br] \"        Algorithm used to compute the nearest neighbors:\" [:br] \"\" [:br] \"        - 'ball_tree' will use :class:`BallTree`\" [:br] \"        - 'kd_tree' will use :class:`KDTree`\" [:br] \"        - 'brute' will use a brute-force search.\" [:br] \"        - 'auto' will attempt to decide the most appropriate algorithm\" [:br] \"          based on the values passed to :meth:`fit` method.\" [:br] \"\" [:br] \"        Note: fitting on sparse input will override the setting of\" [:br] \"        this parameter, using brute force.\" [:br] \"\" [:br] \"    leaf_size : int, default=30\" [:br] \"        Leaf size passed to BallTree or KDTree.  This can affect the\" [:br] \"        speed of the construction and query, as well as the memory\" [:br] \"        required to store the tree.  The optimal value depends on the\" [:br] \"        nature of the problem.\" [:br] \"\" [:br] \"    p : int, default=2\" [:br] \"        Power parameter for the Minkowski metric. When p = 1, this is\" [:br] \"        equivalent to using manhattan_distance (l1), and euclidean_distance\" [:br] \"        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\" [:br] \"\" [:br] \"    metric : str or callable, default='minkowski'\" [:br] \"        the distance metric to use for the tree.  The default metric is\" [:br] \"        minkowski, and with p=2 is equivalent to the standard Euclidean\" [:br] \"        metric. See the documentation of :class:`DistanceMetric` for a\" [:br] \"        list of available metrics.\" [:br] \"        If metric is \\\"precomputed\\\", X is assumed to be a distance matrix and\" [:br] \"        must be square during fit. X may be a :term:`sparse graph`,\" [:br] \"        in which case only \\\"nonzero\\\" elements may be considered neighbors.\" [:br] \"\" [:br] \"    metric_params : dict, default=None\" [:br] \"        Additional keyword arguments for the metric function.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of parallel jobs to run for neighbors search.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    effective_metric_ : str or callable\" [:br] \"        The distance metric to use. It will be same as the `metric` parameter\" [:br] \"        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\" [:br] \"        'minkowski' and `p` parameter set to 2.\" [:br] \"\" [:br] \"    effective_metric_params_ : dict\" [:br] \"        Additional keyword arguments for the metric function. For most metrics\" [:br] \"        will be same with `metric_params` parameter, but may also contain the\" [:br] \"        `p` parameter value if the `effective_metric_` attribute is set to\" [:br] \"        'minkowski'.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> X = [[0], [1], [2], [3]]\" [:br] \"    >>> y = [0, 0, 1, 1]\" [:br] \"    >>> from sklearn.neighbors import RadiusNeighborsRegressor\" [:br] \"    >>> neigh = RadiusNeighborsRegressor(radius=1.0)\" [:br] \"    >>> neigh.fit(X, y)\" [:br] \"    RadiusNeighborsRegressor(...)\" [:br] \"    >>> print(neigh.predict([[1.5]]))\" [:br] \"    [0.5]\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    NearestNeighbors\" [:br] \"    KNeighborsRegressor\" [:br] \"    KNeighborsClassifier\" [:br] \"    RadiusNeighborsClassifier\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\" [:br] \"    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\" [:br] \"\" [:br] \"    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/random-forest-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [18 2]:\\n\\n|                     :name | :default |\\n|---------------------------|----------|\\n| :min-weight-fraction-leaf |    0.000 |\\n|           :max-leaf-nodes |          |\\n|    :min-impurity-decrease |    0.000 |\\n|        :min-samples-split |    2.000 |\\n|                :bootstrap |     true |\\n|                :ccp-alpha |    0.000 |\\n|                   :n-jobs |          |\\n|             :random-state |          |\\n|                :oob-score |    false |\\n|         :min-samples-leaf |        1 |\\n|             :max-features |     auto |\\n|       :min-impurity-split |          |\\n|               :warm-start |    false |\\n|                :max-depth |          |\\n|             :n-estimators |      100 |\\n|              :max-samples |          |\\n|                :criterion |      mse |\\n|                  :verbose |        0 |\\n\"]]] [:span (\"\" [:br] \"    A random forest regressor.\" [:br] \"\" [:br] \"    A random forest is a meta estimator that fits a number of classifying\" [:br] \"    decision trees on various sub-samples of the dataset and uses averaging\" [:br] \"    to improve the predictive accuracy and control over-fitting.\" [:br] \"    The sub-sample size is controlled with the `max_samples` parameter if\" [:br] \"    `bootstrap=True` (default), otherwise the whole dataset is used to build\" [:br] \"    each tree.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <forest>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    n_estimators : int, default=100\" [:br] \"        The number of trees in the forest.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"           The default value of ``n_estimators`` changed from 10 to 100\" [:br] \"           in 0.22.\" [:br] \"\" [:br] \"    criterion : {\\\"mse\\\", \\\"mae\\\"}, default=\\\"mse\\\"\" [:br] \"        The function to measure the quality of a split. Supported criteria\" [:br] \"        are \\\"mse\\\" for the mean squared error, which is equal to variance\" [:br] \"        reduction as feature selection criterion, and \\\"mae\\\" for the mean\" [:br] \"        absolute error.\" [:br] \"\" [:br] \"        .. versionadded:: 0.18\" [:br] \"           Mean Absolute Error (MAE) criterion.\" [:br] \"\" [:br] \"    max_depth : int, default=None\" [:br] \"        The maximum depth of the tree. If None, then nodes are expanded until\" [:br] \"        all leaves are pure or until all leaves contain less than\" [:br] \"        min_samples_split samples.\" [:br] \"\" [:br] \"    min_samples_split : int or float, default=2\" [:br] \"        The minimum number of samples required to split an internal node:\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_split` as the minimum number.\" [:br] \"        - If float, then `min_samples_split` is a fraction and\" [:br] \"          `ceil(min_samples_split * n_samples)` are the minimum\" [:br] \"          number of samples for each split.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_samples_leaf : int or float, default=1\" [:br] \"        The minimum number of samples required to be at a leaf node.\" [:br] \"        A split point at any depth will only be considered if it leaves at\" [:br] \"        least ``min_samples_leaf`` training samples in each of the left and\" [:br] \"        right branches.  This may have the effect of smoothing the model,\" [:br] \"        especially in regression.\" [:br] \"\" [:br] \"        - If int, then consider `min_samples_leaf` as the minimum number.\" [:br] \"        - If float, then `min_samples_leaf` is a fraction and\" [:br] \"          `ceil(min_samples_leaf * n_samples)` are the minimum\" [:br] \"          number of samples for each node.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.18\" [:br] \"           Added float values for fractions.\" [:br] \"\" [:br] \"    min_weight_fraction_leaf : float, default=0.0\" [:br] \"        The minimum weighted fraction of the sum total of weights (of all\" [:br] \"        the input samples) required to be at a leaf node. Samples have\" [:br] \"        equal weight when sample_weight is not provided.\" [:br] \"\" [:br] \"    max_features : {\\\"auto\\\", \\\"sqrt\\\", \\\"log2\\\"}, int or float, default=\\\"auto\\\"\" [:br] \"        The number of features to consider when looking for the best split:\" [:br] \"\" [:br] \"        - If int, then consider `max_features` features at each split.\" [:br] \"        - If float, then `max_features` is a fraction and\" [:br] \"          `int(max_features * n_features)` features are considered at each\" [:br] \"          split.\" [:br] \"        - If \\\"auto\\\", then `max_features=n_features`.\" [:br] \"        - If \\\"sqrt\\\", then `max_features=sqrt(n_features)`.\" [:br] \"        - If \\\"log2\\\", then `max_features=log2(n_features)`.\" [:br] \"        - If None, then `max_features=n_features`.\" [:br] \"\" [:br] \"        Note: the search for a split does not stop until at least one\" [:br] \"        valid partition of the node samples is found, even if it requires to\" [:br] \"        effectively inspect more than ``max_features`` features.\" [:br] \"\" [:br] \"    max_leaf_nodes : int, default=None\" [:br] \"        Grow trees with ``max_leaf_nodes`` in best-first fashion.\" [:br] \"        Best nodes are defined as relative reduction in impurity.\" [:br] \"        If None then unlimited number of leaf nodes.\" [:br] \"\" [:br] \"    min_impurity_decrease : float, default=0.0\" [:br] \"        A node will be split if this split induces a decrease of the impurity\" [:br] \"        greater than or equal to this value.\" [:br] \"\" [:br] \"        The weighted impurity decrease equation is the following::\" [:br] \"\" [:br] \"            N_t / N * (impurity - N_t_R / N_t * right_impurity\" [:br] \"                                - N_t_L / N_t * left_impurity)\" [:br] \"\" [:br] \"        where ``N`` is the total number of samples, ``N_t`` is the number of\" [:br] \"        samples at the current node, ``N_t_L`` is the number of samples in the\" [:br] \"        left child, and ``N_t_R`` is the number of samples in the right child.\" [:br] \"\" [:br] \"        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\" [:br] \"        if ``sample_weight`` is passed.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    min_impurity_split : float, default=None\" [:br] \"        Threshold for early stopping in tree growth. A node will split\" [:br] \"        if its impurity is above the threshold, otherwise it is a leaf.\" [:br] \"\" [:br] \"        .. deprecated:: 0.19\" [:br] \"           ``min_impurity_split`` has been deprecated in favor of\" [:br] \"           ``min_impurity_decrease`` in 0.19. The default value of\" [:br] \"           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\" [:br] \"           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\" [:br] \"\" [:br] \"    bootstrap : bool, default=True\" [:br] \"        Whether bootstrap samples are used when building trees. If False, the\" [:br] \"        whole dataset is used to build each tree.\" [:br] \"\" [:br] \"    oob_score : bool, default=False\" [:br] \"        whether to use out-of-bag samples to estimate\" [:br] \"        the R^2 on unseen data.\" [:br] \"\" [:br] \"    n_jobs : int, default=None\" [:br] \"        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\" [:br] \"        :meth:`decision_path` and :meth:`apply` are all parallelized over the\" [:br] \"        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\" [:br] \"        context. ``-1`` means using all processors. See :term:`Glossary\" [:br] \"        <n_jobs>` for more details.\" [:br] \"\" [:br] \"    random_state : int or RandomState, default=None\" [:br] \"        Controls both the randomness of the bootstrapping of the samples used\" [:br] \"        when building trees (if ``bootstrap=True``) and the sampling of the\" [:br] \"        features to consider when looking for the best split at each node\" [:br] \"        (if ``max_features < n_features``).\" [:br] \"        See :term:`Glossary <random_state>` for details.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        Controls the verbosity when fitting and predicting.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to ``True``, reuse the solution of the previous call to fit\" [:br] \"        and add more estimators to the ensemble, otherwise, just fit a whole\" [:br] \"        new forest. See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"    ccp_alpha : non-negative float, default=0.0\" [:br] \"        Complexity parameter used for Minimal Cost-Complexity Pruning. The\" [:br] \"        subtree with the largest cost complexity that is smaller than\" [:br] \"        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\" [:br] \"        :ref:`minimal_cost_complexity_pruning` for details.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    max_samples : int or float, default=None\" [:br] \"        If bootstrap is True, the number of samples to draw from X\" [:br] \"        to train each base estimator.\" [:br] \"\" [:br] \"        - If None (default), then draw `X.shape[0]` samples.\" [:br] \"        - If int, then draw `max_samples` samples.\" [:br] \"        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\" [:br] \"          `max_samples` should be in the interval `(0, 1)`.\" [:br] \"\" [:br] \"        .. versionadded:: 0.22\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    base_estimator_ : DecisionTreeRegressor\" [:br] \"        The child estimator template used to create the collection of fitted\" [:br] \"        sub-estimators.\" [:br] \"\" [:br] \"    estimators_ : list of DecisionTreeRegressor\" [:br] \"        The collection of fitted sub-estimators.\" [:br] \"\" [:br] \"    feature_importances_ : ndarray of shape (n_features,)\" [:br] \"        The impurity-based feature importances.\" [:br] \"        The higher, the more important the feature.\" [:br] \"        The importance of a feature is computed as the (normalized)\" [:br] \"        total reduction of the criterion brought by that feature.  It is also\" [:br] \"        known as the Gini importance.\" [:br] \"\" [:br] \"        Warning: impurity-based feature importances can be misleading for\" [:br] \"        high cardinality features (many unique values). See\" [:br] \"        :func:`sklearn.inspection.permutation_importance` as an alternative.\" [:br] \"\" [:br] \"    n_features_ : int\" [:br] \"        The number of features when ``fit`` is performed.\" [:br] \"\" [:br] \"    n_outputs_ : int\" [:br] \"        The number of outputs when ``fit`` is performed.\" [:br] \"\" [:br] \"    oob_score_ : float\" [:br] \"        Score of the training dataset obtained using an out-of-bag estimate.\" [:br] \"        This attribute exists only when ``oob_score`` is True.\" [:br] \"\" [:br] \"    oob_prediction_ : ndarray of shape (n_samples,)\" [:br] \"        Prediction computed with out-of-bag estimate on the training set.\" [:br] \"        This attribute exists only when ``oob_score`` is True.\" [:br] \"\" [:br] \"    See Also\" [:br] \"    --------\" [:br] \"    DecisionTreeRegressor, ExtraTreesRegressor\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    The default values for the parameters controlling the size of the trees\" [:br] \"    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\" [:br] \"    unpruned trees which can potentially be very large on some data sets. To\" [:br] \"    reduce memory consumption, the complexity and size of the trees should be\" [:br] \"    controlled by setting those parameter values.\" [:br] \"\" [:br] \"    The features are always randomly permuted at each split. Therefore,\" [:br] \"    the best found split may vary, even with the same training data,\" [:br] \"    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\" [:br] \"    of the criterion is identical for several splits enumerated during the\" [:br] \"    search of the best split. To obtain a deterministic behaviour during\" [:br] \"    fitting, ``random_state`` has to be fixed.\" [:br] \"\" [:br] \"    The default value ``max_features=\\\"auto\\\"`` uses ``n_features``\" [:br] \"    rather than ``n_features / 3``. The latter was originally suggested in\" [:br] \"    [1], whereas the former was more recently justified empirically in [2].\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    .. [1] L. Breiman, \\\"Random Forests\\\", Machine Learning, 45(1), 5-32, 2001.\" [:br] \"\" [:br] \"    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \\\"Extremely randomized\" [:br] \"           trees\\\", Machine Learning, 63(1), 3-42, 2006.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.ensemble import RandomForestRegressor\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> X, y = make_regression(n_features=4, n_informative=2,\" [:br] \"    ...                        random_state=0, shuffle=False)\" [:br] \"    >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\" [:br] \"    >>> regr.fit(X, y)\" [:br] \"    RandomForestRegressor(...)\" [:br] \"    >>> print(regr.predict([[0, 0, 0, 0]]))\" [:br] \"    [-8.32987858]\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/ransac-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [12 2]:\\n\\n|               :name |      :default |\\n|---------------------|---------------|\\n|      :is-data-valid |               |\\n|          :max-skips |      INFINITY |\\n|       :random-state |               |\\n|        :min-samples |               |\\n|   :stop-probability |        0.9900 |\\n|     :stop-n-inliers |      INFINITY |\\n|     :base-estimator |               |\\n|         :max-trials |         100.0 |\\n| :residual-threshold |               |\\n|     :is-model-valid |               |\\n|               :loss | absolute_loss |\\n|         :stop-score |      INFINITY |\\n\"]]] [:span (\"RANSAC (RANdom SAmple Consensus) algorithm.\" [:br] \"\" [:br] \"    RANSAC is an iterative algorithm for the robust estimation of parameters\" [:br] \"    from a subset of inliers from the complete data set.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <ransac_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    base_estimator : object, optional\" [:br] \"        Base estimator object which implements the following methods:\" [:br] \"\" [:br] \"         * `fit(X, y)`: Fit model to given training data and target values.\" [:br] \"         * `score(X, y)`: Returns the mean accuracy on the given test data,\" [:br] \"           which is used for the stop criterion defined by `stop_score`.\" [:br] \"           Additionally, the score is used to decide which of two equally\" [:br] \"           large consensus sets is chosen as the better one.\" [:br] \"         * `predict(X)`: Returns predicted values using the linear model,\" [:br] \"           which is used to compute residual error using loss function.\" [:br] \"\" [:br] \"        If `base_estimator` is None, then\" [:br] \"        ``base_estimator=sklearn.linear_model.LinearRegression()`` is used for\" [:br] \"        target values of dtype float.\" [:br] \"\" [:br] \"        Note that the current implementation only supports regression\" [:br] \"        estimators.\" [:br] \"\" [:br] \"    min_samples : int (>= 1) or float ([0, 1]), optional\" [:br] \"        Minimum number of samples chosen randomly from original data. Treated\" [:br] \"        as an absolute number of samples for `min_samples >= 1`, treated as a\" [:br] \"        relative number `ceil(min_samples * X.shape[0]`) for\" [:br] \"        `min_samples < 1`. This is typically chosen as the minimal number of\" [:br] \"        samples necessary to estimate the given `base_estimator`. By default a\" [:br] \"        ``sklearn.linear_model.LinearRegression()`` estimator is assumed and\" [:br] \"        `min_samples` is chosen as ``X.shape[1] + 1``.\" [:br] \"\" [:br] \"    residual_threshold : float, optional\" [:br] \"        Maximum residual for a data sample to be classified as an inlier.\" [:br] \"        By default the threshold is chosen as the MAD (median absolute\" [:br] \"        deviation) of the target values `y`.\" [:br] \"\" [:br] \"    is_data_valid : callable, optional\" [:br] \"        This function is called with the randomly selected data before the\" [:br] \"        model is fitted to it: `is_data_valid(X, y)`. If its return value is\" [:br] \"        False the current randomly chosen sub-sample is skipped.\" [:br] \"\" [:br] \"    is_model_valid : callable, optional\" [:br] \"        This function is called with the estimated model and the randomly\" [:br] \"        selected data: `is_model_valid(model, X, y)`. If its return value is\" [:br] \"        False the current randomly chosen sub-sample is skipped.\" [:br] \"        Rejecting samples with this function is computationally costlier than\" [:br] \"        with `is_data_valid`. `is_model_valid` should therefore only be used if\" [:br] \"        the estimated model is needed for making the rejection decision.\" [:br] \"\" [:br] \"    max_trials : int, optional\" [:br] \"        Maximum number of iterations for random sample selection.\" [:br] \"\" [:br] \"    max_skips : int, optional\" [:br] \"        Maximum number of iterations that can be skipped due to finding zero\" [:br] \"        inliers or invalid data defined by ``is_data_valid`` or invalid models\" [:br] \"        defined by ``is_model_valid``.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    stop_n_inliers : int, optional\" [:br] \"        Stop iteration if at least this number of inliers are found.\" [:br] \"\" [:br] \"    stop_score : float, optional\" [:br] \"        Stop iteration if score is greater equal than this threshold.\" [:br] \"\" [:br] \"    stop_probability : float in range [0, 1], optional\" [:br] \"        RANSAC iteration stops if at least one outlier-free set of the training\" [:br] \"        data is sampled in RANSAC. This requires to generate at least N\" [:br] \"        samples (iterations)::\" [:br] \"\" [:br] \"            N >= log(1 - probability) / log(1 - e**m)\" [:br] \"\" [:br] \"        where the probability (confidence) is typically set to high value such\" [:br] \"        as 0.99 (the default) and e is the current fraction of inliers w.r.t.\" [:br] \"        the total number of samples.\" [:br] \"\" [:br] \"    loss : string, callable, optional, default \\\"absolute_loss\\\"\" [:br] \"        String inputs, \\\"absolute_loss\\\" and \\\"squared_loss\\\" are supported which\" [:br] \"        find the absolute loss and squared loss per sample\" [:br] \"        respectively.\" [:br] \"\" [:br] \"        If ``loss`` is a callable, then it should be a function that takes\" [:br] \"        two arrays as inputs, the true and predicted value and returns a 1-D\" [:br] \"        array with the i-th value of the array corresponding to the loss\" [:br] \"        on ``X[i]``.\" [:br] \"\" [:br] \"        If the loss on a sample is greater than the ``residual_threshold``,\" [:br] \"        then this sample is classified as an outlier.\" [:br] \"\" [:br] \"        .. versionadded:: 0.18\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        The generator used to initialize the centers.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    estimator_ : object\" [:br] \"        Best fitted model (copy of the `base_estimator` object).\" [:br] \"\" [:br] \"    n_trials_ : int\" [:br] \"        Number of random selection trials until one of the stop criteria is\" [:br] \"        met. It is always ``<= max_trials``.\" [:br] \"\" [:br] \"    inlier_mask_ : bool array of shape [n_samples]\" [:br] \"        Boolean mask of inliers classified as ``True``.\" [:br] \"\" [:br] \"    n_skips_no_inliers_ : int\" [:br] \"        Number of iterations skipped due to finding zero inliers.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    n_skips_invalid_data_ : int\" [:br] \"        Number of iterations skipped due to invalid data defined by\" [:br] \"        ``is_data_valid``.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    n_skips_invalid_model_ : int\" [:br] \"        Number of iterations skipped due to an invalid model defined by\" [:br] \"        ``is_model_valid``.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.linear_model import RANSACRegressor\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> X, y = make_regression(\" [:br] \"    ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\" [:br] \"    >>> reg = RANSACRegressor(random_state=0).fit(X, y)\" [:br] \"    >>> reg.score(X, y)\" [:br] \"    0.9885...\" [:br] \"    >>> reg.predict(X[:1,])\" [:br] \"    array([-31.9417...])\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    .. [1] https://en.wikipedia.org/wiki/RANSAC\" [:br] \"    .. [2] https://www.sri.com/sites/default/files/publications/ransac-publication.pdf\" [:br] \"    .. [3] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/ridge\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n|         :alpha |    1.000 |\\n|        :copy-x |     true |\\n| :fit-intercept |     true |\\n|      :max-iter |          |\\n|     :normalize |    false |\\n|  :random-state |          |\\n|        :solver |     auto |\\n|           :tol | 0.001000 |\\n\"]]] [:span (\"Linear least squares with l2 regularization.\" [:br] \"\" [:br] \"    Minimizes the objective function::\" [:br] \"\" [:br] \"    ||y - Xw||^2_2 + alpha * ||w||^2_2\" [:br] \"\" [:br] \"    This model solves a regression model where the loss function is\" [:br] \"    the linear least squares function and regularization is given by\" [:br] \"    the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\" [:br] \"    This estimator has built-in support for multi-variate regression\" [:br] \"    (i.e., when y is a 2d-array of shape (n_samples, n_targets)).\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <ridge_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alpha : {float, ndarray of shape (n_targets,)}, default=1.0\" [:br] \"        Regularization strength; must be a positive float. Regularization\" [:br] \"        improves the conditioning of the problem and reduces the variance of\" [:br] \"        the estimates. Larger values specify stronger regularization.\" [:br] \"        Alpha corresponds to ``1 / (2C)`` in other linear models such as\" [:br] \"        :class:`~sklearn.linear_model.LogisticRegression` or\" [:br] \"        :class:`sklearn.svm.LinearSVC`. If an array is passed, penalties are\" [:br] \"        assumed to be specific to the targets. Hence they must correspond in\" [:br] \"        number.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether to fit the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. ``X`` and ``y`` are expected to be centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    copy_X : bool, default=True\" [:br] \"        If True, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    max_iter : int, default=None\" [:br] \"        Maximum number of iterations for conjugate gradient solver.\" [:br] \"        For 'sparse_cg' and 'lsqr' solvers, the default value is determined\" [:br] \"        by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\" [:br] \"\" [:br] \"    tol : float, default=1e-3\" [:br] \"        Precision of the solution.\" [:br] \"\" [:br] \"    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\" [:br] \"        Solver to use in the computational routines:\" [:br] \"\" [:br] \"        - 'auto' chooses the solver automatically based on the type of data.\" [:br] \"\" [:br] \"        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\" [:br] \"          coefficients. More stable for singular matrices than 'cholesky'.\" [:br] \"\" [:br] \"        - 'cholesky' uses the standard scipy.linalg.solve function to\" [:br] \"          obtain a closed-form solution.\" [:br] \"\" [:br] \"        - 'sparse_cg' uses the conjugate gradient solver as found in\" [:br] \"          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\" [:br] \"          more appropriate than 'cholesky' for large-scale data\" [:br] \"          (possibility to set `tol` and `max_iter`).\" [:br] \"\" [:br] \"        - 'lsqr' uses the dedicated regularized least-squares routine\" [:br] \"          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\" [:br] \"          procedure.\" [:br] \"\" [:br] \"        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\" [:br] \"          its improved, unbiased version named SAGA. Both methods also use an\" [:br] \"          iterative procedure, and are often faster than other solvers when\" [:br] \"          both n_samples and n_features are large. Note that 'sag' and\" [:br] \"          'saga' fast convergence is only guaranteed on features with\" [:br] \"          approximately the same scale. You can preprocess the data with a\" [:br] \"          scaler from sklearn.preprocessing.\" [:br] \"\" [:br] \"        All last five solvers support both dense and sparse data. However, only\" [:br] \"        'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is\" [:br] \"        True.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"           Stochastic Average Gradient descent solver.\" [:br] \"        .. versionadded:: 0.19\" [:br] \"           SAGA solver.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\" [:br] \"        See :term:`Glossary <random_state>` for details.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"           `random_state` to support Stochastic Average Gradient.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\" [:br] \"        Weight vector(s).\" [:br] \"\" [:br] \"    intercept_ : float or ndarray of shape (n_targets,)\" [:br] \"        Independent term in decision function. Set to 0.0 if\" [:br] \"        ``fit_intercept = False``.\" [:br] \"\" [:br] \"    n_iter_ : None or ndarray of shape (n_targets,)\" [:br] \"        Actual number of iterations for each target. Available only for\" [:br] \"        sag and lsqr solvers. Other solvers will return None.\" [:br] \"\" [:br] \"        .. versionadded:: 0.17\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    RidgeClassifier : Ridge classifier\" [:br] \"    RidgeCV : Ridge regression with built-in cross validation\" [:br] \"    :class:`sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\" [:br] \"        combines ridge regression with the kernel trick\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.linear_model import Ridge\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> n_samples, n_features = 10, 5\" [:br] \"    >>> rng = np.random.RandomState(0)\" [:br] \"    >>> y = rng.randn(n_samples)\" [:br] \"    >>> X = rng.randn(n_samples, n_features)\" [:br] \"    >>> clf = Ridge(alpha=1.0)\" [:br] \"    >>> clf.fit(X, y)\" [:br] \"    Ridge()\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/ridge-cv\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [7 2]:\\n\\n|            :name |                    :default |\\n|------------------|-----------------------------|\\n|          :alphas | #tech.v3.tensor<float64>[3] |\\n|                  | [0.1000 1.000 10.00]        |\\n|              :cv |                             |\\n|   :fit-intercept |                        true |\\n|        :gcv-mode |                             |\\n|       :normalize |                       false |\\n|         :scoring |                             |\\n| :store-cv-values |                       false |\\n\"]]] [:span (\"Ridge regression with built-in cross-validation.\" [:br] \"\" [:br] \"    See glossary entry for :term:`cross-validation estimator`.\" [:br] \"\" [:br] \"    By default, it performs Generalized Cross-Validation, which is a form of\" [:br] \"    efficient Leave-One-Out cross-validation.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <ridge_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    alphas : ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\" [:br] \"        Array of alpha values to try.\" [:br] \"        Regularization strength; must be a positive float. Regularization\" [:br] \"        improves the conditioning of the problem and reduces the variance of\" [:br] \"        the estimates. Larger values specify stronger regularization.\" [:br] \"        Alpha corresponds to ``1 / (2C)`` in other linear models such as\" [:br] \"        :class:`~sklearn.linear_model.LogisticRegression` or\" [:br] \"        :class:`sklearn.svm.LinearSVC`.\" [:br] \"        If using generalized cross-validation, alphas must be positive.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations\" [:br] \"        (i.e. data is expected to be centered).\" [:br] \"\" [:br] \"    normalize : bool, default=False\" [:br] \"        This parameter is ignored when ``fit_intercept`` is set to False.\" [:br] \"        If True, the regressors X will be normalized before regression by\" [:br] \"        subtracting the mean and dividing by the l2-norm.\" [:br] \"        If you wish to standardize, please use\" [:br] \"        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\" [:br] \"        on an estimator with ``normalize=False``.\" [:br] \"\" [:br] \"    scoring : string, callable, default=None\" [:br] \"        A string (see model evaluation documentation) or\" [:br] \"        a scorer callable object / function with signature\" [:br] \"        ``scorer(estimator, X, y)``.\" [:br] \"        If None, the negative mean squared error if cv is 'auto' or None\" [:br] \"        (i.e. when using generalized cross-validation), and r2 score otherwise.\" [:br] \"\" [:br] \"    cv : int, cross-validation generator or an iterable, default=None\" [:br] \"        Determines the cross-validation splitting strategy.\" [:br] \"        Possible inputs for cv are:\" [:br] \"\" [:br] \"        - None, to use the efficient Leave-One-Out cross-validation\" [:br] \"          (also known as Generalized Cross-Validation).\" [:br] \"        - integer, to specify the number of folds.\" [:br] \"        - :term:`CV splitter`,\" [:br] \"        - An iterable yielding (train, test) splits as arrays of indices.\" [:br] \"\" [:br] \"        For integer/None inputs, if ``y`` is binary or multiclass,\" [:br] \"        :class:`sklearn.model_selection.StratifiedKFold` is used, else,\" [:br] \"        :class:`sklearn.model_selection.KFold` is used.\" [:br] \"\" [:br] \"        Refer :ref:`User Guide <cross_validation>` for the various\" [:br] \"        cross-validation strategies that can be used here.\" [:br] \"\" [:br] \"    gcv_mode : {'auto', 'svd', eigen'}, default='auto'\" [:br] \"        Flag indicating which strategy to use when performing\" [:br] \"        Generalized Cross-Validation. Options are::\" [:br] \"\" [:br] \"            'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\" [:br] \"            'svd' : force use of singular value decomposition of X when X is\" [:br] \"                dense, eigenvalue decomposition of X^T.X when X is sparse.\" [:br] \"            'eigen' : force computation via eigendecomposition of X.X^T\" [:br] \"\" [:br] \"        The 'auto' mode is the default and is intended to pick the cheaper\" [:br] \"        option of the two depending on the shape of the training data.\" [:br] \"\" [:br] \"    store_cv_values : bool, default=False\" [:br] \"        Flag indicating if the cross-validation values corresponding to\" [:br] \"        each alpha should be stored in the ``cv_values_`` attribute (see\" [:br] \"        below). This flag is only compatible with ``cv=None`` (i.e. using\" [:br] \"        Generalized Cross-Validation).\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    cv_values_ : ndarray of shape (n_samples, n_alphas) or         shape (n_samples, n_targets, n_alphas), optional\" [:br] \"        Cross-validation values for each alpha (only available if         ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been         called, this attribute will contain the mean squared errors         (by default) or the values of the ``{loss,score}_func`` function         (if provided in the constructor).\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (n_features) or (n_targets, n_features)\" [:br] \"        Weight vector(s).\" [:br] \"\" [:br] \"    intercept_ : float or ndarray of shape (n_targets,)\" [:br] \"        Independent term in decision function. Set to 0.0 if\" [:br] \"        ``fit_intercept = False``.\" [:br] \"\" [:br] \"    alpha_ : float\" [:br] \"        Estimated regularization parameter.\" [:br] \"\" [:br] \"    best_score_ : float\" [:br] \"        Score of base estimator with best alpha.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.datasets import load_diabetes\" [:br] \"    >>> from sklearn.linear_model import RidgeCV\" [:br] \"    >>> X, y = load_diabetes(return_X_y=True)\" [:br] \"    >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\" [:br] \"    >>> clf.score(X, y)\" [:br] \"    0.5166...\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    Ridge : Ridge regression\" [:br] \"    RidgeClassifier : Ridge classifier\" [:br] \"    RidgeClassifierCV : Ridge classifier with built-in cross validation\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/sgd-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [19 2]:\\n\\n|                :name |     :default |\\n|----------------------|--------------|\\n|    :n-iter-no-change |            5 |\\n|       :learning-rate |   invscaling |\\n|             :average |        false |\\n|                 :tol |     0.001000 |\\n|      :early-stopping |        false |\\n|               :eta-0 |      0.01000 |\\n|             :shuffle |         true |\\n|             :penalty |           l2 |\\n|             :power-t |       0.2500 |\\n|            :max-iter |         1000 |\\n|        :random-state |              |\\n|       :fit-intercept |         true |\\n|               :alpha |    0.0001000 |\\n|          :warm-start |        false |\\n|           :l-1-ratio |       0.1500 |\\n| :validation-fraction |       0.1000 |\\n|                :loss | squared_loss |\\n|             :verbose |            0 |\\n|             :epsilon |       0.1000 |\\n\"]]] [:span (\"Linear model fitted by minimizing a regularized empirical loss with SGD\" [:br] \"\" [:br] \"    SGD stands for Stochastic Gradient Descent: the gradient of the loss is\" [:br] \"    estimated each sample at a time and the model is updated along the way with\" [:br] \"    a decreasing strength schedule (aka learning rate).\" [:br] \"\" [:br] \"    The regularizer is a penalty added to the loss function that shrinks model\" [:br] \"    parameters towards the zero vector using either the squared euclidean norm\" [:br] \"    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\" [:br] \"    parameter update crosses the 0.0 value because of the regularizer, the\" [:br] \"    update is truncated to 0.0 to allow for learning sparse models and achieve\" [:br] \"    online feature selection.\" [:br] \"\" [:br] \"    This implementation works with data represented as dense numpy arrays of\" [:br] \"    floating point values for the features.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <sgd>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    loss : str, default='squared_loss'\" [:br] \"        The loss function to be used. The possible values are 'squared_loss',\" [:br] \"        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\" [:br] \"\" [:br] \"        The 'squared_loss' refers to the ordinary least squares fit.\" [:br] \"        'huber' modifies 'squared_loss' to focus less on getting outliers\" [:br] \"        correct by switching from squared to linear loss past a distance of\" [:br] \"        epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\" [:br] \"        linear past that; this is the loss function used in SVR.\" [:br] \"        'squared_epsilon_insensitive' is the same but becomes squared loss past\" [:br] \"        a tolerance of epsilon.\" [:br] \"\" [:br] \"        More details about the losses formulas can be found in the\" [:br] \"        :ref:`User Guide <sgd_mathematical_formulation>`.\" [:br] \"\" [:br] \"    penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\" [:br] \"        The penalty (aka regularization term) to be used. Defaults to 'l2'\" [:br] \"        which is the standard regularizer for linear SVM models. 'l1' and\" [:br] \"        'elasticnet' might bring sparsity to the model (feature selection)\" [:br] \"        not achievable with 'l2'.\" [:br] \"\" [:br] \"    alpha : float, default=0.0001\" [:br] \"        Constant that multiplies the regularization term. The higher the\" [:br] \"        value, the stronger the regularization.\" [:br] \"        Also used to compute the learning rate when set to `learning_rate` is\" [:br] \"        set to 'optimal'.\" [:br] \"\" [:br] \"    l1_ratio : float, default=0.15\" [:br] \"        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\" [:br] \"        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\" [:br] \"        Only used if `penalty` is 'elasticnet'.\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Whether the intercept should be estimated or not. If False, the\" [:br] \"        data is assumed to be already centered.\" [:br] \"\" [:br] \"    max_iter : int, default=1000\" [:br] \"        The maximum number of passes over the training data (aka epochs).\" [:br] \"        It only impacts the behavior in the ``fit`` method, and not the\" [:br] \"        :meth:`partial_fit` method.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    tol : float, default=1e-3\" [:br] \"        The stopping criterion. If it is not None, training will stop\" [:br] \"        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\" [:br] \"        epochs.\" [:br] \"\" [:br] \"        .. versionadded:: 0.19\" [:br] \"\" [:br] \"    shuffle : bool, default=True\" [:br] \"        Whether or not the training data should be shuffled after each epoch.\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        The verbosity level.\" [:br] \"\" [:br] \"    epsilon : float, default=0.1\" [:br] \"        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\" [:br] \"        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\" [:br] \"        For 'huber', determines the threshold at which it becomes less\" [:br] \"        important to get the prediction exactly right.\" [:br] \"        For epsilon-insensitive, any differences between the current prediction\" [:br] \"        and the correct label are ignored if they are less than this threshold.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        Used for shuffling the data, when ``shuffle`` is set to ``True``.\" [:br] \"        Pass an int for reproducible output across multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`.\" [:br] \"\" [:br] \"    learning_rate : string, default='invscaling'\" [:br] \"        The learning rate schedule:\" [:br] \"\" [:br] \"        - 'constant': `eta = eta0`\" [:br] \"        - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\" [:br] \"          where t0 is chosen by a heuristic proposed by Leon Bottou.\" [:br] \"        - 'invscaling': `eta = eta0 / pow(t, power_t)`\" [:br] \"        - 'adaptive': eta = eta0, as long as the training keeps decreasing.\" [:br] \"          Each time n_iter_no_change consecutive epochs fail to decrease the\" [:br] \"          training loss by tol or fail to increase validation score by tol if\" [:br] \"          early_stopping is True, the current learning rate is divided by 5.\" [:br] \"\" [:br] \"            .. versionadded:: 0.20\" [:br] \"                Added 'adaptive' option\" [:br] \"\" [:br] \"    eta0 : double, default=0.01\" [:br] \"        The initial learning rate for the 'constant', 'invscaling' or\" [:br] \"        'adaptive' schedules. The default value is 0.01.\" [:br] \"\" [:br] \"    power_t : double, default=0.25\" [:br] \"        The exponent for inverse scaling learning rate.\" [:br] \"\" [:br] \"    early_stopping : bool, default=False\" [:br] \"        Whether to use early stopping to terminate training when validation\" [:br] \"        score is not improving. If set to True, it will automatically set aside\" [:br] \"        a fraction of training data as validation and terminate\" [:br] \"        training when validation score returned by the `score` method is not\" [:br] \"        improving by at least `tol` for `n_iter_no_change` consecutive\" [:br] \"        epochs.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"            Added 'early_stopping' option\" [:br] \"\" [:br] \"    validation_fraction : float, default=0.1\" [:br] \"        The proportion of training data to set aside as validation set for\" [:br] \"        early stopping. Must be between 0 and 1.\" [:br] \"        Only used if `early_stopping` is True.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"            Added 'validation_fraction' option\" [:br] \"\" [:br] \"    n_iter_no_change : int, default=5\" [:br] \"        Number of iterations with no improvement to wait before early stopping.\" [:br] \"\" [:br] \"        .. versionadded:: 0.20\" [:br] \"            Added 'n_iter_no_change' option\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        When set to True, reuse the solution of the previous call to fit as\" [:br] \"        initialization, otherwise, just erase the previous solution.\" [:br] \"        See :term:`the Glossary <warm_start>`.\" [:br] \"\" [:br] \"        Repeatedly calling fit or partial_fit when warm_start is True can\" [:br] \"        result in a different solution than when calling fit a single time\" [:br] \"        because of the way the data is shuffled.\" [:br] \"        If a dynamic learning rate is used, the learning rate is adapted\" [:br] \"        depending on the number of samples already seen. Calling ``fit`` resets\" [:br] \"        this counter, while ``partial_fit``  will result in increasing the\" [:br] \"        existing counter.\" [:br] \"\" [:br] \"    average : bool or int, default=False\" [:br] \"        When set to True, computes the averaged SGD weights accross all\" [:br] \"        updates and stores the result in the ``coef_`` attribute. If set to\" [:br] \"        an int greater than 1, averaging will begin once the total number of\" [:br] \"        samples seen reaches `average`. So ``average=10`` will begin\" [:br] \"        averaging after seeing 10 samples.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : ndarray of shape (n_features,)\" [:br] \"        Weights assigned to the features.\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape (1,)\" [:br] \"        The intercept term.\" [:br] \"\" [:br] \"    average_coef_ : ndarray of shape (n_features,)\" [:br] \"        Averaged weights assigned to the features. Only available\" [:br] \"        if ``average=True``.\" [:br] \"\" [:br] \"        .. deprecated:: 0.23\" [:br] \"            Attribute ``average_coef_`` was deprecated\" [:br] \"            in version 0.23 and will be removed in 0.25.\" [:br] \"\" [:br] \"    average_intercept_ : ndarray of shape (1,)\" [:br] \"        The averaged intercept term. Only available if ``average=True``.\" [:br] \"\" [:br] \"        .. deprecated:: 0.23\" [:br] \"            Attribute ``average_intercept_`` was deprecated\" [:br] \"            in version 0.23 and will be removed in 0.25.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        The actual number of iterations before reaching the stopping criterion.\" [:br] \"\" [:br] \"    t_ : int\" [:br] \"        Number of weight updates performed during training.\" [:br] \"        Same as ``(n_iter_ * n_samples)``.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> from sklearn.linear_model import SGDRegressor\" [:br] \"    >>> from sklearn.pipeline import make_pipeline\" [:br] \"    >>> from sklearn.preprocessing import StandardScaler\" [:br] \"    >>> n_samples, n_features = 10, 5\" [:br] \"    >>> rng = np.random.RandomState(0)\" [:br] \"    >>> y = rng.randn(n_samples)\" [:br] \"    >>> X = rng.randn(n_samples, n_features)\" [:br] \"    >>> # Always scale the input. The most convenient way is to use a pipeline.\" [:br] \"    >>> reg = make_pipeline(StandardScaler(),\" [:br] \"    ...                     SGDRegressor(max_iter=1000, tol=1e-3))\" [:br] \"    >>> reg.fit(X, y)\" [:br] \"    Pipeline(steps=[('standardscaler', StandardScaler()),\" [:br] \"                    ('sgdregressor', SGDRegressor())])\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    Ridge, ElasticNet, Lasso, sklearn.svm.SVR\" [:br] \"\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/svr\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [11 2]:\\n\\n|       :name | :default |\\n|-------------|----------|\\n|     :kernel |      rbf |\\n|      :gamma |    scale |\\n|     :degree |        3 |\\n|        :tol | 0.001000 |\\n|  :shrinking |     true |\\n|          :c |    1.000 |\\n|   :max-iter |       -1 |\\n|     :coef-0 |    0.000 |\\n| :cache-size |      200 |\\n|    :verbose |    false |\\n|    :epsilon |   0.1000 |\\n\"]]] [:span (\"Epsilon-Support Vector Regression.\" [:br] \"\" [:br] \"    The free parameters in the model are C and epsilon.\" [:br] \"\" [:br] \"    The implementation is based on libsvm. The fit time complexity\" [:br] \"    is more than quadratic with the number of samples which makes it hard\" [:br] \"    to scale to datasets with more than a couple of 10000 samples. For large\" [:br] \"    datasets consider using :class:`sklearn.svm.LinearSVR` or\" [:br] \"    :class:`sklearn.linear_model.SGDRegressor` instead, possibly after a\" [:br] \"    :class:`sklearn.kernel_approximation.Nystroem` transformer.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <svm_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\" [:br] \"         Specifies the kernel type to be used in the algorithm.\" [:br] \"         It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\" [:br] \"         a callable.\" [:br] \"         If none is given, 'rbf' will be used. If a callable is given it is\" [:br] \"         used to precompute the kernel matrix.\" [:br] \"\" [:br] \"    degree : int, default=3\" [:br] \"        Degree of the polynomial kernel function ('poly').\" [:br] \"        Ignored by all other kernels.\" [:br] \"\" [:br] \"    gamma : {'scale', 'auto'} or float, default='scale'\" [:br] \"        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\" [:br] \"\" [:br] \"        - if ``gamma='scale'`` (default) is passed then it uses\" [:br] \"          1 / (n_features * X.var()) as value of gamma,\" [:br] \"        - if 'auto', uses 1 / n_features.\" [:br] \"\" [:br] \"        .. versionchanged:: 0.22\" [:br] \"           The default value of ``gamma`` changed from 'auto' to 'scale'.\" [:br] \"\" [:br] \"    coef0 : float, default=0.0\" [:br] \"        Independent term in kernel function.\" [:br] \"        It is only significant in 'poly' and 'sigmoid'.\" [:br] \"\" [:br] \"    tol : float, default=1e-3\" [:br] \"        Tolerance for stopping criterion.\" [:br] \"\" [:br] \"    C : float, default=1.0\" [:br] \"        Regularization parameter. The strength of the regularization is\" [:br] \"        inversely proportional to C. Must be strictly positive.\" [:br] \"        The penalty is a squared l2 penalty.\" [:br] \"\" [:br] \"    epsilon : float, default=0.1\" [:br] \"         Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\" [:br] \"         within which no penalty is associated in the training loss function\" [:br] \"         with points predicted within a distance epsilon from the actual\" [:br] \"         value.\" [:br] \"\" [:br] \"    shrinking : bool, default=True\" [:br] \"        Whether to use the shrinking heuristic.\" [:br] \"        See the :ref:`User Guide <shrinking_svm>`.\" [:br] \"\" [:br] \"    cache_size : float, default=200\" [:br] \"        Specify the size of the kernel cache (in MB).\" [:br] \"\" [:br] \"    verbose : bool, default=False\" [:br] \"        Enable verbose output. Note that this setting takes advantage of a\" [:br] \"        per-process runtime setting in libsvm that, if enabled, may not work\" [:br] \"        properly in a multithreaded context.\" [:br] \"\" [:br] \"    max_iter : int, default=-1\" [:br] \"        Hard limit on iterations within solver, or -1 for no limit.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    support_ : ndarray of shape (n_SV,)\" [:br] \"        Indices of support vectors.\" [:br] \"\" [:br] \"    support_vectors_ : ndarray of shape (n_SV, n_features)\" [:br] \"        Support vectors.\" [:br] \"\" [:br] \"    dual_coef_ : ndarray of shape (1, n_SV)\" [:br] \"        Coefficients of the support vector in the decision function.\" [:br] \"\" [:br] \"    coef_ : ndarray of shape (1, n_features)\" [:br] \"        Weights assigned to the features (coefficients in the primal\" [:br] \"        problem). This is only available in the case of a linear kernel.\" [:br] \"\" [:br] \"        `coef_` is readonly property derived from `dual_coef_` and\" [:br] \"        `support_vectors_`.\" [:br] \"\" [:br] \"    fit_status_ : int\" [:br] \"        0 if correctly fitted, 1 otherwise (will raise warning)\" [:br] \"\" [:br] \"    intercept_ : ndarray of shape (1,)\" [:br] \"        Constants in decision function.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.svm import SVR\" [:br] \"    >>> from sklearn.pipeline import make_pipeline\" [:br] \"    >>> from sklearn.preprocessing import StandardScaler\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> n_samples, n_features = 10, 5\" [:br] \"    >>> rng = np.random.RandomState(0)\" [:br] \"    >>> y = rng.randn(n_samples)\" [:br] \"    >>> X = rng.randn(n_samples, n_features)\" [:br] \"    >>> regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\" [:br] \"    >>> regr.fit(X, y)\" [:br] \"    Pipeline(steps=[('standardscaler', StandardScaler()),\" [:br] \"                    ('svr', SVR(epsilon=0.2))])\" [:br] \"\" [:br] \"\" [:br] \"    See also\" [:br] \"    --------\" [:br] \"    NuSVR\" [:br] \"        Support Vector Machine for regression implemented using libsvm\" [:br] \"        using a parameter to control the number of support vectors.\" [:br] \"\" [:br] \"    LinearSVR\" [:br] \"        Scalable Linear Support Vector Machine for regression\" [:br] \"        implemented using liblinear.\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    **References:**\" [:br] \"    `LIBSVM: A Library for Support Vector Machines\" [:br] \"    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`__\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/theil-sen-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [9 2]:\\n\\n|              :name |  :default |\\n|--------------------|-----------|\\n| :max-subpopulation | 1.000E+04 |\\n|               :tol |  0.001000 |\\n|      :n-subsamples |           |\\n|          :max-iter |     300.0 |\\n|            :n-jobs |           |\\n|      :random-state |           |\\n|            :copy-x |      true |\\n|     :fit-intercept |      true |\\n|           :verbose |     false |\\n\"]]] [:span (\"Theil-Sen Estimator: robust multivariate regression model.\" [:br] \"\" [:br] \"    The algorithm calculates least square solutions on subsets with size\" [:br] \"    n_subsamples of the samples in X. Any value of n_subsamples between the\" [:br] \"    number of features and samples leads to an estimator with a compromise\" [:br] \"    between robustness and efficiency. Since the number of least square\" [:br] \"    solutions is \\\"n_samples choose n_subsamples\\\", it can be extremely large\" [:br] \"    and can therefore be limited with max_subpopulation. If this limit is\" [:br] \"    reached, the subsets are chosen randomly. In a final step, the spatial\" [:br] \"    median (or L1 median) is calculated of all least square solutions.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <theil_sen_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    fit_intercept : boolean, optional, default True\" [:br] \"        Whether to calculate the intercept for this model. If set\" [:br] \"        to false, no intercept will be used in calculations.\" [:br] \"\" [:br] \"    copy_X : boolean, optional, default True\" [:br] \"        If True, X will be copied; else, it may be overwritten.\" [:br] \"\" [:br] \"    max_subpopulation : int, optional, default 1e4\" [:br] \"        Instead of computing with a set of cardinality 'n choose k', where n is\" [:br] \"        the number of samples and k is the number of subsamples (at least\" [:br] \"        number of features), consider only a stochastic subpopulation of a\" [:br] \"        given maximal size if 'n choose k' is larger than max_subpopulation.\" [:br] \"        For other than small problem sizes this parameter will determine\" [:br] \"        memory usage and runtime if n_subsamples is not changed.\" [:br] \"\" [:br] \"    n_subsamples : int, optional, default None\" [:br] \"        Number of samples to calculate the parameters. This is at least the\" [:br] \"        number of features (plus 1 if fit_intercept=True) and the number of\" [:br] \"        samples as a maximum. A lower number leads to a higher breakdown\" [:br] \"        point and a low efficiency while a high number leads to a low\" [:br] \"        breakdown point and a high efficiency. If None, take the\" [:br] \"        minimum number of subsamples leading to maximal robustness.\" [:br] \"        If n_subsamples is set to n_samples, Theil-Sen is identical to least\" [:br] \"        squares.\" [:br] \"\" [:br] \"    max_iter : int, optional, default 300\" [:br] \"        Maximum number of iterations for the calculation of spatial median.\" [:br] \"\" [:br] \"    tol : float, optional, default 1.e-3\" [:br] \"        Tolerance when calculating spatial median.\" [:br] \"\" [:br] \"    random_state : int, RandomState instance, default=None\" [:br] \"        A random number generator instance to define the state of the random\" [:br] \"        permutations generator. Pass an int for reproducible output across\" [:br] \"        multiple function calls.\" [:br] \"        See :term:`Glossary <random_state>`\" [:br] \"\" [:br] \"    n_jobs : int or None, optional (default=None)\" [:br] \"        Number of CPUs to use during the cross validation.\" [:br] \"        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\" [:br] \"        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\" [:br] \"        for more details.\" [:br] \"\" [:br] \"    verbose : boolean, optional, default False\" [:br] \"        Verbose mode when fitting the model.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array, shape = (n_features)\" [:br] \"        Coefficients of the regression model (median of distribution).\" [:br] \"\" [:br] \"    intercept_ : float\" [:br] \"        Estimated intercept of regression model.\" [:br] \"\" [:br] \"    breakdown_ : float\" [:br] \"        Approximated breakdown point.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        Number of iterations needed for the spatial median.\" [:br] \"\" [:br] \"    n_subpopulation_ : int\" [:br] \"        Number of combinations taken into account from 'n choose k', where n is\" [:br] \"        the number of samples and k is the number of subsamples.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> from sklearn.linear_model import TheilSenRegressor\" [:br] \"    >>> from sklearn.datasets import make_regression\" [:br] \"    >>> X, y = make_regression(\" [:br] \"    ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\" [:br] \"    >>> reg = TheilSenRegressor(random_state=0).fit(X, y)\" [:br] \"    >>> reg.score(X, y)\" [:br] \"    0.9884...\" [:br] \"    >>> reg.predict(X[:1,])\" [:br] \"    array([-31.5871...])\" [:br] \"\" [:br] \"    References\" [:br] \"    ----------\" [:br] \"    - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\" [:br] \"      Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\" [:br] \"      http://home.olemiss.edu/~xdang/papers/MTSE.pdf\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/transformed-target-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"322px\"}} [:p/markdown \"_unnamed [5 2]:\\n\\n|          :name | :default |\\n|----------------|----------|\\n| :check-inverse |     true |\\n|          :func |          |\\n|  :inverse-func |          |\\n|     :regressor |          |\\n|   :transformer |          |\\n\"]]] [:span (\"Meta-estimator to regress on a transformed target.\" [:br] \"\" [:br] \"    Useful for applying a non-linear transformation to the target ``y`` in\" [:br] \"    regression problems. This transformation can be given as a Transformer\" [:br] \"    such as the QuantileTransformer or as a function and its inverse such as\" [:br] \"    ``log`` and ``exp``.\" [:br] \"\" [:br] \"    The computation during ``fit`` is::\" [:br] \"\" [:br] \"        regressor.fit(X, func(y))\" [:br] \"\" [:br] \"    or::\" [:br] \"\" [:br] \"        regressor.fit(X, transformer.transform(y))\" [:br] \"\" [:br] \"    The computation during ``predict`` is::\" [:br] \"\" [:br] \"        inverse_func(regressor.predict(X))\" [:br] \"\" [:br] \"    or::\" [:br] \"\" [:br] \"        transformer.inverse_transform(regressor.predict(X))\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <transformed_target_regressor>`.\" [:br] \"\" [:br] \"    .. versionadded:: 0.20\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    regressor : object, default=None\" [:br] \"        Regressor object such as derived from ``RegressorMixin``. This\" [:br] \"        regressor will automatically be cloned each time prior to fitting.\" [:br] \"        If regressor is ``None``, ``LinearRegression()`` is created and used.\" [:br] \"\" [:br] \"    transformer : object, default=None\" [:br] \"        Estimator object such as derived from ``TransformerMixin``. Cannot be\" [:br] \"        set at the same time as ``func`` and ``inverse_func``. If\" [:br] \"        ``transformer`` is ``None`` as well as ``func`` and ``inverse_func``,\" [:br] \"        the transformer will be an identity transformer. Note that the\" [:br] \"        transformer will be cloned during fitting. Also, the transformer is\" [:br] \"        restricting ``y`` to be a numpy array.\" [:br] \"\" [:br] \"    func : function, default=None\" [:br] \"        Function to apply to ``y`` before passing to ``fit``. Cannot be set at\" [:br] \"        the same time as ``transformer``. The function needs to return a\" [:br] \"        2-dimensional array. If ``func`` is ``None``, the function used will be\" [:br] \"        the identity function.\" [:br] \"\" [:br] \"    inverse_func : function, default=None\" [:br] \"        Function to apply to the prediction of the regressor. Cannot be set at\" [:br] \"        the same time as ``transformer`` as well. The function needs to return\" [:br] \"        a 2-dimensional array. The inverse function is used to return\" [:br] \"        predictions to the same space of the original training labels.\" [:br] \"\" [:br] \"    check_inverse : bool, default=True\" [:br] \"        Whether to check that ``transform`` followed by ``inverse_transform``\" [:br] \"        or ``func`` followed by ``inverse_func`` leads to the original targets.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    regressor_ : object\" [:br] \"        Fitted regressor.\" [:br] \"\" [:br] \"    transformer_ : object\" [:br] \"        Transformer used in ``fit`` and ``predict``.\" [:br] \"\" [:br] \"    Examples\" [:br] \"    --------\" [:br] \"    >>> import numpy as np\" [:br] \"    >>> from sklearn.linear_model import LinearRegression\" [:br] \"    >>> from sklearn.compose import TransformedTargetRegressor\" [:br] \"    >>> tt = TransformedTargetRegressor(regressor=LinearRegression(),\" [:br] \"    ...                                 func=np.log, inverse_func=np.exp)\" [:br] \"    >>> X = np.arange(4).reshape(-1, 1)\" [:br] \"    >>> y = np.exp(2 * X).ravel()\" [:br] \"    >>> tt.fit(X, y)\" [:br] \"    TransformedTargetRegressor(...)\" [:br] \"    >>> tt.score(X, y)\" [:br] \"    1.0\" [:br] \"    >>> tt.regressor_.coef_\" [:br] \"    array([2.])\" [:br] \"\" [:br] \"    Notes\" [:br] \"    -----\" [:br] \"    Internally, the target ``y`` is always converted into a 2-dimensional array\" [:br] \"    to be used by scikit-learn transformers. At the time of prediction, the\" [:br] \"    output will be reshaped to a have the same number of dimensions as ``y``.\" [:br] \"\" [:br] \"    See :ref:`examples/compose/plot_transformed_target.py\" [:br] \"    <sphx_glr_auto_examples_compose_plot_transformed_target.py>`.\" [:br] \"\" [:br] \"    \")] [:hr]] [:div [:h3 \":sklearn.regression/tweedie-regressor\"] [:div \"\"] [:div \"\"] [:span [:div {:class \"table table-striped table-hover table-condensed table-responsive\", :style {:height \"400px\"}} [:p/markdown \"_unnamed [8 2]:\\n\\n|          :name |  :default |\\n|----------------|-----------|\\n|         :alpha |     1.000 |\\n| :fit-intercept |      true |\\n|          :link |      auto |\\n|      :max-iter |       100 |\\n|         :power |     0.000 |\\n|           :tol | 0.0001000 |\\n|       :verbose |         0 |\\n|    :warm-start |     false |\\n\"]]] [:span (\"Generalized Linear Model with a Tweedie distribution.\" [:br] \"\" [:br] \"    This estimator can be used to model different GLMs depending on the\" [:br] \"    ``power`` parameter, which determines the underlying distribution.\" [:br] \"\" [:br] \"    Read more in the :ref:`User Guide <Generalized_linear_regression>`.\" [:br] \"\" [:br] \"    Parameters\" [:br] \"    ----------\" [:br] \"    power : float, default=0\" [:br] \"            The power determines the underlying target distribution according\" [:br] \"            to the following table:\" [:br] \"\" [:br] \"            +-------+------------------------+\" [:br] \"            | Power | Distribution           |\" [:br] \"            +=======+========================+\" [:br] \"            | 0     | Normal                 |\" [:br] \"            +-------+------------------------+\" [:br] \"            | 1     | Poisson                |\" [:br] \"            +-------+------------------------+\" [:br] \"            | (1,2) | Compound Poisson Gamma |\" [:br] \"            +-------+------------------------+\" [:br] \"            | 2     | Gamma                  |\" [:br] \"            +-------+------------------------+\" [:br] \"            | 3     | Inverse Gaussian       |\" [:br] \"            +-------+------------------------+\" [:br] \"\" [:br] \"            For ``0 < power < 1``, no distribution exists.\" [:br] \"\" [:br] \"    alpha : float, default=1\" [:br] \"        Constant that multiplies the penalty term and thus determines the\" [:br] \"        regularization strength. ``alpha = 0`` is equivalent to unpenalized\" [:br] \"        GLMs. In this case, the design matrix `X` must have full column rank\" [:br] \"        (no collinearities).\" [:br] \"\" [:br] \"    link : {'auto', 'identity', 'log'}, default='auto'\" [:br] \"        The link function of the GLM, i.e. mapping from linear predictor\" [:br] \"        `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets\" [:br] \"        the link depending on the chosen family as follows:\" [:br] \"\" [:br] \"        - 'identity' for Normal distribution\" [:br] \"        - 'log' for Poisson,  Gamma and Inverse Gaussian distributions\" [:br] \"\" [:br] \"    fit_intercept : bool, default=True\" [:br] \"        Specifies if a constant (a.k.a. bias or intercept) should be\" [:br] \"        added to the linear predictor (X @ coef + intercept).\" [:br] \"\" [:br] \"    max_iter : int, default=100\" [:br] \"        The maximal number of iterations for the solver.\" [:br] \"\" [:br] \"    tol : float, default=1e-4\" [:br] \"        Stopping criterion. For the lbfgs solver,\" [:br] \"        the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\" [:br] \"        where ``g_j`` is the j-th component of the gradient (derivative) of\" [:br] \"        the objective function.\" [:br] \"\" [:br] \"    warm_start : bool, default=False\" [:br] \"        If set to ``True``, reuse the solution of the previous call to ``fit``\" [:br] \"        as initialization for ``coef_`` and ``intercept_`` .\" [:br] \"\" [:br] \"    verbose : int, default=0\" [:br] \"        For the lbfgs solver set verbose to any positive number for verbosity.\" [:br] \"\" [:br] \"    Attributes\" [:br] \"    ----------\" [:br] \"    coef_ : array of shape (n_features,)\" [:br] \"        Estimated coefficients for the linear predictor (`X @ coef_ +\" [:br] \"        intercept_`) in the GLM.\" [:br] \"\" [:br] \"    intercept_ : float\" [:br] \"        Intercept (a.k.a. bias) added to linear predictor.\" [:br] \"\" [:br] \"    n_iter_ : int\" [:br] \"        Actual number of iterations used in the solver.\" [:br] \"    \")] [:hr]])]}}");
     document.getElementById("loading").remove();
    </script>
</html>
